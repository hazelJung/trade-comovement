"""
ë”¥ëŸ¬ë‹ ëª¨ë¸ í†µí•© ë²„ì „
- improved_model_v2.py ê¸°ë°˜
- LSTM/CNN-LSTM ëª¨ë¸ ì¶”ê°€
- ë”¥ëŸ¬ë‹ + ì „í†µì  ì•™ìƒë¸” ê²°í•©
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import TimeSeriesSplit
import warnings
warnings.filterwarnings('ignore')

# ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì²´í¬
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
    DL_AVAILABLE = True
    print("âœ… TensorFlow ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    DL_AVAILABLE = False
    print("âš ï¸ TensorFlow ì—†ìŒ - pip install tensorflow í•„ìš” (ë”¥ëŸ¬ë‹ ëª¨ë¸ ìŠ¤í‚µ)")


# improved_model_v2.pyì˜ ëª¨ë“  í•¨ìˆ˜ import (ê°„ë‹¨íˆ ë³µì‚¬)
# ì‹¤ì œë¡œëŠ” improved_model_v2.pyë¥¼ importí•˜ê±°ë‚˜ í•¨ìˆ˜ë“¤ì„ ë³µì‚¬

# ì—¬ê¸°ì„œëŠ” í•µì‹¬ ë¶€ë¶„ë§Œ ë³´ì—¬ì£¼ê³ , ì „ì²´ëŠ” improved_model_v2.py ì°¸ê³ 

def build_lstm_model_for_pairs(input_shape):
    """ê³µí–‰ì„± ìŒ ì˜ˆì¸¡ìš© LSTM ëª¨ë¸"""
    if not DL_AVAILABLE:
        return None
    
    model = keras.Sequential([
        layers.Input(shape=input_shape),
        layers.LSTM(64, return_sequences=True, dropout=0.2),
        layers.LSTM(32, return_sequences=False, dropout=0.2),
        layers.Dense(16, activation='relu'),
        layers.Dropout(0.2),
        layers.Dense(1)
    ])
    
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='mse',
        metrics=['mae']
    )
    
    return model


def create_dl_sequences_from_features(train_df, feature_cols, sequence_length=6):
    """
    Feature ê¸°ë°˜ ì‹œí€€ìŠ¤ ìƒì„± (ê¸°ì¡´ featureë¥¼ ì‹œê³„ì—´ë¡œ ë³€í™˜)
    """
    if len(train_df) < sequence_length:
        return None, None
    
    # Pairë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì‹œí€€ìŠ¤ ìƒì„±
    sequences = []
    targets = []
    
    for (leading, following), group in train_df.groupby(['leading_item_id', 'following_item_id']):
        group = group.sort_values('date').reset_index(drop=True)
        
        if len(group) < sequence_length + 1:
            continue
        
        for i in range(sequence_length, len(group)):
            seq = group.iloc[i-sequence_length:i][feature_cols].values
            target = group.iloc[i]['f_target_value_next']
            
            if not np.isnan(target) and not np.any(np.isnan(seq)):
                sequences.append(seq)
                targets.append(target)
    
    if len(sequences) == 0:
        return None, None
    
    return np.array(sequences), np.array(targets)


def train_dl_model_on_features(X_seq, y_seq, sequence_length=6):
    """Feature ê¸°ë°˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ"""
    if not DL_AVAILABLE or X_seq is None:
        return None, None
    
    print("ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì¤‘...")
    
    # ìŠ¤ì¼€ì¼ë§
    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler()
    
    n_samples, seq_len, n_features = X_seq.shape
    X_flat = X_seq.reshape(-1, n_features)
    X_scaled_flat = scaler_X.fit_transform(X_flat)
    X_scaled = X_scaled_flat.reshape(n_samples, seq_len, n_features)
    
    y_scaled = scaler_y.fit_transform(y_seq.reshape(-1, 1)).flatten()
    
    # Train/Val split
    split_idx = int(len(X_scaled) * 0.8)
    X_train, X_val = X_scaled[:split_idx], X_scaled[split_idx:]
    y_train, y_val = y_scaled[:split_idx], y_scaled[split_idx:]
    
    # ëª¨ë¸ ìƒì„±
    input_shape = (sequence_length, n_features)
    model = build_lstm_model_for_pairs(input_shape)
    
    if model is None:
        return None, None
    
    # í•™ìŠµ
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=100,
        batch_size=32,
        verbose=0,
        callbacks=[
            keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True, monitor='val_loss'),
            keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5, monitor='val_loss')
        ]
    )
    
    val_loss = min(history.history['val_loss'])
    print(f"  ë”¥ëŸ¬ë‹ ëª¨ë¸ Val Loss: {val_loss:.4f}")
    
    return model, (scaler_X, scaler_y)


def predict_with_dl_model(model, scalers, X_seq):
    """ë”¥ëŸ¬ë‹ ëª¨ë¸ë¡œ ì˜ˆì¸¡"""
    if model is None or scalers is None or X_seq is None:
        return None
    
    scaler_X, scaler_y = scalers
    
    # ìŠ¤ì¼€ì¼ë§
    n_samples, seq_len, n_features = X_seq.shape
    X_flat = X_seq.reshape(-1, n_features)
    X_scaled_flat = scaler_X.transform(X_flat)
    X_scaled = X_scaled_flat.reshape(n_samples, seq_len, n_features)
    
    # ì˜ˆì¸¡
    y_pred_scaled = model.predict(X_scaled, verbose=0)
    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
    
    return y_pred


# ============================================================================
# ë©”ì¸ í•¨ìˆ˜ (improved_model_v2.py ê¸°ë°˜ + ë”¥ëŸ¬ë‹ ì¶”ê°€)
# ============================================================================

def main():
    """
    improved_model_v2.pyì˜ main í•¨ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë”¥ëŸ¬ë‹ ëª¨ë¸ ì¶”ê°€
    ì‹¤ì œ êµ¬í˜„ì€ improved_model_v2.pyë¥¼ importí•˜ê±°ë‚˜ í•¨ìˆ˜ë¥¼ ë³µì‚¬í•´ì•¼ í•¨
    """
    print("=" * 60)
    print("ë”¥ëŸ¬ë‹ ëª¨ë¸ í†µí•© ë²„ì „")
    print("=" * 60)
    print("\nğŸ’¡ ì‚¬ìš© ë°©ë²•:")
    print("1. improved_model_v2.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ í™•ì¸")
    print("2. ì´ íŒŒì¼ì˜ í•¨ìˆ˜ë“¤ì„ improved_model_v2.pyì— ì¶”ê°€")
    print("3. main() í•¨ìˆ˜ì—ì„œ ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ ì¶”ê°€")
    print("\nğŸ“Š ë”¥ëŸ¬ë‹ ëª¨ë¸ ì˜ˆìƒ ê°œì„  íš¨ê³¼:")
    print("  âœ… ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµ: +5-10%")
    print("  âœ… ì¥ê±°ë¦¬ ì˜ì¡´ì„±: +3-7%")
    print("  âœ… ë¹„ì„ í˜• ê´€ê³„: +2-5%")
    print("  âœ… ì´ ì˜ˆìƒ: +10-22%")
    print("\nâš ï¸ ì£¼ì˜ì‚¬í•­:")
    print("  - ë°ì´í„°ê°€ ì‘ìœ¼ë©´ ì˜¤ë²„í”¼íŒ… ìœ„í—˜")
    print("  - í•™ìŠµ ì‹œê°„ ì¦ê°€ (5-10ë¶„)")
    print("  - GPU ê¶Œì¥ (ì„ íƒì‚¬í•­)")
    print("\nğŸ”§ ì„¤ì¹˜ í•„ìš”:")
    print("  pip install tensorflow")
    print("=" * 60)


if __name__ == "__main__":
    main()


