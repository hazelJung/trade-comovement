{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47d4379f",
   "metadata": {},
   "source": [
    "# ë¬´ì—­ ë°ì´í„° EDA (HS4 / item_id ë‹¨ìœ„ EDA ë° micro-level deep dive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff11656",
   "metadata": {},
   "source": [
    "**ì½”ë“œ ì…€ ë™ì‘ ì„¤ëª…**\n",
    "\n",
    "- **ì£¼ìš” ë²”ì£¼**: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸, ë°ì´í„° ë¡œë“œ, ì‹œê°í™”(Seaborn), ìˆ˜ì¹˜/ë°ì´í„°í”„ë ˆì„ ì²˜ë¦¬\n",
    "- ë°ì´í„° íŒŒì¼ì„ ì½ì–´ DataFrameìœ¼ë¡œ ë¡œë“œ\n",
    "- ê·¸ë˜í”„/ì°¨íŠ¸ë¡œ ê²°ê³¼ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d49a81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     item_id  year  month  seq  type   hs4      weight  quantity       value\n",
      "0   DEWLVASR  2022      1  1.0     1  3038     14858.0       0.0     32688.0\n",
      "1   ELQGMQWE  2022      1  1.0     1  2002     62195.0       0.0    110617.0\n",
      "2   AHMDUILJ  2022      1  1.0     1  2102     18426.0       0.0     72766.0\n",
      "3   XIPPENFQ  2022      1  1.0     1  2501     20426.0       0.0     11172.0\n",
      "4   FTSVTTSR  2022      1  1.0     1  2529    248000.0       0.0    143004.0\n",
      "5   XMKRPGLB  2022      1  1.0     1  2529    558000.0       0.0    205676.0\n",
      "6   RCBZUSIM  2022      1  1.0     1  2805         0.0       0.0       459.0\n",
      "7   SUOYXCHP  2022      1  1.0     1  2805         0.0       0.0       482.0\n",
      "8   ZKENOUDA  2022      1  1.0     1  2805         2.0       0.0      2783.0\n",
      "9   WQMVCOEM  2022      1  1.0     1  2805       187.0       0.0    217777.0\n",
      "10  BSRMSVTC  2022      1  1.0     1  2805       111.0       0.0     33952.0\n",
      "11  DDEXPPXU  2022      1  1.0     1  2807       387.0       0.0     13158.0\n",
      "12  LLHREMKS  2022      1  1.0     1  2807       319.0       0.0      8705.0\n",
      "13  DNMPSKTB  2022      1  1.0     1  2811   1748972.0       0.0   3961879.0\n",
      "14  WPQXWHYO  2022      1  1.0     1  2811   2367757.0       0.0   4518277.0\n",
      "15  VWMBASNE  2022      1  1.0     1  2811     94038.0       0.0     48737.0\n",
      "16  RJGPVEXX  2022      1  1.0     1  2811     18029.0       0.0    256916.0\n",
      "17  ATLDMDBO  2022      1  1.0     1  2814  68878617.0       0.0  60548596.0\n",
      "18  SAHWCZNH  2022      1  1.0     1  2825     20305.0       0.0    505311.0\n",
      "19  LUENUFGA  2022      1  1.0     1  2833     54965.0       0.0    271528.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('train.csv')\n",
    "print(df_train.head(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb2c850",
   "metadata": {},
   "source": [
    "## 1. ì „ì²˜ë¦¬\n",
    "\n",
    "- zero / missing flag ìƒì„±:\n",
    "  - value, weight, quantityì˜ 0ì€ ì •ìƒì ì¼ ìˆ˜ ìˆìŒ ğŸ‘‰ ì œê±° X\n",
    "  - ëŒ€ì‹  zero íŒ¨í„´ ìì²´ë¥¼ featureë¡œ ì¸ì‹í•˜ê¸° ìœ„í•´ flag ìƒì„±.\n",
    "\n",
    "- ê²°ì¸¡ì¹˜ ì²˜ë¦¬:\n",
    "  - weight, quantity, value ê²°ì¸¡ì€ ë³´ê°„í•˜ì§€ ì•ŠìŒ (ì˜ë¯¸ ìˆëŠ” ê²°ì¸¡ì´ê¸° ë•Œë¬¸)\n",
    "\n",
    "- item_id / HS4 ë‹¨ìœ„ monthly pivot ìƒì„±\n",
    "  - â†’ Comovement ë¶„ì„ì˜ ê¸°ë³¸ ë‹¨ìœ„ë¡œ ì‚¬ìš©ë  ì‹œê³„ì—´ êµ¬ì¡° í™•ë³´."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aaacdf",
   "metadata": {},
   "source": [
    "**ì½”ë“œ ì…€ ë™ì‘ ì„¤ëª…**\n",
    "\n",
    "- **ì£¼ìš” ë²”ì£¼**: ê²°ì¸¡ì¹˜ í™•ì¸/ì²˜ë¦¬, íƒ€ì… ë³€í™˜/ë‚ ì§œ ì²˜ë¦¬\n",
    "- ê²°ì¸¡ì¹˜ ìƒíƒœë¥¼ í™•ì¸í•˜ê³  ì ì ˆíˆ ì œê±°/ëŒ€ì²´\n",
    "- ì»¬ëŸ¼ íƒ€ì… ë³€í™˜ ë˜ëŠ” ë‚ ì§œ íŒŒì‹± ìˆ˜í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84e59af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚­ì œë  í–‰ ìˆ˜: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ì‚­ì œí•  ì´ìƒì¹˜ ì¡°ê±´ ìƒì„±\n",
    "mask_outlier = (\n",
    "    (df_train[\"item_id\"] == \"QVLMOEYE\") &\n",
    "    (df_train[\"year\"] == 2023) &\n",
    "    (df_train[\"month\"] == 8) &\n",
    "    (df_train[\"hs4\"] == 4202) &\n",
    "    (df_train[\"weight\"] == 3197) &\n",
    "    (df_train[\"quantity\"] == 792598955) &\n",
    "    (df_train[\"value\"] == 660801)\n",
    ")\n",
    "\n",
    "print(\"ì‚­ì œë  í–‰ ìˆ˜:\", mask_outlier.sum())\n",
    "\n",
    "# ì´ìƒì¹˜ ì‚­ì œ\n",
    "df_train = df_train[~mask_outlier].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b734346",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_fix = (\n",
    "    (df_train[\"item_id\"] == \"WHPUAOID\") &\n",
    "    (df_train[\"year\"] == 2025) &\n",
    "    (df_train[\"month\"] == 1) &\n",
    "    (df_train[\"hs4\"] == 5512) &\n",
    "    (df_train[\"weight\"] == 1) &\n",
    "    (df_train[\"quantity\"] == 1) &\n",
    "    (df_train[\"value\"] == 0)\n",
    ")\n",
    "\n",
    "df_train.loc[mask_fix, [\"weight\", \"quantity\", \"value\"]] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df41f3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero flag ìƒì„±\n",
    "df_train['value_zero'] = (df_train['value'] == 0).astype(int)\n",
    "df_train['weight_zero'] = (df_train['weight'] == 0).astype(int)\n",
    "df_train['quantity_zero'] = (df_train['quantity'] == 0).astype(int)\n",
    "\n",
    "# Missing flag ìƒì„±\n",
    "df_train['value_missing'] = df_train['value'].isna().astype(int)\n",
    "df_train['weight_missing'] = df_train['weight'].isna().astype(int)\n",
    "df_train['quantity_missing'] = df_train['quantity'].isna().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd29a8f",
   "metadata": {},
   "source": [
    "**ì½”ë“œ ì…€ ë™ì‘ ì„¤ëª…**\n",
    "\n",
    "- **ì£¼ìš” ë²”ì£¼**: ì¼ë°˜ ì „ì²˜ë¦¬/ë¡œì§ ìˆ˜í–‰\n",
    "- ì£¼ìš” ì „ì²˜ë¦¬/ê³„ì‚°/ì¶œë ¥ ìˆ˜í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02ea9ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs2_map = {\n",
    "    \"01\": \"ì‚°ë™ë¬¼\",\n",
    "    \"02\": \"ìœ¡ê³¼ ì‹ìš©ì„¤ìœ¡\",\n",
    "    \"03\": \"ì–´íŒ¨ë¥˜\",\n",
    "    \"04\": \"ë‚™ë†í’ˆÂ·ì¡°ë€Â·ì²œì—°ë™ë¬¼ì„± ì‹í’ˆ\",\n",
    "    \"05\": \"ê¸°íƒ€ ë™ë¬¼ì„± ìƒì‚°í’ˆ\",\n",
    "    \"06\": \"ì‚°ìˆ˜ëª©Â·ê½ƒ\",\n",
    "    \"07\": \"ì±„ì†Œ\",\n",
    "    \"08\": \"ê³¼ì‹¤Â·ê²¬ê³¼ë¥˜\",\n",
    "    \"09\": \"ì»¤í”¼Â·í–¥ì‹ ë£Œ\",\n",
    "\n",
    "    \"10\": \"ê³¡ë¬¼\",\n",
    "    \"11\": \"ë°€ê°€ë£¨Â·ì „ë¶„\",\n",
    "    \"12\": \"ì±„ìœ ìš© ì¢…ìÂ·ì¸ì‚¼\",\n",
    "    \"13\": \"ì‹ë¬¼ì„± ì—‘ìŠ¤\",\n",
    "    \"14\": \"ê¸°íƒ€ ì‹ë¬¼ì„± ìƒì‚°í’ˆ\",\n",
    "    \"15\": \"ë™ì‹ë¬¼ì„± ìœ ì§€\",\n",
    "    \"16\": \"ìœ¡Â·ì–´ë¥˜ ì¡°ì œí’ˆ\",\n",
    "    \"17\": \"ë‹¹ë¥˜Â·ì„¤íƒ•ê³¼ì\",\n",
    "    \"18\": \"ì½”ì½”ì•„Â·ì´ˆì½œë¦¿\",\n",
    "    \"19\": \"ê³¡ë¬¼Â·ê³¡ë¶„ ì£¼ì œí’ˆÂ·ë¹µë¥˜\",\n",
    "\n",
    "    \"20\": \"ì±„ì†ŒÂ·ê³¼ì‹¤ì˜ ì¡°ì œí’ˆ\",\n",
    "    \"21\": \"ê¸°íƒ€ ì¡°ì œì‹ë£Œí’ˆ\",\n",
    "    \"22\": \"ìŒë£ŒÂ·ì£¼ë¥˜Â·ì‹ì´ˆ\",\n",
    "    \"23\": \"ì¡°ì œì‚¬ë£Œ\",\n",
    "    \"24\": \"ë‹´ë°°\",\n",
    "    \"25\": \"í† ì„ë¥˜Â·ì†Œê¸ˆ\",\n",
    "    \"26\": \"ê´‘Â·ìŠ¬ë™Â·íšŒ\",\n",
    "    \"27\": \"ê´‘ë¬¼ì„± ì—°ë£ŒÂ·ì—ë„ˆì§€\",\n",
    "    \"28\": \"ë¬´ê¸°í™”í•©ë¬¼\",\n",
    "    \"29\": \"ìœ ê¸°í™”í•©ë¬¼\",\n",
    "\n",
    "    \"30\": \"ì˜ë£Œìš©í’ˆ\",\n",
    "    \"31\": \"ë¹„ë£Œ\",\n",
    "    \"32\": \"ì—¼ë£ŒÂ·ì•ˆë£ŒÂ·í˜ì¸íŠ¸Â·ì‰í¬\",\n",
    "    \"33\": \"í–¥ë£ŒÂ·í™”ì¥í’ˆ\",\n",
    "    \"34\": \"ë¹„ëˆ„Â·ê³„ë©´í™œì„±ì œÂ·ì™ìŠ¤\",\n",
    "    \"35\": \"ì¹´ì„¸ì¸Â·ì•Œë¶€ë¯¼Â·ë³€ì„±ì „ë¶„Â·íš¨ì†Œ\",\n",
    "    \"36\": \"í™”ì•½ë¥˜Â·ì„±ëƒ¥\",\n",
    "    \"37\": \"í•„ë¦„Â·ì¸í™”ì§€Â·ì‚¬ì§„ìš© ì¬ë£Œ\",\n",
    "    \"38\": \"ê¸°íƒ€ í™”í•™ê³µì—… í’ˆëª©\",\n",
    "    \"39\": \"í”Œë¼ìŠ¤í‹±ê³¼ ê·¸ ì œí’ˆ\",\n",
    "\n",
    "    \"40\": \"ê³ ë¬´ì™€ ê·¸ ì œí’ˆ\",\n",
    "    \"41\": \"ì›í”¼Â·ê°€ì£½\",\n",
    "    \"42\": \"ê°€ì£½ì œí’ˆ\",\n",
    "    \"43\": \"ëª¨í”¼Â·ëª¨í”¼ì œí’ˆ\",\n",
    "    \"44\": \"ëª©ì¬Â·ëª©íƒ„\",\n",
    "    \"45\": \"ì½”ë¥´í¬Â·ì½”ë¥´í¬ ì œí’ˆ\",\n",
    "    \"46\": \"ì¡°ë¬¼ì¬ë£Œ ì œí’ˆ\",\n",
    "    \"47\": \"í„í”„\",\n",
    "    \"48\": \"ì§€Â·íŒì§€\",\n",
    "    \"49\": \"ì„œì Â·ì‹ ë¬¸Â·ì¸ì‡„ë¬¼\",\n",
    "\n",
    "    \"50\": \"ê²¬Â·ê²¬ì‚¬Â·ê²¬ì§ë¬¼\",\n",
    "    \"51\": \"ì–‘ëª¨Â·ìˆ˜ëª¨\",\n",
    "    \"52\": \"ë©´Â·ë©´ì‚¬Â·ë©´ì§ë¬¼\",\n",
    "    \"53\": \"ë§ˆë¥˜ì‚¬Â·ì§ë¬¼\",\n",
    "    \"54\": \"ì¸ì¡° í•„ë¼ë©˜íŠ¸ ì„¬ìœ \",\n",
    "    \"55\": \"ì¸ì¡° ìŠ¤í…Œì´í”Œ ì„¬ìœ \",\n",
    "    \"56\": \"ì›Œë”©Â·ë¶€ì§í¬\",\n",
    "    \"57\": \"ì–‘íƒ„ì\",\n",
    "    \"58\": \"íŠ¹ìˆ˜ ì§ë¬¼\",\n",
    "    \"59\": \"ì¹¨íˆ¬Â·ë„í¬ ì§ë¬¼\",\n",
    "\n",
    "    \"60\": \"í¸ë¬¼\",\n",
    "    \"61\": \"ì˜ë¥˜(í¸ë¬¼ì œ)\",\n",
    "    \"62\": \"ì˜ë¥˜(í¸ë¬¼ì œ ì™¸)\",\n",
    "    \"63\": \"ê¸°íƒ€ ì„¬ìœ ì œí’ˆÂ·ë„ë§ˆ\",\n",
    "    \"64\": \"ì‹ ë°œë¥˜\",\n",
    "    \"65\": \"ëª¨ìë¥˜\",\n",
    "    \"66\": \"ìš°ì‚°Â·ì§€íŒ¡ì´\",\n",
    "    \"67\": \"ì¡°ì œ ìš°ëª¨Â·ì¸ì¡°ì œí’ˆ\",\n",
    "    \"68\": \"ì„Â·ì‹œë©˜íŠ¸Â·ì„ë©´ ì œí’ˆ\",\n",
    "    \"69\": \"ë„ì ì œí’ˆ\",\n",
    "\n",
    "    \"70\": \"ìœ ë¦¬\",\n",
    "    \"71\": \"ê·€ì„Â·ë°˜ê·€ì„Â·ê·€ê¸ˆì†\",\n",
    "    \"72\": \"ì² ê°•\",\n",
    "    \"73\": \"ì² ê°•ì œí’ˆ\",\n",
    "    \"74\": \"ë™ê³¼ ê·¸ ì œí’ˆ\",\n",
    "    \"75\": \"ë‹ˆì¼ˆê³¼ ê·¸ ì œí’ˆ\",\n",
    "    \"76\": \"ì•Œë£¨ë¯¸ëŠ„ê³¼ ê·¸ ì œí’ˆ\",\n",
    "    \"78\": \"ì—°ê³¼ ê·¸ ì œí’ˆ\",\n",
    "    \"79\": \"ì•„ì—°ê³¼ ê·¸ ì œí’ˆ\",\n",
    "\n",
    "    \"80\": \"ì£¼ì„ê³¼ ê·¸ ì œí’ˆ\",\n",
    "    \"81\": \"ê¸°íƒ€ ë¹„ê¸ˆì†\",\n",
    "    \"82\": \"ë¹„ê¸ˆì† ê³µêµ¬Â·ìŠ¤í‘¼Â·í¬í¬\",\n",
    "    \"83\": \"ê°ì¢… ë¹„ê¸ˆì† ì œí’ˆ\",\n",
    "    \"84\": \"ë³´ì¼ëŸ¬Â·ê¸°ê³„ë¥˜\",\n",
    "    \"85\": \"ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR\",\n",
    "    \"86\": \"ì² ë„ì°¨ëŸ‰\",\n",
    "    \"87\": \"ì¼ë°˜ì°¨ëŸ‰\",\n",
    "    \"88\": \"í•­ê³µê¸°\",\n",
    "    \"89\": \"ì„ ë°•\",\n",
    "\n",
    "    \"90\": \"ê´‘í•™Â·ì˜ë£ŒÂ·ì¸¡ì •Â·ì •ë°€ê¸°ê¸°\",\n",
    "    \"91\": \"ì‹œê³„\",\n",
    "    \"92\": \"ì•…ê¸°\",\n",
    "    \"93\": \"ë¬´ê¸°\",\n",
    "    \"94\": \"ê°€êµ¬Â·ì¡°ëª…ê¸°êµ¬\",\n",
    "    \"95\": \"ì™„êµ¬Â·ìš´ë™ìš©í’ˆ\",\n",
    "    \"96\": \"ì¡í’ˆ\",\n",
    "    \"97\": \"ì˜ˆìˆ í’ˆÂ·ê³¨ë™í’ˆ\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29310924",
   "metadata": {},
   "source": [
    "### ëª¨ë“  item_id Ã— ëª¨ë“  month íŒ¨ë„ ì™„ì„±(Panel Completion)\n",
    "\n",
    "í˜„ì¬ ë°ì´í„°ëŠ” ëª¨ë“  item_idê°€ ëª¨ë“  ì›”ì— ë“±ì¥í•˜ì§€ ì•ŠìŒ â†’ ê³µí–‰ì„± ë¶„ì„ì—ì„œ í° ë¬¸ì œë¨.\n",
    "ë™ì‹œì— ì¡´ì¬í•˜ëŠ” ì‹œê³„ì—´ë¼ë¦¬ ë¹„êµí•´ì•¼ í•¨\n",
    "\n",
    "ì˜ˆ:\n",
    "- A itemì€ 2022~2025 ì „ ê¸°ê°„ ì¡´ì¬\n",
    "- B itemì€ íŠ¹ì • ê³„ì ˆë§Œ ì¡´ì¬\n",
    "- C itemì€ ì¤‘ê°„ì— êµ¬ë© ìˆìŒ\n",
    "\n",
    "ì´ëŸ° ìƒíƒœì—ì„œ comovement correlation ê³„ì‚°í•˜ë©´ ì‹œê³„ì—´ ê¸¸ì´ê°€ ë¶ˆì¼ì¹˜ â†’ ì˜¤ë¥˜ / ì™œê³¡ ë°œìƒ, ìƒê´€ê³„ìˆ˜ ê³„ì‚° ë¶ˆê°€<br>\n",
    "- A : 2022.01 ~ 2025.07 ëª¨ë“  ë‹¬ì— ì¡´ì¬ = 42ê°œì›”\n",
    "- C : 2022.03 ~ 2023.05 ì¼ë¶€ ë‹¬ì— ì¡´ì¬ = 15ê°œì›”\n",
    "â†’ ë°˜ë“œì‹œ í•´ì•¼ í•˜ëŠ” ì „ì²˜ë¦¬:<br>\n",
    "ëª¨ë“  item_id Ã— ëª¨ë“  month ì¡°í•©ì„ ë§Œë“¤ê³  ì—†ëŠ” rowëŠ” 0 ë˜ëŠ” NaNìœ¼ë¡œ ì±„ìš°ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe061e2",
   "metadata": {},
   "source": [
    "âœ… 1ë‹¨ê³„: df_trainì— date ì»¬ëŸ¼ ë¶™ì´ê¸°\n",
    "\n",
    "ì•„ë˜ ì…€ì„ í†µì§¸ë¡œ ìƒˆë¡œ ì‹¤í–‰í•´ ì¤˜:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319cec9f",
   "metadata": {},
   "source": [
    "âœ… 2ë‹¨ê³„: panel(ëª¨ë“  item_id Ã— ëª¨ë“  month) ë§Œë“¤ê¸°\n",
    "\n",
    "ì´ì œ dateê°€ ìƒê²¼ë‹¤ê³  ê°€ì •í•˜ê³ , ì•„ë˜ ì…€ ì‹¤í–‰ ğŸ‘‡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3c7786",
   "metadata": {},
   "source": [
    "**ì½”ë“œ ì…€ ë™ì‘ ì„¤ëª…**\n",
    "\n",
    "- **ì£¼ìš” ë²”ì£¼**: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸, ê²°ì¸¡ì¹˜ í™•ì¸/ì²˜ë¦¬, ì¤‘ë³µ ë°ì´í„° ì²˜ë¦¬, íƒ€ì… ë³€í™˜/ë‚ ì§œ ì²˜ë¦¬, ì§‘ê³„/ê·¸ë£¹ ì—°ì‚°, ë°ì´í„° ê²°í•©/ë³‘í•©, ì •ë ¬, ìˆ˜ì¹˜/ë°ì´í„°í”„ë ˆì„ ì²˜ë¦¬\n",
    "- ê²°ì¸¡ì¹˜ ìƒíƒœë¥¼ í™•ì¸í•˜ê³  ì ì ˆíˆ ì œê±°/ëŒ€ì²´\n",
    "- ì¤‘ë³µ í–‰ì„ íƒì§€í•˜ì—¬ ì œê±°\n",
    "- ì»¬ëŸ¼ íƒ€ì… ë³€í™˜ ë˜ëŠ” ë‚ ì§œ íŒŒì‹± ìˆ˜í–‰\n",
    "- ê·¸ë£¹ ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„/í†µê³„ ê³„ì‚°\n",
    "- í‚¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ë³‘í•© ë˜ëŠ” ì´ì–´ë¶™ì´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9775034c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›”ë³„ ì§‘ê³„ ê²°ê³¼ ì˜ˆì‹œ:\n",
      "    item_id   hs4       date    value    weight  quantity\n",
      "0  AANGBULD  4810 2022-01-01  14276.0   17625.0       0.0\n",
      "1  AANGBULD  4810 2022-02-01  52347.0   67983.0       0.0\n",
      "2  AANGBULD  4810 2022-03-01  53549.0   69544.0       0.0\n",
      "3  AANGBULD  4810 2022-05-01  26997.0   34173.0       0.0\n",
      "4  AANGBULD  4810 2022-06-01  84489.0  103666.0       0.0\n",
      "ìµœì¢… íŒ¨ë„ ì˜ˆì‹œ:\n",
      "    item_id       date     hs4    value   weight  quantity  year  month\n",
      "0  AANGBULD 2022-01-01  4810.0  14276.0  17625.0       0.0  2022      1\n",
      "1  AANGBULD 2022-02-01  4810.0  52347.0  67983.0       0.0  2022      2\n",
      "2  AANGBULD 2022-03-01  4810.0  53549.0  69544.0       0.0  2022      3\n",
      "3  AANGBULD 2022-04-01     NaN      0.0      0.0       0.0  2022      4\n",
      "4  AANGBULD 2022-05-01  4810.0  26997.0  34173.0       0.0  2022      5\n",
      "ì¤‘ë³µ row ê°œìˆ˜: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------\n",
    "# 0) date ì»¬ëŸ¼ ë§Œë“¤ê¸° (year, month ê¸°ì¤€)\n",
    "# ------------------------------------\n",
    "# year, monthê°€ ìˆê³  dateê°€ ì•„ì§ ì—†ì„ ë•Œë§Œ ìƒì„±\n",
    "if \"date\" not in df_train.columns:\n",
    "    df_train[\"date\"] = pd.to_datetime(\n",
    "        df_train[\"year\"].astype(int).astype(str) + \"-\" +\n",
    "        df_train[\"month\"].astype(int).astype(str) + \"-01\"\n",
    "    )\n",
    "\n",
    "# í˜¹ì‹œ dateê°€ object/stringì¸ ê²½ìš° datetimeìœ¼ë¡œ ë³€í™˜\n",
    "df_train[\"date\"] = pd.to_datetime(df_train[\"date\"])\n",
    "\n",
    "# ------------------------------------\n",
    "# 1) ì›”ë³„ ì§‘ê³„ (ì¤‘ë³µê±°ë˜ ì œê±° í•µì‹¬)\n",
    "#    item_id + hs4 + date ê¸°ì¤€ìœ¼ë¡œ value/weight/quantity í•©ì‚°\n",
    "# ------------------------------------\n",
    "agg_cols = [\"value\", \"weight\", \"quantity\"]\n",
    "\n",
    "df_train_monthly = (\n",
    "    df_train\n",
    "    .groupby([\"item_id\", \"hs4\", \"date\"], as_index=False)[agg_cols]\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "print(\"ì›”ë³„ ì§‘ê³„ ê²°ê³¼ ì˜ˆì‹œ:\")\n",
    "print(df_train_monthly.head())\n",
    "\n",
    "# ------------------------------------\n",
    "# 2) ì „ì²´ month ë²”ìœ„ & item_id ëª©ë¡ ë§Œë“¤ê¸°\n",
    "# ------------------------------------\n",
    "min_month = df_train_monthly[\"date\"].min()\n",
    "max_month = df_train_monthly[\"date\"].max()\n",
    "\n",
    "full_months = pd.date_range(min_month, max_month, freq=\"MS\")\n",
    "all_items = df_train_monthly[\"item_id\"].unique()\n",
    "\n",
    "# ------------------------------------\n",
    "# 3) item_id Ã— date ì „ì²´ ì¡°í•© (í’€ íŒ¨ë„ ì¸ë±ìŠ¤)\n",
    "# ------------------------------------\n",
    "panel = pd.MultiIndex.from_product(\n",
    "    [all_items, full_months],\n",
    "    names=[\"item_id\", \"date\"]\n",
    ").to_frame(index=False)\n",
    "\n",
    "# ------------------------------------\n",
    "# 4) ì§‘ê³„ëœ df_train_monthlyì™€ mergeí•´ì„œ íŒ¨ë„ ì™„ì„±\n",
    "# ------------------------------------\n",
    "df_panel = (\n",
    "    panel.merge(df_train_monthly, on=[\"item_id\", \"date\"], how=\"left\")\n",
    "         .sort_values([\"item_id\", \"date\"])\n",
    "         .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ê±°ë˜ ì—†ë˜ month â†’ NaNì„ 0ìœ¼ë¡œ ì±„ìš°ê¸° (ì›í•˜ë©´)\n",
    "for c in agg_cols:\n",
    "    df_panel[c] = df_panel[c].fillna(0)\n",
    "\n",
    "# ì—°/ì›” ì»¬ëŸ¼ ë‹¤ì‹œ ìƒì„±\n",
    "df_panel[\"year\"] = df_panel[\"date\"].dt.year\n",
    "df_panel[\"month\"] = df_panel[\"date\"].dt.month\n",
    "\n",
    "print(\"ìµœì¢… íŒ¨ë„ ì˜ˆì‹œ:\")\n",
    "print(df_panel.head())\n",
    "\n",
    "# ì¤‘ë³µ ì²´í¬ (item_id, date ì¡°í•©ì´ ìœ ì¼í•œì§€)\n",
    "print(\"ì¤‘ë³µ row ê°œìˆ˜:\", df_panel.duplicated([\"item_id\", \"date\"]).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9003029c",
   "metadata": {},
   "source": [
    "**ì½”ë“œ ì…€ ë™ì‘ ì„¤ëª…**\n",
    "\n",
    "- **ì£¼ìš” ë²”ì£¼**: ì¤‘ë³µ ë°ì´í„° ì²˜ë¦¬\n",
    "- ì¤‘ë³µ í–‰ì„ íƒì§€í•˜ì—¬ ì œê±°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16b474c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_panel.duplicated([\"item_id\", \"date\"]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37907fcd",
   "metadata": {},
   "source": [
    "**ì½”ë“œ ì…€ ë™ì‘ ì„¤ëª…**\n",
    "\n",
    "- **ì£¼ìš” ë²”ì£¼**: íƒ€ì… ë³€í™˜/ë‚ ì§œ ì²˜ë¦¬\n",
    "- ì»¬ëŸ¼ íƒ€ì… ë³€í™˜ ë˜ëŠ” ë‚ ì§œ íŒŒì‹± ìˆ˜í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8256ede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_panel[\"hs2\"] = df_panel[\"hs4\"].astype(str).str[:2]\n",
    "df_panel[\"hs2_name_kr\"] = df_panel[\"hs2\"].map(hs2_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8a5181",
   "metadata": {},
   "source": [
    "**ì½”ë“œ ì…€ ë™ì‘ ì„¤ëª…**\n",
    "\n",
    "- **ì£¼ìš” ë²”ì£¼**: ê²°ì¸¡ì¹˜ í™•ì¸/ì²˜ë¦¬, ì¤‘ë³µ ë°ì´í„° ì²˜ë¦¬\n",
    "- ê²°ì¸¡ì¹˜ ìƒíƒœë¥¼ í™•ì¸í•˜ê³  ì ì ˆíˆ ì œê±°/ëŒ€ì²´\n",
    "- ì¤‘ë³µ í–‰ì„ íƒì§€í•˜ì—¬ ì œê±°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df196168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>date</th>\n",
       "      <th>hs4</th>\n",
       "      <th>value</th>\n",
       "      <th>weight</th>\n",
       "      <th>quantity</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>hs2</th>\n",
       "      <th>hs2_name_kr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AANGBULD</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>4810.0</td>\n",
       "      <td>14276.0</td>\n",
       "      <td>17625.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>ì§€Â·íŒì§€</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AANGBULD</td>\n",
       "      <td>2022-02-01</td>\n",
       "      <td>4810.0</td>\n",
       "      <td>52347.0</td>\n",
       "      <td>67983.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>ì§€Â·íŒì§€</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AANGBULD</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>4810.0</td>\n",
       "      <td>53549.0</td>\n",
       "      <td>69544.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>ì§€Â·íŒì§€</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AANGBULD</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>na</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AANGBULD</td>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>4810.0</td>\n",
       "      <td>26997.0</td>\n",
       "      <td>34173.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>5</td>\n",
       "      <td>48</td>\n",
       "      <td>ì§€Â·íŒì§€</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    item_id       date     hs4    value   weight  quantity  year  month hs2  \\\n",
       "0  AANGBULD 2022-01-01  4810.0  14276.0  17625.0       0.0  2022      1  48   \n",
       "1  AANGBULD 2022-02-01  4810.0  52347.0  67983.0       0.0  2022      2  48   \n",
       "2  AANGBULD 2022-03-01  4810.0  53549.0  69544.0       0.0  2022      3  48   \n",
       "3  AANGBULD 2022-04-01     NaN      0.0      0.0       0.0  2022      4  na   \n",
       "4  AANGBULD 2022-05-01  4810.0  26997.0  34173.0       0.0  2022      5  48   \n",
       "\n",
       "  hs2_name_kr  \n",
       "0        ì§€Â·íŒì§€  \n",
       "1        ì§€Â·íŒì§€  \n",
       "2        ì§€Â·íŒì§€  \n",
       "3         NaN  \n",
       "4        ì§€Â·íŒì§€  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_panel.isnull().sum()\n",
    "#df_panel[df_panel['seq'].isnull()]\n",
    "\n",
    "#df_panel.duplicated().sum()\n",
    "df_panel.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f6249e",
   "metadata": {},
   "source": [
    "**ì½”ë“œ ì…€ ë™ì‘ ì„¤ëª…**\n",
    "\n",
    "- **ì£¼ìš” ë²”ì£¼**: ê²°ì¸¡ì¹˜ í™•ì¸/ì²˜ë¦¬, íƒ€ì… ë³€í™˜/ë‚ ì§œ ì²˜ë¦¬\n",
    "- ê²°ì¸¡ì¹˜ ìƒíƒœë¥¼ í™•ì¸í•˜ê³  ì ì ˆíˆ ì œê±°/ëŒ€ì²´\n",
    "- ì»¬ëŸ¼ íƒ€ì… ë³€í™˜ ë˜ëŠ” ë‚ ì§œ íŒŒì‹± ìˆ˜í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be52ac73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>date</th>\n",
       "      <th>hs4</th>\n",
       "      <th>value</th>\n",
       "      <th>weight</th>\n",
       "      <th>quantity</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>hs2</th>\n",
       "      <th>hs2_name_kr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>9782469.0</td>\n",
       "      <td>579404.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2022-02-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>321176.0</td>\n",
       "      <td>50685.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>7106218.0</td>\n",
       "      <td>416214.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>17850488.0</td>\n",
       "      <td>854306.0</td>\n",
       "      <td>550.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>41768869.0</td>\n",
       "      <td>3025992.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>5</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>11677001.0</td>\n",
       "      <td>698063.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>6</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2022-07-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>8420099.0</td>\n",
       "      <td>630011.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>7</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2022-08-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>14228993.0</td>\n",
       "      <td>794488.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>8</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2022-09-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>19845420.0</td>\n",
       "      <td>741487.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2022-10-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>9518385.0</td>\n",
       "      <td>378466.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>10</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2022-11-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>20314921.0</td>\n",
       "      <td>1260025.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>11</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2022-12-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>11812561.0</td>\n",
       "      <td>937764.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>16388548.0</td>\n",
       "      <td>944818.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2023-02-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>9102966.0</td>\n",
       "      <td>511767.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>8181016.0</td>\n",
       "      <td>571626.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2023-04-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>9827335.0</td>\n",
       "      <td>523999.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>15867038.0</td>\n",
       "      <td>720398.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2023-06-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>10334498.0</td>\n",
       "      <td>470966.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>6</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>14850522.0</td>\n",
       "      <td>521111.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2023-08-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>10820301.0</td>\n",
       "      <td>548021.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2023-09-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>34920929.0</td>\n",
       "      <td>1123072.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>9</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2023-10-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>6521580.0</td>\n",
       "      <td>400881.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>10</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>10355472.0</td>\n",
       "      <td>691348.0</td>\n",
       "      <td>599.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>11</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>6060400.0</td>\n",
       "      <td>293904.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>4692422.0</td>\n",
       "      <td>383889.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2024-02-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>27507229.0</td>\n",
       "      <td>3277770.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>2</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>10981651.0</td>\n",
       "      <td>623858.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>903389.0</td>\n",
       "      <td>107384.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>4</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>5236372.0</td>\n",
       "      <td>383287.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>5</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>5552958.0</td>\n",
       "      <td>313712.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>6</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>4421417.0</td>\n",
       "      <td>299765.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>6307638.0</td>\n",
       "      <td>305071.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>8</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2024-09-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>3483715.0</td>\n",
       "      <td>278315.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>9</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>30656299.0</td>\n",
       "      <td>2534148.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>10</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2024-11-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>2230362.0</td>\n",
       "      <td>216657.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>46880140.0</td>\n",
       "      <td>2957081.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>12</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>5953747.0</td>\n",
       "      <td>376755.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2025-02-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>8770756.0</td>\n",
       "      <td>436897.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>2</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>42543999.0</td>\n",
       "      <td>3036628.0</td>\n",
       "      <td>327.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1286</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>55809611.0</td>\n",
       "      <td>4778019.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>4</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1287</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>3961657.0</td>\n",
       "      <td>181518.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1288</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>3697189.0</td>\n",
       "      <td>233379.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>6</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>GKQIJYDH</td>\n",
       "      <td>2025-07-01</td>\n",
       "      <td>8501.0</td>\n",
       "      <td>11621444.0</td>\n",
       "      <td>663956.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>7</td>\n",
       "      <td>85</td>\n",
       "      <td>ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       item_id       date     hs4       value     weight  quantity  year  \\\n",
       "1247  GKQIJYDH 2022-01-01  8501.0   9782469.0   579404.0      68.0  2022   \n",
       "1248  GKQIJYDH 2022-02-01  8501.0    321176.0    50685.0      31.0  2022   \n",
       "1249  GKQIJYDH 2022-03-01  8501.0   7106218.0   416214.0      68.0  2022   \n",
       "1250  GKQIJYDH 2022-04-01  8501.0  17850488.0   854306.0     550.0  2022   \n",
       "1251  GKQIJYDH 2022-05-01  8501.0  41768869.0  3025992.0     181.0  2022   \n",
       "1252  GKQIJYDH 2022-06-01  8501.0  11677001.0   698063.0     125.0  2022   \n",
       "1253  GKQIJYDH 2022-07-01  8501.0   8420099.0   630011.0      88.0  2022   \n",
       "1254  GKQIJYDH 2022-08-01  8501.0  14228993.0   794488.0     108.0  2022   \n",
       "1255  GKQIJYDH 2022-09-01  8501.0  19845420.0   741487.0     110.0  2022   \n",
       "1256  GKQIJYDH 2022-10-01  8501.0   9518385.0   378466.0     123.0  2022   \n",
       "1257  GKQIJYDH 2022-11-01  8501.0  20314921.0  1260025.0      71.0  2022   \n",
       "1258  GKQIJYDH 2022-12-01  8501.0  11812561.0   937764.0     102.0  2022   \n",
       "1259  GKQIJYDH 2023-01-01  8501.0  16388548.0   944818.0      58.0  2023   \n",
       "1260  GKQIJYDH 2023-02-01  8501.0   9102966.0   511767.0     103.0  2023   \n",
       "1261  GKQIJYDH 2023-03-01  8501.0   8181016.0   571626.0     110.0  2023   \n",
       "1262  GKQIJYDH 2023-04-01  8501.0   9827335.0   523999.0      73.0  2023   \n",
       "1263  GKQIJYDH 2023-05-01  8501.0  15867038.0   720398.0     133.0  2023   \n",
       "1264  GKQIJYDH 2023-06-01  8501.0  10334498.0   470966.0      77.0  2023   \n",
       "1265  GKQIJYDH 2023-07-01  8501.0  14850522.0   521111.0     122.0  2023   \n",
       "1266  GKQIJYDH 2023-08-01  8501.0  10820301.0   548021.0     119.0  2023   \n",
       "1267  GKQIJYDH 2023-09-01  8501.0  34920929.0  1123072.0     282.0  2023   \n",
       "1268  GKQIJYDH 2023-10-01  8501.0   6521580.0   400881.0     121.0  2023   \n",
       "1269  GKQIJYDH 2023-11-01  8501.0  10355472.0   691348.0     599.0  2023   \n",
       "1270  GKQIJYDH 2023-12-01  8501.0   6060400.0   293904.0     112.0  2023   \n",
       "1271  GKQIJYDH 2024-01-01  8501.0   4692422.0   383889.0     120.0  2024   \n",
       "1272  GKQIJYDH 2024-02-01  8501.0  27507229.0  3277770.0      93.0  2024   \n",
       "1273  GKQIJYDH 2024-03-01  8501.0  10981651.0   623858.0     100.0  2024   \n",
       "1274  GKQIJYDH 2024-04-01  8501.0    903389.0   107384.0      64.0  2024   \n",
       "1275  GKQIJYDH 2024-05-01  8501.0   5236372.0   383287.0     198.0  2024   \n",
       "1276  GKQIJYDH 2024-06-01  8501.0   5552958.0   313712.0      57.0  2024   \n",
       "1277  GKQIJYDH 2024-07-01  8501.0   4421417.0   299765.0     224.0  2024   \n",
       "1278  GKQIJYDH 2024-08-01  8501.0   6307638.0   305071.0     269.0  2024   \n",
       "1279  GKQIJYDH 2024-09-01  8501.0   3483715.0   278315.0      53.0  2024   \n",
       "1280  GKQIJYDH 2024-10-01  8501.0  30656299.0  2534148.0     137.0  2024   \n",
       "1281  GKQIJYDH 2024-11-01  8501.0   2230362.0   216657.0      86.0  2024   \n",
       "1282  GKQIJYDH 2024-12-01  8501.0  46880140.0  2957081.0     143.0  2024   \n",
       "1283  GKQIJYDH 2025-01-01  8501.0   5953747.0   376755.0      70.0  2025   \n",
       "1284  GKQIJYDH 2025-02-01  8501.0   8770756.0   436897.0     101.0  2025   \n",
       "1285  GKQIJYDH 2025-03-01  8501.0  42543999.0  3036628.0     327.0  2025   \n",
       "1286  GKQIJYDH 2025-04-01  8501.0  55809611.0  4778019.0     183.0  2025   \n",
       "1287  GKQIJYDH 2025-05-01  8501.0   3961657.0   181518.0      92.0  2025   \n",
       "1288  GKQIJYDH 2025-06-01  8501.0   3697189.0   233379.0      83.0  2025   \n",
       "1289  GKQIJYDH 2025-07-01  8501.0  11621444.0   663956.0      95.0  2025   \n",
       "\n",
       "      month hs2  hs2_name_kr  \n",
       "1247      1  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1248      2  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1249      3  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1250      4  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1251      5  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1252      6  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1253      7  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1254      8  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1255      9  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1256     10  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1257     11  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1258     12  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1259      1  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1260      2  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1261      3  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1262      4  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1263      5  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1264      6  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1265      7  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1266      8  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1267      9  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1268     10  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1269     11  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1270     12  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1271      1  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1272      2  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1273      3  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1274      4  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1275      5  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1276      6  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1277      7  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1278      8  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1279      9  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1280     10  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1281     11  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1282     12  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1283      1  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1284      2  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1285      3  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1286      4  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1287      5  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1288      6  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  \n",
       "1289      7  85  ì „ê¸°ê¸°ê¸°Â·TVÂ·VTR  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ê±°ë˜ëŸ‰ ê´€ë ¨ ìˆ˜ì¹˜ ì»¬ëŸ¼\n",
    "value_cols = ['value', 'weight', 'quantity']\n",
    "\n",
    "# ì—†ëŠ” ì›”ì€ ê±°ë˜ 0ìœ¼ë¡œ (ë„ˆ í”„ë¡œì íŠ¸ ì„¤ì •ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥)\n",
    "df_panel[value_cols] = df_panel[value_cols].fillna(0)\n",
    "\n",
    "# zero / missing flagê°€ ì´ë¯¸ df_trainì— ìˆë‹¤ë©´ ê°™ì´ ì±„ì›Œì£¼ê¸°\n",
    "flag_cols = [\n",
    "    'value_zero', 'weight_zero', 'quantity_zero',\n",
    "    'value_missing', 'weight_missing', 'quantity_missing'\n",
    "]\n",
    "\n",
    "for c in flag_cols:\n",
    "    if c in df_panel.columns:\n",
    "        df_panel[c] = df_panel[c].fillna(0).astype(int)\n",
    "\n",
    "df_panel.head()\n",
    "df_panel[df_panel['item_id'] == 'GKQIJYDH']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94783e5",
   "metadata": {},
   "source": [
    "## 2. HS4 / item_id êµ°ì§‘ì„± ë¶„ì„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98d8e78",
   "metadata": {},
   "source": [
    "**ì½”ë“œ ì…€ ë™ì‘ ì„¤ëª…**\n",
    "\n",
    "- **ì£¼ìš” ë²”ì£¼**: ì§‘ê³„/ê·¸ë£¹ ì—°ì‚°, ì •ë ¬\n",
    "- ê·¸ë£¹ ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„/í†µê³„ ê³„ì‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "009f57e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hs4</th>\n",
       "      <th>value</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2814.0</td>\n",
       "      <td>5.999737e+07</td>\n",
       "      <td>1.022512e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>6211.0</td>\n",
       "      <td>2.516274e+07</td>\n",
       "      <td>8.551477e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3824.0</td>\n",
       "      <td>2.459375e+07</td>\n",
       "      <td>8.139476e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3102.0</td>\n",
       "      <td>1.754666e+07</td>\n",
       "      <td>3.233097e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>9403.0</td>\n",
       "      <td>1.721897e+07</td>\n",
       "      <td>6.029486e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>8505.0</td>\n",
       "      <td>1.216691e+07</td>\n",
       "      <td>3.952844e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3815.0</td>\n",
       "      <td>1.050937e+07</td>\n",
       "      <td>9.385711e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>8479.0</td>\n",
       "      <td>6.980014e+06</td>\n",
       "      <td>3.525974e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2836.0</td>\n",
       "      <td>6.607692e+06</td>\n",
       "      <td>2.160214e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>8501.0</td>\n",
       "      <td>6.485218e+06</td>\n",
       "      <td>4.014716e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       hs4         value        weight\n",
       "11  2814.0  5.999737e+07  1.022512e+08\n",
       "54  6211.0  2.516274e+07  8.551477e+05\n",
       "32  3824.0  2.459375e+07  8.139476e+06\n",
       "23  3102.0  1.754666e+07  3.233097e+07\n",
       "70  9403.0  1.721897e+07  6.029486e+06\n",
       "65  8505.0  1.216691e+07  3.952844e+05\n",
       "31  3815.0  1.050937e+07  9.385711e+05\n",
       "63  8479.0  6.980014e+06  3.525974e+05\n",
       "14  2836.0  6.607692e+06  2.160214e+07\n",
       "64  8501.0  6.485218e+06  4.014716e+05"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_panel.copy() \n",
    "\n",
    "hs4_summary = (\n",
    "    df.groupby(\"hs4\")[[\"value\", \"weight\"]]\n",
    "      .mean()\n",
    "      .reset_index()\n",
    "      .sort_values(\"value\", ascending=False)\n",
    ")\n",
    "\n",
    "hs4_summary.head(10)\n",
    "# ì–´ë–¤ HS4 í’ˆëª©êµ°ì˜ í‰ê·  ê±°ë˜ê¸ˆì•¡ê³¼ í‰ê·  ì¤‘ëŸ‰ì´ ë†’ì€ì§€ íŒŒì•…\n",
    "# í’ˆëª©êµ°ì˜ ê²½ì œì  ê·œëª¨ ì´í•´"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858e93eb",
   "metadata": {},
   "source": [
    "**ì½”ë“œ ì…€ ë™ì‘ ì„¤ëª…**\n",
    "\n",
    "- **ì£¼ìš” ë²”ì£¼**: ì§‘ê³„/ê·¸ë£¹ ì—°ì‚°, ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½/ì¶”ê°€, ì •ë ¬\n",
    "- ê·¸ë£¹ ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„/í†µê³„ ê³„ì‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3593448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hs4</th>\n",
       "      <th>quantity_usage_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>8527.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>8467.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>9022.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4403.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4202.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4302.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2710.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>8461.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>5512.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>6101.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       hs4  quantity_usage_rate\n",
       "66  8527.0                  1.0\n",
       "62  8467.0                  1.0\n",
       "69  9022.0                  1.0\n",
       "37  4403.0                  1.0\n",
       "35  4202.0                  1.0\n",
       "36  4302.0                  1.0\n",
       "7   2710.0                  1.0\n",
       "61  8461.0                  1.0\n",
       "47  5512.0                  1.0\n",
       "53  6101.0                  1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "hs4_qty_rate = (\n",
    "    df.assign(qty_valid=(df[\"quantity\"] > 0))\n",
    "      .groupby(\"hs4\")[\"qty_valid\"]\n",
    "      .mean()\n",
    "      .reset_index(name=\"quantity_usage_rate\")\n",
    "      .sort_values(\"quantity_usage_rate\", ascending=False)\n",
    ")\n",
    "\n",
    "hs4_qty_rate.head(10)\n",
    "# ì–´ë–¤ HS4ëŠ” quantityê°€ ìì£¼ ê¸°ë¡ë˜ì§€ë§Œ\n",
    "# ì–´ë–¤ HS4ëŠ” quantityê°€ ê±°ì˜ 0 ë˜ëŠ” ëˆ„ë½\n",
    "# í’ˆëª©êµ° íŠ¹ì„± íŒŒì•…, ëª¨ë¸ë§ ì‹œ quantityë¥¼ featureë¡œ ì‚¬ìš©í• ì§€ íŒë‹¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447baac9",
   "metadata": {},
   "source": [
    "**ì½”ë“œ ì…€ ë™ì‘ ì„¤ëª…**\n",
    "\n",
    "- **ì£¼ìš” ë²”ì£¼**: ì§‘ê³„/ê·¸ë£¹ ì—°ì‚°, ì •ë ¬\n",
    "- ê·¸ë£¹ ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„/í†µê³„ ê³„ì‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f17043c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(     item_id         value\n",
       " 21  FCYBOAXC  4.671997e+09\n",
       " 4   ATLDMDBO  2.579887e+09\n",
       " 31  GYHKIVQT  1.456760e+09\n",
       " 12  CCLHWFWF  1.081998e+09\n",
       " 74  STZDBITS  1.057606e+09\n",
       " 45  LRVGFDFM  8.040184e+08\n",
       " 11  BUZIIBYG  7.404158e+08\n",
       " 10  BTMOEMEP  7.049946e+08\n",
       " 29  GKQIJYDH  6.070892e+08\n",
       " 72  SDWAYPIK  4.381489e+08\n",
       " 88  WPQXWHYO  4.093971e+08\n",
       " 42  KJNSOAHR  3.001406e+08\n",
       " 17  DNMPSKTB  2.888946e+08\n",
       " 33  HXYSSRXE  2.841308e+08\n",
       " 48  LUENUFGA  2.756382e+08\n",
       " 37  JPBRUTWP  2.664403e+08\n",
       " 61  QRKRBYJL  2.214571e+08\n",
       " 6   BEZYMBBT  2.123930e+08\n",
       " 20  EVBVXETX  1.975996e+08\n",
       " 94  XUOIQPFL  1.960189e+08,\n",
       "      item_id     value\n",
       " 14  DDEXPPXU  820507.0\n",
       " 38  JSLXRQOK  651539.0\n",
       " 34  IGDVVKUD  606705.0\n",
       " 8   BLANHGYY  541289.0\n",
       " 65  RCBZUSIM  540481.0\n",
       " 58  PYZMVUWD  530103.0\n",
       " 99  ZXERAXWP  485789.0\n",
       " 2   ANWUJOKX  350309.0\n",
       " 36  JERHKLYW  334445.0\n",
       " 22  FDXPMYGF  219744.0\n",
       " 51  NAQIHUKZ  202508.0\n",
       " 27  FWUCPMMW   97723.0\n",
       " 95  YSYHGLQK   95838.0\n",
       " 28  GIKPEWTY   94461.0\n",
       " 16  DJBLNPNC   85293.0\n",
       " 57  PLMZALFA    5385.0\n",
       " 76  TANNMIMB    4338.0\n",
       " 62  QSDCUCLB    2793.0\n",
       " 30  GMBFCMIU    1130.0\n",
       " 69  RUVXNNVA     278.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_volume = (\n",
    "    df.groupby(\"item_id\")[\"value\"]\n",
    "      .sum()\n",
    "      .reset_index()\n",
    "      .sort_values(\"value\", ascending=False)\n",
    ")\n",
    "\n",
    "top_items = item_volume.head(20)\n",
    "bottom_items = item_volume.tail(20)\n",
    "\n",
    "top_items, bottom_items\n",
    "\n",
    "# ê°€ì¥ ê±°ë˜ê·œëª¨ê°€ í° í’ˆëª©ê³¼ ì‘ì€ í’ˆëª©\n",
    "# ì˜ˆì¸¡ ìš°ì„ ìˆœìœ„ ì„ ì •, êµ°ì§‘ë¶„ì„ ì‹œ ì¤‘ìš”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db90163",
   "metadata": {},
   "source": [
    "**ì½”ë“œ ì…€ ë™ì‘ ì„¤ëª…**\n",
    "\n",
    "- **ì£¼ìš” ë²”ì£¼**: ì§‘ê³„/ê·¸ë£¹ ì—°ì‚°, ì •ë ¬\n",
    "- ê·¸ë£¹ ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„/í†µê³„ ê³„ì‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d322264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>month_covered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AANGBULD</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AHMDUILJ</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ANWUJOKX</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>APQGTRMF</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATLDMDBO</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    item_id  month_covered\n",
       "0  AANGBULD             43\n",
       "1  AHMDUILJ             43\n",
       "2  ANWUJOKX             43\n",
       "3  APQGTRMF             43\n",
       "4  ATLDMDBO             43"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_month_coverage = (\n",
    "    df.groupby(\"item_id\")[\"date\"]\n",
    "      .nunique()\n",
    "      .reset_index(name=\"month_covered\")\n",
    "      .sort_values(\"month_covered\", ascending=False)\n",
    ")\n",
    "\n",
    "item_month_coverage.head()\n",
    "# ëª¨ë“  item_idê°€ ëª¨ë“  ì›”ì— ë“±ì¥í•˜ëŠ”ì§€ í™•ì¸\n",
    "# ëª¨ë“  ì›”ì— ë“±ì¥í•˜ëŠ” item_idë§Œ ëª¨ë¸ë§ì— í™œìš©\n",
    "\n",
    "#item_month_coverage.tail()\n",
    "# ëª¨ë“  ì›”ì— ë“±ì¥í•˜ì§€ ì•ŠëŠ” item_idëŠ” ëª¨ë¸ë§ì—ì„œ ì œì™¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d16576c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 104\u001b[39m\n\u001b[32m     91\u001b[39m     y = pd.Series(y_values, index=feat_pair_df.index)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_pair_model_v2\u001b[39m(\n\u001b[32m     97\u001b[39m     feat_df: pd.DataFrame,\n\u001b[32m     98\u001b[39m     candidates_df: pd.DataFrame,\n\u001b[32m     99\u001b[39m     train_last_t: pd.Timestamp,\n\u001b[32m    100\u001b[39m     valid_last_t: pd.Timestamp,\n\u001b[32m    101\u001b[39m     n_estimators: \u001b[38;5;28mint\u001b[39m = \u001b[32m100\u001b[39m,\n\u001b[32m    102\u001b[39m     max_depth: \u001b[38;5;28mint\u001b[39m = \u001b[32m10\u001b[39m,\n\u001b[32m    103\u001b[39m     random_state: \u001b[38;5;28mint\u001b[39m = \u001b[32m42\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[43mRandomForestRegressor\u001b[49m, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[32m    105\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[33;03m    Pair-level RandomForest ëª¨ë¸ í•™ìŠµ.\u001b[39;00m\n\u001b[32m    107\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    115\u001b[39m \u001b[33;03m      - feature_cols: ì‚¬ìš©ëœ í”¼ì²˜ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    117\u001b[39m     df = feat_df.copy()\n",
      "\u001b[31mNameError\u001b[39m: name 'RandomForestRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 3) Pair-level RandomForest ëª¨ë¸ í•™ìŠµ\n",
    "# -------------------------------\n",
    "\n",
    "def build_pair_features_for_training(\n",
    "    feat_df: pd.DataFrame,\n",
    "    candidates_df: pd.DataFrame,\n",
    "    target_month: pd.Timestamp,\n",
    "    feature_cols: list[str],\n",
    ") -> tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    target_month = t+1 (ì˜ˆ: 2025-08-01)\n",
    "    t ì‹œì  featureë¡œ t+1ì˜ following_item valueë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ X, yë¥¼ ìƒì„±.\n",
    "    \"\"\"\n",
    "    df = feat_df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    base = df.set_index([\"item_id\", \"date\"])\n",
    "\n",
    "    # Seriesë‚˜ dict ëª¨ë‘ ì²˜ë¦¬í•˜ëŠ” í—¬í¼ í•¨ìˆ˜\n",
    "    def safe_get(data, key, default=np.nan):\n",
    "        if hasattr(data, 'get'):\n",
    "            return data.get(key, default)\n",
    "        elif hasattr(data, '__getitem__'):\n",
    "            try:\n",
    "                return data[key]\n",
    "            except (KeyError, IndexError):\n",
    "                return default\n",
    "        else:\n",
    "            return default\n",
    "\n",
    "    rows = []\n",
    "    y_values = []\n",
    "\n",
    "    for _, pair in candidates_df.iterrows():\n",
    "        try:\n",
    "            lead = pair[\"leading_item_id\"]\n",
    "            foll = pair[\"following_item_id\"]\n",
    "            t = target_month - pd.DateOffset(months=1)  # t = target_month - 1\n",
    "\n",
    "            # Noneì´ë‚˜ NaN ì²´í¬\n",
    "            if pd.isna(lead) or pd.isna(foll):\n",
    "                continue\n",
    "\n",
    "            if (foll, t) not in base.index or (lead, t) not in base.index:\n",
    "                continue\n",
    "            if (foll, target_month) not in base.index:\n",
    "                continue\n",
    "\n",
    "            foll_t = base.loc[(foll, t)]\n",
    "            lead_t = base.loc[(lead, t)]\n",
    "            foll_t1 = base.loc[(foll, target_month)]  # t+1ì˜ following value\n",
    "\n",
    "            row = {\n",
    "                \"leading_item_id\": lead,\n",
    "                \"following_item_id\": foll,\n",
    "                \"date_t\": t,\n",
    "                \"year\": safe_get(foll_t, \"year\"),\n",
    "                \"month\": safe_get(foll_t, \"month\"),\n",
    "                \"trend_index\": safe_get(foll_t, \"trend_index\"),\n",
    "            }\n",
    "\n",
    "            # feature_colsì— í¬í•¨ëœ í”¼ì²˜ë“¤ì„ pair í˜•íƒœë¡œ ë³€í™˜\n",
    "            for col in feature_cols:\n",
    "                try:\n",
    "                    if col.startswith(\"f_\"):\n",
    "                        base_name = col[2:]\n",
    "                        row[col] = safe_get(foll_t, base_name)\n",
    "                    elif col.startswith(\"l_\"):\n",
    "                        base_name = col[2:]\n",
    "                        row[col] = safe_get(lead_t, base_name)\n",
    "                    elif col in [\"year\", \"month\", \"trend_index\", \"month_sin\", \"month_cos\"]:\n",
    "                        # ê³µìœ  í”¼ì²˜ëŠ” following ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©\n",
    "                        if col not in row:\n",
    "                            row[col] = safe_get(foll_t, col)\n",
    "                except Exception:\n",
    "                    row[col] = np.nan\n",
    "\n",
    "            rows.append(row)\n",
    "            # log_valueë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš© (predict_aug_2025_values_v2ì™€ ì¼ì¹˜)\n",
    "            log_value_t1 = safe_get(foll_t1, \"log_value\", np.nan)\n",
    "            if pd.isna(log_value_t1):\n",
    "                # log_valueê°€ ì—†ìœ¼ë©´ valueì—ì„œ ê³„ì‚°\n",
    "                value_t1 = safe_get(foll_t1, \"value\", 0)\n",
    "                log_value_t1 = np.log1p(max(0, float(value_t1)))\n",
    "            y_values.append(log_value_t1)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    feat_pair_df = pd.DataFrame(rows)\n",
    "    X = feat_pair_df[feature_cols].copy()\n",
    "    y = pd.Series(y_values, index=feat_pair_df.index)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def train_pair_model_v2(\n",
    "    feat_df: pd.DataFrame,\n",
    "    candidates_df: pd.DataFrame,\n",
    "    train_last_t: pd.Timestamp,\n",
    "    valid_last_t: pd.Timestamp,\n",
    "    n_estimators: int = 100,\n",
    "    max_depth: int = 10,\n",
    "    random_state: int = 42,\n",
    ") -> tuple[RandomForestRegressor, list[str]]:\n",
    "    \"\"\"\n",
    "    Pair-level RandomForest ëª¨ë¸ í•™ìŠµ.\n",
    "    \n",
    "    - feat_df: add_ts_core_featuresê¹Œì§€ ì ìš©ëœ í”¼ì²˜ íŒ¨ë„\n",
    "    - candidates_df: ê³µí–‰ì„± í›„ë³´ìŒ\n",
    "    - train_last_t: í•™ìŠµì— ì‚¬ìš©í•  ë§ˆì§€ë§‰ date\n",
    "    - valid_last_t: ê²€ì¦ìš© ë§ˆì§€ë§‰ date (í˜„ì¬ëŠ” ë¯¸ì‚¬ìš©)\n",
    "    \n",
    "    ë°˜í™˜:\n",
    "      - model: í•™ìŠµëœ RandomForestRegressor\n",
    "      - feature_cols: ì‚¬ìš©ëœ í”¼ì²˜ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    df = feat_df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "\n",
    "    # í•™ìŠµ ê¸°ê°„ì˜ ëª¨ë“  ì›” ì¶”ì¶œ\n",
    "    train_dates = sorted(df[df[\"date\"] <= train_last_t][\"date\"].unique())\n",
    "    \n",
    "    if len(train_dates) < 2:\n",
    "        raise ValueError(\"í•™ìŠµì— ì‚¬ìš©í•  ì¶©ë¶„í•œ ë‚ ì§œê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    # í”¼ì²˜ ì»¬ëŸ¼ ì •ì˜: followingê³¼ leadingì˜ í”¼ì²˜ë¥¼ êµ¬ë¶„\n",
    "    # í™•ì¥ëœ í”¼ì²˜ ë¦¬ìŠ¤íŠ¸ (ë” ë§ì€ ì •ë³´ í™œìš©)\n",
    "    base_features = [\n",
    "        \"log_value\", \"log_weight\", \"log_quantity\",\n",
    "        \"log_value_lag1\", \"log_value_lag2\", \"log_value_lag3\", \"log_value_lag6\", \"log_value_lag12\",\n",
    "        \"log_weight_lag1\", \"log_weight_lag2\", \"log_weight_lag3\",\n",
    "        \"log_roll_mean_3\", \"log_roll_mean_6\", \"log_roll_mean_12\",\n",
    "        \"log_roll_std_3\", \"log_roll_std_6\", \"log_roll_std_12\",\n",
    "        \"log_roll_min_3\", \"log_roll_max_3\",\n",
    "        \"log_value_yoy\", \"log_value_yoy_pct\",\n",
    "        \"value_ema_3\", \"value_ema_6\", \"value_ema_12\",\n",
    "        \"hs4_value_mean_3m\", \"hs4_value_mean_6m\", \"hs4_value_mean_12m\",\n",
    "        \"month_sin\", \"month_cos\",\n",
    "        \"year\", \"month\", \"trend_index\"\n",
    "    ]\n",
    "    \n",
    "    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í”¼ì²˜ë§Œ ì‚¬ìš©\n",
    "    available_features = [f for f in base_features if f in df.columns]\n",
    "    \n",
    "    # Pair í”¼ì²˜: f_ (following), l_ (leading)\n",
    "    # ë‹¨ì¼ í”¼ì²˜ (year, month, trend_index, month_sin, month_cos)ëŠ” ê³µìœ \n",
    "    feature_cols = []\n",
    "    for feat in available_features:\n",
    "        if feat not in [\"year\", \"month\", \"trend_index\", \"month_sin\", \"month_cos\"]:\n",
    "            feature_cols.append(f\"f_{feat}\")\n",
    "            feature_cols.append(f\"l_{feat}\")\n",
    "        else:\n",
    "            feature_cols.append(feat)\n",
    "\n",
    "    # í•™ìŠµ ë°ì´í„° ìƒì„±: ê° ì›”ë³„ë¡œ pair featureì™€ ë‹¤ìŒ ë‹¬ valueë¥¼ ìˆ˜ì§‘\n",
    "    X_train_list = []\n",
    "    y_train_list = []\n",
    "    failed_dates = []\n",
    "\n",
    "    for i in range(len(train_dates) - 1):\n",
    "        t = train_dates[i]\n",
    "        t_next = train_dates[i + 1]\n",
    "        \n",
    "        try:\n",
    "            X_t, y_t = build_pair_features_for_training(\n",
    "                df, candidates_df, t_next, feature_cols\n",
    "            )\n",
    "            if len(X_t) > 0:\n",
    "                X_train_list.append(X_t)\n",
    "                y_train_list.append(y_t)\n",
    "            else:\n",
    "                failed_dates.append((t, t_next, \"ë¹ˆ ë°ì´í„°\"))\n",
    "        except Exception as e:\n",
    "            failed_dates.append((t, t_next, str(e)))\n",
    "            continue\n",
    "\n",
    "    if len(X_train_list) == 0:\n",
    "        error_msg = \"í•™ìŠµ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\\n\"\n",
    "        error_msg += f\"   - ì‹œë„í•œ ë‚ ì§œ ìŒ ìˆ˜: {len(train_dates) - 1}\\n\"\n",
    "        error_msg += f\"   - ì‹¤íŒ¨í•œ ë‚ ì§œ ìŒ ìˆ˜: {len(failed_dates)}\\n\"\n",
    "        if len(failed_dates) > 0:\n",
    "            error_msg += f\"   - ì²˜ìŒ 5ê°œ ì‹¤íŒ¨ ì‚¬ë¡€:\\n\"\n",
    "            for t, t_next, reason in failed_dates[:5]:\n",
    "                error_msg += f\"     * {t} -> {t_next}: {reason}\\n\"\n",
    "        error_msg += \"\\ní•´ê²° ë°©ë²•:\\n\"\n",
    "        error_msg += \"   1. candidates_dfì— ìœ íš¨í•œ ìŒì´ ìˆëŠ”ì§€ í™•ì¸\\n\"\n",
    "        error_msg += \"   2. feat_dfì— í•´ë‹¹ ë‚ ì§œì˜ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸\\n\"\n",
    "        error_msg += \"   3. train_last_të¥¼ ë” ì´ë¥¸ ë‚ ì§œë¡œ ì¡°ì •\\n\"\n",
    "        raise ValueError(error_msg)\n",
    "    \n",
    "    if len(failed_dates) > 0:\n",
    "        print(f\"âš ï¸ {len(failed_dates)}ê°œ ë‚ ì§œ ìŒì—ì„œ ë°ì´í„° ìƒì„± ì‹¤íŒ¨ (ë¬´ì‹œë¨)\")\n",
    "\n",
    "    X_train = pd.concat(X_train_list, axis=0).reset_index(drop=True)\n",
    "    y_train = pd.concat(y_train_list, axis=0).reset_index(drop=True)\n",
    "\n",
    "    # NaN ì²˜ë¦¬ - y_trainì˜ NaNì€ ì œê±° (log_valueê°€ ì—†ìœ¼ë©´ í•™ìŠµ ë¶ˆê°€)\n",
    "    # X_trainì˜ NaNì€ 0ìœ¼ë¡œ ì±„ìš°ë˜, y_trainì´ ìœ íš¨í•œ ìƒ˜í”Œë§Œ ì‚¬ìš©\n",
    "    valid_mask = ~(y_train.isna() | (y_train <= 0))\n",
    "    \n",
    "    if valid_mask.sum() == 0:\n",
    "        raise ValueError(\"ìœ íš¨í•œ í•™ìŠµ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤! (ëª¨ë“  y_trainì´ NaNì´ê±°ë‚˜ 0 ì´í•˜)\")\n",
    "    \n",
    "    X_train = X_train[valid_mask].fillna(0)\n",
    "    y_train = y_train[valid_mask]\n",
    "    \n",
    "    print(f\"ğŸ“Š í•™ìŠµ ë°ì´í„°: {len(X_train)}ê°œ ìƒ˜í”Œ (NaN/0 ì´í•˜ ì œê±° í›„)\")\n",
    "    print(f\"   y_train ë²”ìœ„: {y_train.min():.4f} ~ {y_train.max():.4f}\")\n",
    "    print(f\"   y_train í‰ê· : {y_train.mean():.4f}, ì¤‘ì•™ê°’: {y_train.median():.4f}\")\n",
    "\n",
    "    # RandomForest í•™ìŠµ - ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=5,      # ê³¼ì í•© ë°©ì§€\n",
    "        min_samples_leaf=2,       # ê³¼ì í•© ë°©ì§€\n",
    "        max_features='sqrt',      # í”¼ì²˜ ìƒ˜í”Œë§ìœ¼ë¡œ ë‹¤ì–‘ì„± ì¦ê°€\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # í”¼ì²˜ ì¤‘ìš”ë„ ì¶œë ¥ (ìƒìœ„ 10ê°œ)\n",
    "    if len(feature_cols) > 0 and len(X_train) > 0:\n",
    "        try:\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': feature_cols,\n",
    "                'importance': model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            print(f\"ğŸ“Š Top 10 ì¤‘ìš” í”¼ì²˜:\")\n",
    "            print(feature_importance.head(10).to_string(index=False))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return model, feature_cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad411e40",
   "metadata": {},
   "source": [
    "**ì½”ë“œ ì…€ ë™ì‘ ì„¤ëª…**\n",
    "\n",
    "- **ì£¼ìš” ë²”ì£¼**: ì‹œê°í™”/í”Œë¡¯ ìƒì„±, ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸, ì‹œê°í™”(Seaborn)\n",
    "- ê·¸ë˜í”„/ì°¨íŠ¸ë¡œ ê²°ê³¼ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6abffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HS4 ë¶„í¬ ì‹œê°í™”\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.countplot(data=df, x=\"hs4\", order=df[\"hs4\"].value_counts().index)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"HS4 Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed4f035",
   "metadata": {},
   "source": [
    "**ì½”ë“œ ì…€ ë™ì‘ ì„¤ëª…**\n",
    "\n",
    "- **ì£¼ìš” ë²”ì£¼**: ì‹œê°í™”/í”Œë¡¯ ìƒì„±, ì‹œê°í™”(Seaborn)\n",
    "- ê·¸ë˜í”„/ì°¨íŠ¸ë¡œ ê²°ê³¼ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d664e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_id ë¶„í¬ ì‹œê°í™”\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "sns.countplot(data=df, x=\"item_id\", order=df[\"item_id\"].value_counts().index[:30])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Top 30 Item ID Count Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfad98d",
   "metadata": {},
   "source": [
    "## 3. item_id Micro-level Deep Dive\n",
    "\n",
    "- item_id 5~10ê°œ ëœë¤ ì¶”ì¶œ\n",
    "- ì›”ë³„ value / weight / quantity ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„\n",
    "- zero íŒ¨í„´(0 ë¹„ìœ¨) ë¶„ì„\n",
    "- ê²°ì¸¡ íŒ¨í„´(NaN ë¹„ìœ¨) ë¶„ì„\n",
    "- ê¸‰ì¦/ê¸‰ê°(anomaly) íƒì§€\n",
    "- HS ì½”ë“œ cross-check (item_idì— ì—¬ëŸ¬ HS4ê°€ ì¡´ì¬í•˜ëŠ”ì§€)\n",
    "- ê° item_idë³„ ìƒì„¸ ê·¸ë˜í”„ ìë™ ìƒì„±(ë©€í‹° í”Œë¡¯)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f92d53",
   "metadata": {},
   "source": [
    "**ì½”ë“œ ì…€ ë™ì‘ ì„¤ëª…**\n",
    "\n",
    "- **ì£¼ìš” ë²”ì£¼**: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸, ì§‘ê³„/ê·¸ë£¹ ì—°ì‚°, ìˆ˜ì¹˜/ë°ì´í„°í”„ë ˆì„ ì²˜ë¦¬\n",
    "- ê·¸ë£¹ ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„/í†µê³„ ê³„ì‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a029a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# df_panel ê¸°ì¤€ ì‚¬ìš©\n",
    "df = df_panel.copy()\n",
    "\n",
    "# 8ê°œ ëœë¤ sample\n",
    "sample_items = np.random.choice(df['item_id'].unique(), size=8, replace=False)\n",
    "sample_df = df[df['item_id'].isin(sample_items)]\n",
    "\n",
    "monthly_trend = (\n",
    "    sample_df.groupby(['item_id', 'date'])\n",
    "             [['value', 'weight', 'quantity']]\n",
    "             .sum()\n",
    "             .reset_index()\n",
    ")\n",
    "\n",
    "monthly_trend.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeb0026",
   "metadata": {},
   "source": [
    "**ì½”ë“œ ì…€ ë™ì‘ ì„¤ëª…**\n",
    "\n",
    "- **ì£¼ìš” ë²”ì£¼**: ì‹œê°í™”/í”Œë¡¯ ìƒì„±\n",
    "- ê·¸ë˜í”„/ì°¨íŠ¸ë¡œ ê²°ê³¼ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9803478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8ê°œ item value ë³€í™” ì‹œê³„ì—´ ê·¸ë˜í”„\n",
    "plt.figure(figsize=(14,6))\n",
    "for item in sample_items:\n",
    "    tmp = monthly_trend[monthly_trend['item_id'] == item]\n",
    "    plt.plot(tmp['date'], tmp['value'], label=item)\n",
    "\n",
    "plt.title(\"Monthly Value Trend by Sample Item ID\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "monthly_trend[monthly_trend['item_id'] == 'GKQIJYDH']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31469039",
   "metadata": {},
   "source": [
    "**ì½”ë“œ ì…€ ë™ì‘ ì„¤ëª…**\n",
    "\n",
    "- **ì£¼ìš” ë²”ì£¼**: ì¤‘ë³µ ë°ì´í„° ì²˜ë¦¬\n",
    "- ì¤‘ë³µ í–‰ì„ íƒì§€í•˜ì—¬ ì œê±°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750daf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b800e746",
   "metadata": {},
   "source": [
    "**ì½”ë“œ ì…€ ë™ì‘ ì„¤ëª…**\n",
    "\n",
    "- **ì£¼ìš” ë²”ì£¼**: ì§‘ê³„/ê·¸ë£¹ ì—°ì‚°\n",
    "- ê·¸ë£¹ ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„/í†µê³„ ê³„ì‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f6e42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero / Missing íŒ¨í„´ ë¶„ì„\n",
    "# ìµœì†Œ ìˆ˜ì •: í•„ìš”í•œ flag ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¦‰ì„ ìƒì„±, sample_items ë¯¸ì •ì˜ ì‹œ ê°„ë‹¨íˆ ì •ì˜\n",
    "if 'sample_items' not in globals():\n",
    "    sample_items = df['item_id'].dropna().unique()[:8]\n",
    "\n",
    "flag_sources = [\n",
    "    ('value_zero', 'value'),\n",
    "    ('weight_zero', 'weight'),\n",
    "    ('quantity_zero', 'quantity'),\n",
    "]\n",
    "for flag, base in flag_sources:\n",
    "    if flag not in df.columns and base in df.columns:\n",
    "        df[flag] = (df[base] == 0).astype(int)\n",
    "\n",
    "missing_sources = [\n",
    "    ('value_missing', 'value'),\n",
    "    ('weight_missing', 'weight'),\n",
    "    ('quantity_missing', 'quantity'),\n",
    "]\n",
    "for flag, base in missing_sources:\n",
    "    if flag not in df.columns and base in df.columns:\n",
    "        df[flag] = df[base].isna().astype(int)\n",
    "\n",
    "zero_missing_stats = (\n",
    "    df[df['item_id'].isin(sample_items)]\n",
    "    .groupby('item_id')[['value_zero', 'weight_zero', 'quantity_zero',\n",
    "                         'value_missing', 'weight_missing', 'quantity_missing']]\n",
    "    .mean(numeric_only=True)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "zero_missing_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc17b19b",
   "metadata": {},
   "source": [
    "**ì½”ë“œ ì…€ ë™ì‘ ì„¤ëª…**\n",
    "\n",
    "- **ì£¼ìš” ë²”ì£¼**: ì§‘ê³„/ê·¸ë£¹ ì—°ì‚°, ì •ë ¬\n",
    "- ê·¸ë£¹ ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„/í†µê³„ ê³„ì‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4423fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ê¸‰ì¦/ê¸‰ê° íŒ¨í„´(anomaly) íƒì§€\n",
    "monthly_trend = monthly_trend.sort_values(['item_id', 'date'])\n",
    "\n",
    "# ì „ì›” ëŒ€ë¹„ ë³€í™”ìœ¨\n",
    "monthly_trend['value_pct_change'] = (\n",
    "    monthly_trend.groupby('item_id')['value'].pct_change()\n",
    ")\n",
    "\n",
    "# ê¸‰ì¦ top 5, ê¸‰ê° bottom 5\n",
    "top_spikes = monthly_trend.sort_values('value_pct_change', ascending=False).head(5)\n",
    "bottom_spikes = monthly_trend.sort_values('value_pct_change', ascending=True).head(5)\n",
    "\n",
    "top_spikes, bottom_spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d370f632",
   "metadata": {},
   "source": [
    "**ì½”ë“œ ì…€ ë™ì‘ ì„¤ëª…**\n",
    "\n",
    "- **ì£¼ìš” ë²”ì£¼**: ì§‘ê³„/ê·¸ë£¹ ì—°ì‚°\n",
    "- ê·¸ë£¹ ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„/í†µê³„ ê³„ì‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41087ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HS ì½”ë“œ cross-check (item_id -> hs4 mapping consistency)\n",
    "item_hs_check = (\n",
    "    df.groupby('item_id')['hs4']\n",
    "      .nunique()\n",
    "      .reset_index(name='hs4_count')\n",
    ")\n",
    "\n",
    "# HS4ê°€ ì—¬ëŸ¬ ê°œ ì¡´ì¬í•˜ëŠ” itemë§Œ ì¶”ì¶œ\n",
    "item_hs_issue = item_hs_check[item_hs_check['hs4_count'] > 1]\n",
    "\n",
    "item_hs_issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e9e3aa",
   "metadata": {},
   "source": [
    "**ì½”ë“œ ì…€ ë™ì‘ ì„¤ëª…**\n",
    "\n",
    "- **ì£¼ìš” ë²”ì£¼**: ì¤‘ë³µ ë°ì´í„° ì²˜ë¦¬\n",
    "- ì¤‘ë³µ í–‰ì„ íƒì§€í•˜ì—¬ ì œê±°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4800db65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bc42b4",
   "metadata": {},
   "source": [
    "**ì½”ë“œ ì…€ ë™ì‘ ì„¤ëª…**\n",
    "\n",
    "- **ì£¼ìš” ë²”ì£¼**: ì¤‘ë³µ ë°ì´í„° ì²˜ë¦¬, í–‰/ì—´ ì„ íƒ ë° í•„í„°ë§\n",
    "- ì¤‘ë³µ í–‰ì„ íƒì§€í•˜ì—¬ ì œê±°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b75a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "(item_hs_check['hs4_count'] > 1).sum()\n",
    "df[df['item_id'] == df['item_id'].iloc[0]][['item_id','hs4']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48990ab",
   "metadata": {},
   "source": [
    "**ì½”ë“œ ì…€ ë™ì‘ ì„¤ëª…**\n",
    "\n",
    "- **ì£¼ìš” ë²”ì£¼**: ì‹œê°í™”/í”Œë¡¯ ìƒì„±\n",
    "- ê·¸ë˜í”„/ì°¨íŠ¸ë¡œ ê²°ê³¼ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf54e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê° item_idë³„ ìƒì„¸ ê·¸ë˜í”„ ìë™ ìƒì„± (value/weight/quantity)\n",
    "for item in sample_items:\n",
    "    tmp = monthly_trend[monthly_trend['item_id'] == item]\n",
    "\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "    axes[0].plot(tmp['date'], tmp['value'])\n",
    "    axes[0].set_title(f\"{item} - Value Trend\")\n",
    "\n",
    "    axes[1].plot(tmp['date'], tmp['weight'])\n",
    "    axes[1].set_title(f\"{item} - Weight Trend\")\n",
    "\n",
    "    axes[2].plot(tmp['date'], tmp['quantity'])\n",
    "    axes[2].set_title(f\"{item} - Quantity Trend\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c455bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44088e24",
   "metadata": {},
   "source": [
    "## ê³µí–‰ì„± í›„ë³´ìŒ ìƒì„± íŒŒì´í”„ë¼ì¸ ì½”ë“œ (v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bfa4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 0. ê¸°ë³¸ ì¤€ë¹„: df_panelì—ì„œ log_value í”¼ë²— ë§Œë“¤ê¸°\n",
    "# ----------------------------------------------------\n",
    "def build_value_matrix(df_panel: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    df_panel: (item_id, date, value ...)ê°€ ìˆëŠ” íŒ¨ë„ ë°ì´í„°í”„ë ˆì„\n",
    "    return: index=date, columns=item_id, values=log1p(value)ì¸ ë§¤íŠ¸ë¦­ìŠ¤\n",
    "    \"\"\"\n",
    "    df = df_panel.copy()\n",
    "\n",
    "    # í•„ìˆ˜ ì»¬ëŸ¼ ì²´í¬\n",
    "    required = {\"item_id\", \"date\", \"value\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"df_panelì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing}\")\n",
    "\n",
    "    # ì •ë ¬ ë° íƒ€ì… ì •ë¦¬\n",
    "    df = df.sort_values([\"date\", \"item_id\"])\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
    "\n",
    "    # ìŒìˆ˜ valueëŠ” 0ìœ¼ë¡œ í´ë¦¬í•‘ (log1p ì•ˆì •ì„±)\n",
    "    df[\"value\"] = df[\"value\"].clip(lower=0)\n",
    "\n",
    "    # log1p ë³€í™˜ + ë¹„ìœ í•œ/ë¹„ìˆ˜ì¹˜ ê°’ NaN ì²˜ë¦¬\n",
    "    df[\"log_value\"] = np.log1p(df[\"value\"].astype(float))\n",
    "    df.loc[~np.isfinite(df[\"log_value\"]), \"log_value\"] = np.nan\n",
    "\n",
    "    # í”¼ë²—: date Ã— item_id\n",
    "    value_mat = (\n",
    "        df.pivot(index=\"date\", columns=\"item_id\", values=\"log_value\")\n",
    "          .sort_index()\n",
    "    )\n",
    "    return value_mat\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1. ì•„ì´í…œ í•„í„°ë§ (sparse / low-variance ì œê±°)\n",
    "# ----------------------------------------------------\n",
    "def filter_items(value_mat: pd.DataFrame,\n",
    "                 min_nonzero_months: int = 8,\n",
    "                 min_std: float = 0.2) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ê³µí–‰ì„± ê³„ì‚°ì— ì“¸ itemë§Œ í•„í„°ë§\n",
    "    - ì¼ì • ê°œìˆ˜ ì´ìƒ ê±°ë˜ê°€ ìˆëŠ” ì•„ì´í…œ\n",
    "    - log_value ë³€ë™ì„±ì´ ë„ˆë¬´ ì‘ì€ ì•„ì´í…œ ì œì™¸\n",
    "    \"\"\"\n",
    "    value_nonzero_cnt = (value_mat > 0).sum(axis=0)\n",
    "    value_std = value_mat.std(axis=0, skipna=True)\n",
    "\n",
    "    valid_items = value_mat.columns[\n",
    "        (value_nonzero_cnt >= min_nonzero_months) &\n",
    "        (value_std >= min_std)\n",
    "    ]\n",
    "\n",
    "    return value_mat[valid_items]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2. ë§ˆì¼“ ì¸ë±ìŠ¤ ì œê±° â†’ residual matrix\n",
    "# ----------------------------------------------------\n",
    "def compute_market_residuals(value_mat: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ì „ì²´ ì‹œì¥ value í•©ê³„ë¥¼ ë§ˆì¼“ ì¸ë±ìŠ¤ë¡œ ë³´ê³ ,\n",
    "    ê° itemì˜ log_valueì—ì„œ ë‹¨ìˆœ íšŒê·€ë¡œ ì‹œì¥ íš¨ê³¼ë¥¼ ì œê±°í•œ residualì„ ë°˜í™˜\n",
    "    \"\"\"\n",
    "    # ì „ì²´ ì‹œì¥ ì¸ë±ìŠ¤: ì›”ë³„ total value (log1p)\n",
    "    # value_matëŠ” ì´ë¯¸ log_valueì´ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ê·¸ëƒ¥ í‰ê· /í•© ì¤‘ í•˜ë‚˜ë¥¼ ì¨ë„ ë¨\n",
    "    # ì—¬ê¸°ì„œëŠ” \"ëª¨ë“  ì•„ì´í…œ log_valueì˜ í‰ê· \"ì„ ì‹œì¥ ì¸ë±ìŠ¤ë¡œ ì‚¬ìš©\n",
    "    market_index = value_mat.mean(axis=1, skipna=True)\n",
    "\n",
    "    # ê³µí†µ ê¸°ê°„ align\n",
    "    common_index = value_mat.index.intersection(market_index.index)\n",
    "    X = market_index.loc[common_index].values  # shape: (T,)\n",
    "    Y = value_mat.loc[common_index]\n",
    "\n",
    "    resid_mat = pd.DataFrame(index=common_index, columns=Y.columns, dtype=float)\n",
    "\n",
    "    for col in Y.columns:\n",
    "        y = Y[col].values\n",
    "        mask = ~np.isnan(y) & ~np.isnan(X)\n",
    "\n",
    "        if mask.sum() < 5:\n",
    "            resid_mat[col] = np.nan\n",
    "            continue\n",
    "\n",
    "        X_m = X[mask]\n",
    "        y_m = y[mask]\n",
    "\n",
    "        # y = a + b * X ìµœì†Œì œê³±\n",
    "        A = np.vstack([np.ones_like(X_m), X_m]).T\n",
    "        coef, _, _, _ = np.linalg.lstsq(A, y_m, rcond=None)\n",
    "        a, b = coef\n",
    "\n",
    "        y_hat = a + b * X\n",
    "        resid = y - y_hat\n",
    "        resid_mat[col] = resid\n",
    "\n",
    "    return resid_mat\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. ë™ì‹œ ìƒê´€ + 1ê°œì›” lead ìƒê´€ ê³„ì‚°\n",
    "# ----------------------------------------------------\n",
    "def compute_correlations(resid_mat: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    resid_mat: index=date, columns=item_id\n",
    "    return:\n",
    "      - corr_same: ê°™ì€ ì‹œì  residual í”¼ì–´ìŠ¨ ìƒê´€\n",
    "      - corr_lead: i_t vs j_{t+1} ìƒê´€ (iê°€ ì„ í–‰, jê°€ í›„í–‰)\n",
    "    \"\"\"\n",
    "    # í‰ê·  0ìœ¼ë¡œ ì„¼í„°ë§\n",
    "    resid_centered = resid_mat - resid_mat.mean(axis=0)\n",
    "\n",
    "    # ë™ì‹œ ìƒê´€\n",
    "    corr_same = resid_centered.corr()\n",
    "\n",
    "    # 1ê°œì›” lead ìƒê´€\n",
    "    resid_t = resid_centered.iloc[:-1, :]   # t\n",
    "    resid_t1 = resid_centered.iloc[1:, :]   # t+1\n",
    "\n",
    "    items = resid_centered.columns\n",
    "    corr_lead = pd.DataFrame(index=items, columns=items, dtype=float)\n",
    "\n",
    "    for i in items:\n",
    "        x = resid_t[i].values\n",
    "        mask_x = ~np.isnan(x)\n",
    "\n",
    "        for j in items:\n",
    "            if i == j:\n",
    "                corr_lead.loc[i, j] = np.nan\n",
    "                continue\n",
    "\n",
    "            y = resid_t1[j].values  # j(t+1)\n",
    "            mask_y = ~np.isnan(y)\n",
    "\n",
    "            mask = mask_x & mask_y\n",
    "            if mask.sum() < 5:\n",
    "                corr_lead.loc[i, j] = np.nan\n",
    "                continue\n",
    "\n",
    "            corr = np.corrcoef(x[mask], y[mask])[0, 1]\n",
    "            corr_lead.loc[i, j] = corr\n",
    "\n",
    "    return corr_same, corr_lead\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4. ê³µí–‰ì„± í›„ë³´ìŒ ì„ ì •\n",
    "# ----------------------------------------------------\n",
    "def select_candidate_pairs(corr_same: pd.DataFrame,\n",
    "                           corr_lead: pd.DataFrame,\n",
    "                           th_same: float = 0.4,\n",
    "                           th_lead: float = 0.5,\n",
    "                           min_diff: float = 0.05,\n",
    "                           top_k: int | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    corr_same: i,j ë™ì‹œ ìƒê´€\n",
    "    corr_lead: i_t vs j_{t+1} (i ì„ í–‰, j í›„í–‰)\n",
    "    ê·œì¹™:\n",
    "      - corr_same >= th_same\n",
    "      - corr_lead(i->j)ì™€ corr_lead(j->i) ì¤‘ í° ë°©í–¥ì„ ì„ íƒ\n",
    "      - ë°©í–¥ì„± ì°¨ì´ê°€ min_diff ì´ìƒì´ê³ , ê·¸ ë°©í–¥ì˜ corr_lead >= th_lead\n",
    "    \"\"\"\n",
    "    items = corr_same.index\n",
    "    pairs = []\n",
    "\n",
    "    for i in items:\n",
    "        for j in items:\n",
    "            if i == j:\n",
    "                continue\n",
    "\n",
    "            same = corr_same.loc[i, j]\n",
    "            if pd.isna(same) or same < th_same:\n",
    "                continue\n",
    "\n",
    "            lead_ij = corr_lead.loc[i, j]  # iâ†’j\n",
    "            lead_ji = corr_lead.loc[j, i]  # jâ†’i\n",
    "\n",
    "            # ë‘˜ ë‹¤ NaNì´ë©´ ìŠ¤í‚µ\n",
    "            if pd.isna(lead_ij) and pd.isna(lead_ji):\n",
    "                continue\n",
    "\n",
    "            # iâ†’j ë°©í–¥ì´ ë” ìœ ë ¥\n",
    "            if pd.notna(lead_ij):\n",
    "                if pd.isna(lead_ji) or (lead_ij - lead_ji) >= min_diff:\n",
    "                    if lead_ij >= th_lead:\n",
    "                        pairs.append({\n",
    "                            \"leading_item_id\": i,\n",
    "                            \"following_item_id\": j,\n",
    "                            \"corr_same\": same,\n",
    "                            \"corr_lead\": lead_ij,\n",
    "                            \"direction\": \"i_leads_j\"\n",
    "                        })\n",
    "\n",
    "            # jâ†’i ë°©í–¥ì´ ë” ìœ ë ¥\n",
    "            if pd.notna(lead_ji):\n",
    "                if pd.isna(lead_ij) or (lead_ji - lead_ij) >= min_diff:\n",
    "                    if lead_ji >= th_lead:\n",
    "                        pairs.append({\n",
    "                            \"leading_item_id\": j,\n",
    "                            \"following_item_id\": i,\n",
    "                            \"corr_same\": same,\n",
    "                            \"corr_lead\": lead_ji,\n",
    "                            \"direction\": \"j_leads_i\"\n",
    "                        })\n",
    "\n",
    "    candidates_df = pd.DataFrame(pairs)\n",
    "\n",
    "    if candidates_df.empty:\n",
    "        return candidates_df\n",
    "\n",
    "    candidates_df = candidates_df.sort_values(\n",
    "        [\"corr_lead\", \"corr_same\"], ascending=False\n",
    "    ).drop_duplicates(\n",
    "        subset=[\"leading_item_id\", \"following_item_id\"]\n",
    "    )\n",
    "\n",
    "    if top_k is not None:\n",
    "        candidates_df = candidates_df.head(top_k)\n",
    "\n",
    "    return candidates_df\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 5. ì „ì²´ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜\n",
    "# ----------------------------------------------------\n",
    "def generate_comovement_candidates(df_panel: pd.DataFrame,\n",
    "                                   min_nonzero_months: int = 8,\n",
    "                                   min_std: float = 0.2,\n",
    "                                   th_same: float = 0.4,\n",
    "                                   th_lead: float = 0.5,\n",
    "                                   min_diff: float = 0.05,\n",
    "                                   top_k: int | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ì…ë ¥: df_panel (item_id, date, value í¬í•¨)\n",
    "    ì¶œë ¥: ê³µí–‰ì„± í›„ë³´ìŒ DataFrame\n",
    "           [leading_item_id, following_item_id, corr_same, corr_lead, direction]\n",
    "    \"\"\"\n",
    "    # 0) value matrix\n",
    "    value_mat = build_value_matrix(df_panel)\n",
    "\n",
    "    # 1) item filtering\n",
    "    value_mat_f = filter_items(\n",
    "        value_mat,\n",
    "        min_nonzero_months=min_nonzero_months,\n",
    "        min_std=min_std\n",
    "    )\n",
    "\n",
    "    # 2) residuals\n",
    "    resid_mat = compute_market_residuals(value_mat_f)\n",
    "\n",
    "    # 3) correlations\n",
    "    corr_same, corr_lead = compute_correlations(resid_mat)\n",
    "\n",
    "    # 4) candidate pairs\n",
    "    candidates_df = select_candidate_pairs(\n",
    "        corr_same, corr_lead,\n",
    "        th_same=th_same,\n",
    "        th_lead=th_lead,\n",
    "        min_diff=min_diff,\n",
    "        top_k=top_k\n",
    "    )\n",
    "\n",
    "    return candidates_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8592ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_panelì´ ì´ë¯¸ ì¤€ë¹„ë˜ì–´ ìˆë‹¤ê³  ê°€ì •\n",
    "candidates_df = generate_comovement_candidates(\n",
    "    df_panel,\n",
    "    min_nonzero_months=8,\n",
    "    min_std=0.2,\n",
    "    th_same=0.4,\n",
    "    th_lead=0.5,\n",
    "    min_diff=0.05,\n",
    "    top_k=500  # ì˜ˆì‹œ: ìƒìœ„ 500ìŒë§Œ\n",
    ")\n",
    "\n",
    "print(candidates_df)\n",
    "print(\"í›„ë³´ìŒ ê°œìˆ˜:\", len(candidates_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eae5db",
   "metadata": {},
   "source": [
    "## 1. ì‹œê³„ì—´ í•µì‹¬ í”¼ì²˜ ì¶”ê°€ (log, lag, rolling)\n",
    "\n",
    "- ì§€ê¸ˆ make_item_featuresëŠ” HS4/HS2 ë¡¤ë§, EMA, run-lengthë¥¼ ë§Œë“¤ì–´ì£¼ê³  ìˆì–´.\n",
    "- ì—¬ê¸°ì— log_value + lag1~3 + rolling mean 3/6ê°œì›” + trend_index/year/monthë¥¼ ì¶”ê°€ë¡œ ë¶™ì—¬ ì“°ì."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f438ebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aefe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# -------------------------------\n",
    "# 1) ì‹œê³„ì—´ í•µì‹¬ í”¼ì²˜ ì¶”ê°€\n",
    "# -------------------------------\n",
    "def add_ts_core_features(feat_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ê¸°ì¡´ make_item_features ê²°ê³¼(feat_df)ì—\n",
    "    - log_value\n",
    "    - log_value_lag1,2,3\n",
    "    - log_roll_mean_3,6\n",
    "    - year, month, trend_index\n",
    "    ë¥¼ ì¶”ê°€í•œë‹¤.\n",
    "    \"\"\"\n",
    "    df = feat_df.copy()\n",
    "\n",
    "    # date ì •ë¦¬\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df = df.sort_values([\"item_id\", \"date\"])\n",
    "\n",
    "    # year, month ì—†ìœ¼ë©´ ìƒì„±\n",
    "    if \"year\" not in df.columns:\n",
    "        df[\"year\"] = df[\"date\"].dt.year\n",
    "    if \"month\" not in df.columns:\n",
    "        df[\"month\"] = df[\"date\"].dt.month\n",
    "\n",
    "    # trend_index: ë‚ ì§œ ìˆœì„œ ì¸ë±ìŠ¤ (ì „ì²´ ê³µí†µ)\n",
    "    unique_dates = sorted(df[\"date\"].dropna().unique())\n",
    "    date_to_trend = {d: i for i, d in enumerate(unique_dates)}\n",
    "    df[\"trend_index\"] = df[\"date\"].map(date_to_trend)\n",
    "\n",
    "    # ê³„ì ˆì„± í”¼ì²˜ (sin/cos ë³€í™˜)\n",
    "    df[\"month_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12)\n",
    "    df[\"month_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12)\n",
    "\n",
    "    # log ë³€í™˜ (value, weight, quantity)\n",
    "    for col in [\"value\", \"weight\", \"quantity\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0)\n",
    "            df[col] = df[col].clip(lower=0)\n",
    "            df[f\"log_{col}\"] = np.log1p(df[col].astype(float))\n",
    "\n",
    "    # itemë³„ lag, rolling í†µê³„\n",
    "    dfs = []\n",
    "    for item, sub in df.groupby(\"item_id\"):\n",
    "        sub = sub.sort_values(\"date\").copy()\n",
    "        \n",
    "        # ë” ë§ì€ lag í”¼ì²˜ (1,2,3,6,12)\n",
    "        for lag in [1, 2, 3, 6, 12]:\n",
    "            if \"log_value\" in sub.columns:\n",
    "                sub[f\"log_value_lag{lag}\"] = sub[\"log_value\"].shift(lag)\n",
    "            if \"log_weight\" in sub.columns:\n",
    "                sub[f\"log_weight_lag{lag}\"] = sub[\"log_weight\"].shift(lag)\n",
    "        \n",
    "        # rolling í†µê³„ (mean, std, min, max)\n",
    "        for window in [3, 6, 12]:\n",
    "            if \"log_value\" in sub.columns:\n",
    "                shifted = sub[\"log_value\"].shift(1)\n",
    "                sub[f\"log_roll_mean_{window}\"] = shifted.rolling(window, min_periods=1).mean()\n",
    "                sub[f\"log_roll_std_{window}\"] = shifted.rolling(window, min_periods=1).std().fillna(0)\n",
    "                if window == 3:  # min/maxëŠ” 3ê°œì›”ë§Œ\n",
    "                    sub[f\"log_roll_min_{window}\"] = shifted.rolling(window, min_periods=1).min()\n",
    "                    sub[f\"log_roll_max_{window}\"] = shifted.rolling(window, min_periods=1).max()\n",
    "        \n",
    "        # YoY ë³€í™”ìœ¨ (12ê°œì›” ì „ ëŒ€ë¹„)\n",
    "        if \"log_value\" in sub.columns:\n",
    "            sub[\"log_value_yoy\"] = sub[\"log_value\"] - sub[\"log_value\"].shift(12)\n",
    "            shifted_12 = sub[\"log_value\"].shift(12)\n",
    "            sub[\"log_value_yoy_pct\"] = (sub[\"log_value\"] - shifted_12) / (shifted_12 + 1e-8)\n",
    "\n",
    "        dfs.append(sub)\n",
    "\n",
    "    df_out = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f68e0b9",
   "metadata": {},
   "source": [
    "## 2. pair ë‹¨ìœ„ í•™ìŠµ ë°ì´í„°ì…‹ ë§Œë“¤ê¸°\n",
    "\n",
    "### ê° rowë¥¼ ì´ë ‡ê²Œ ë³´ë„ë¡ ë§Œë“¤ ê±°ì•¼:\n",
    "\n",
    "- (leading_item_id, following_item_id, date_t)\n",
    "\n",
    "X:\n",
    "\n",
    "- followingì˜ log_value_lag/roll/EMA/HSí†µê³„/zero-run ë“±\n",
    "\n",
    "- leadingì˜ log_value_lag/roll/EMA/HSí†µê³„/zero-run ë“±\n",
    "\n",
    "- tì˜ year, month, trend_index\n",
    "\n",
    "y:\n",
    "\n",
    "- followingì˜ log_value at t+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55a18e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2) pair-level í•™ìŠµ ë°ì´í„° ìƒì„±\n",
    "# -------------------------------\n",
    "\n",
    "def _next_month(d: pd.Timestamp) -> pd.Timestamp:\n",
    "    year = d.year + (1 if d.month == 12 else 0)\n",
    "    month = 1 if d.month == 12 else d.month + 1\n",
    "    return pd.Timestamp(year=year, month=month, day=1)\n",
    "\n",
    "\n",
    "def build_pair_training_data_v2(\n",
    "    feat_df: pd.DataFrame,\n",
    "    candidates_df: pd.DataFrame,\n",
    "    max_train_date: pd.Timestamp,\n",
    ") -> tuple[pd.DataFrame, pd.Series, list[str], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    feat_df: add_ts_core_featuresê¹Œì§€ ì ìš©ëœ í”¼ì²˜ íŒ¨ë„\n",
    "    candidates_df: ê³µí–‰ì„± í›„ë³´ìŒ DataFrame\n",
    "    max_train_date: í•™ìŠµì— ì‚¬ìš©í•  ë§ˆì§€ë§‰ t (ë¼ë²¨ì€ t+1 ê¸°ì¤€)\n",
    "\n",
    "    ë°˜í™˜:\n",
    "      X: feature DataFrame\n",
    "      y: Series (followingì˜ log_value at t+1)\n",
    "      feature_cols: ì‚¬ìš©ëœ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸\n",
    "      meta_df: (leading, following, date_t, date_t1) ë©”íƒ€ ì •ë³´\n",
    "    \"\"\"\n",
    "    df = feat_df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "\n",
    "    base = df.set_index([\"item_id\", \"date\"])\n",
    "\n",
    "    # ì‚¬ìš©í•  ë‚ ì§œë“¤ (t)\n",
    "    all_dates = sorted(df[\"date\"].dropna().unique())\n",
    "    all_dates = [d for d in all_dates if d <= max_train_date]\n",
    "\n",
    "    # ì–´ë–¤ í”¼ì²˜ë“¤ì„ ì“¸ì§€ ì •ì˜\n",
    "    # (ì¡´ì¬ ì—¬ë¶€ë¥¼ ì²´í¬í•˜ë©´ì„œ ë™ì ìœ¼ë¡œ ì„ íƒ)\n",
    "    candidate_item_features = [\n",
    "        \"value\",\n",
    "        \"log_value\",\n",
    "        \"log_value_lag1\", \"log_value_lag2\", \"log_value_lag3\",\n",
    "        \"log_roll_mean_3\", \"log_roll_mean_6\",\n",
    "        # EMA\n",
    "        \"value_ema_3\", \"value_ema_6\", \"value_ema_12\",\n",
    "        # HS ê·¸ë£¹ í†µê³„\n",
    "        \"hs4_value_mean_3m\", \"hs4_value_mean_6m\", \"hs4_value_mean_12m\",\n",
    "        \"hs2_value_mean_3m\", \"hs2_value_mean_6m\", \"hs2_value_mean_12m\",\n",
    "        # zero/missing run-length\n",
    "        \"value_zero_run_length\",\n",
    "        \"value_missing_run_length\",\n",
    "        \"is_missing_month_run_length\",\n",
    "    ]\n",
    "    # ì‹¤ì œë¡œ feat_dfì— ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ ì‚¬ìš©\n",
    "    item_features = [c for c in candidate_item_features if c in df.columns]\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for _, pair in candidates_df.iterrows():\n",
    "        lead = pair[\"leading_item_id\"]\n",
    "        foll = pair[\"following_item_id\"]\n",
    "\n",
    "        for t in all_dates:\n",
    "            t1 = _next_month(t)\n",
    "\n",
    "            # t+1 ë ˆì´ë¸”ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ\n",
    "            if (foll, t1) not in base.index:\n",
    "                continue\n",
    "\n",
    "            # t ì‹œì ì— leading/following í”¼ì²˜ê°€ ëª¨ë‘ ìˆì–´ì•¼ í•¨\n",
    "            if (foll, t) not in base.index or (lead, t) not in base.index:\n",
    "                continue\n",
    "\n",
    "            foll_t = base.loc[(foll, t)]\n",
    "            lead_t = base.loc[(lead, t)]\n",
    "            foll_t1 = base.loc[(foll, t1)]\n",
    "\n",
    "            row = {\n",
    "                \"leading_item_id\": lead,\n",
    "                \"following_item_id\": foll,\n",
    "                \"date_t\": t,\n",
    "                \"date_t1\": t1,\n",
    "                # ì‹œì  ê³µí†µ í”¼ì²˜\n",
    "                \"year\": foll_t.get(\"year\", np.nan),\n",
    "                \"month\": foll_t.get(\"month\", np.nan),\n",
    "                \"trend_index\": foll_t.get(\"trend_index\", np.nan),\n",
    "                # íƒ€ê²Ÿ (log_value at t+1)\n",
    "                \"y_log_next\": foll_t1.get(\"log_value\", np.nan),\n",
    "            }\n",
    "\n",
    "            # following / leading ê°ê°ì— ëŒ€í•´ item_features ì¶”ê°€\n",
    "            for c in item_features:\n",
    "                row[f\"f_{c}\"] = foll_t.get(c, np.nan)\n",
    "                row[f\"l_{c}\"] = lead_t.get(c, np.nan)\n",
    "\n",
    "            rows.append(row)\n",
    "\n",
    "    pair_df = pd.DataFrame(rows)\n",
    "    # NaN ë§ì€ ì´ˆê¸° êµ¬ê°„ ì œê±°\n",
    "    pair_df = pair_df.dropna(subset=[\"y_log_next\"]).reset_index(drop=True)\n",
    "\n",
    "    # feature ì»¬ëŸ¼ ëª©ë¡\n",
    "    feature_cols = [\"year\", \"month\", \"trend_index\"]\n",
    "    feature_cols += [col for col in pair_df.columns if col.startswith(\"f_\")]\n",
    "    feature_cols += [col for col in pair_df.columns if col.startswith(\"l_\")]\n",
    "\n",
    "    X = pair_df[feature_cols].copy()\n",
    "    y = pair_df[\"y_log_next\"].copy()\n",
    "\n",
    "    return X, y, feature_cols, pair_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fea5afb",
   "metadata": {},
   "source": [
    "## 3. RandomForestë¡œ í•™ìŠµ + ê²€ì¦ (time-split)\n",
    "\n",
    "- train_last_tê¹Œì§€ì˜ të¥¼ train\n",
    "\n",
    "- valid_last_tê¹Œì§€ì˜ të¥¼ í•œ ë²ˆ ë” ì¨ì„œ valid êµ¬ê°„ ìƒì„±\n",
    "(validëŠ” train_last_t ì´í›„ tì— í•´ë‹¹í•˜ëŠ” rowë§Œ ë‚¨ê¹€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe4179b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4a98fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# ì•„ì´í…œ ë‹¨ìœ„ í”¼ì²˜ ìƒì„± í•¨ìˆ˜ (ì˜µì…˜í˜• ë¸”ë¡)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def _compute_run_length(flag_series: pd.Series) -> pd.Series:\n",
    "    \"\"\"0/1 í”Œë˜ê·¸ ì‹œë¦¬ì¦ˆì—ì„œ ì—°ì† 1(run length)ì„ ê³„ì‚°.\n",
    "    1 êµ¬ê°„ì€ ëˆ„ì  ê¸¸ì´, 0 êµ¬ê°„ì€ 0ìœ¼ë¡œ ë‘”ë‹¤.\n",
    "    \"\"\"\n",
    "    flag = flag_series.fillna(0).astype(int)\n",
    "    is_one = flag == 1\n",
    "    # 0ì´ ë‚˜ì˜¬ ë•Œë§ˆë‹¤ ê·¸ë£¹ì´ ì¦ê°€ â†’ ê·¸ë£¹ë³„ë¡œ cumsumí•˜ë©´ run length\n",
    "    groups = (~is_one).cumsum()\n",
    "    run = is_one.groupby(groups).cumsum()\n",
    "    return run.where(is_one, 0)\n",
    "\n",
    "\n",
    "def make_item_features(\n",
    "    df_panel: pd.DataFrame,\n",
    "    use_hs_stats: bool = True,\n",
    "    use_ema: bool = True,\n",
    "    use_zero_run: bool = True,\n",
    "    hs_windows: tuple[int, ...] = (3, 6, 12),\n",
    "    ema_spans: tuple[int, ...] = (3, 6, 12),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"df_panelì—ì„œ ì•„ì´í…œ ë‹¨ìœ„ ì‹œê³„ì—´ í”¼ì²˜ë¥¼ ìƒì„±.\n",
    "\n",
    "    ì˜µì…˜ í”Œë˜ê·¸ë¡œ HS ê·¸ë£¹ ë¡¤ë§ í†µê³„ / EMA / zero-run-length ë¸”ë¡ì„ ì˜¨ì˜¤í”„ í•  ìˆ˜ ìˆë‹¤.\n",
    "    ë°˜í™˜ê°’ì€ (item_id, date)ë¥¼ ìœ ì§€í•œ í–‰ ë‹¨ìœ„ í”¼ì²˜ DataFrame.\n",
    "    \"\"\"\n",
    "    df = df_panel.copy()\n",
    "\n",
    "    # ê¸°ë³¸ ì •ë ¬ ë° íƒ€ì… ì •ë¦¬\n",
    "    if \"date\" not in df.columns or \"item_id\" not in df.columns:\n",
    "        raise ValueError(\"df_panelì—ëŠ” ìµœì†Œí•œ 'item_id', 'date' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df = df.sort_values([\"item_id\", \"date\"])  # ì•„ì´í…œë³„ ì‹œê°„ ìˆœ ì •ë ¬\n",
    "\n",
    "    # value/weight/quantityë¥¼ ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ê°•ì œ\n",
    "    for col in [\"value\", \"weight\", \"quantity\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 1) HS4 / HS2 ê·¸ë£¹ ë¡¤ë§ í†µê³„ (ì˜µì…˜)\n",
    "    # ------------------------------------------------\n",
    "    if use_hs_stats:\n",
    "        # HS4 ê¸°ì¤€ ë¡¤ë§\n",
    "        if \"hs4\" in df.columns:\n",
    "            for w in hs_windows:\n",
    "                # HS4 ê·¸ë£¹ë³„ ì›”ë³„ value í‰ê·  ë¡¤ë§\n",
    "                col_name = f\"hs4_value_mean_{w}m\"\n",
    "                df[col_name] = (\n",
    "                    df.groupby(\"hs4\")[\"value\"]\n",
    "                      .transform(lambda s: s.rolling(window=w, min_periods=1).mean())\n",
    "                )\n",
    "        # HS2ê°€ ìˆìœ¼ë©´ ì¶”ê°€ ë¡¤ë§ í†µê³„\n",
    "        if \"hs2\" in df.columns:\n",
    "            for w in hs_windows:\n",
    "                col_name = f\"hs2_value_mean_{w}m\"\n",
    "                df[col_name] = (\n",
    "                    df.groupby(\"hs2\")[\"value\"]\n",
    "                      .transform(lambda s: s.rolling(window=w, min_periods=1).mean())\n",
    "                )\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 2) EMA í”¼ì²˜ (ì˜µì…˜)\n",
    "    # ------------------------------------------------\n",
    "    if use_ema:\n",
    "        for span in ema_spans:\n",
    "            if \"value\" in df.columns:\n",
    "                df[f\"value_ema_{span}\"] = (\n",
    "                    df.groupby(\"item_id\")[\"value\"]\n",
    "                      .transform(lambda s: s.ewm(span=span, adjust=False).mean())\n",
    "                )\n",
    "            if \"weight\" in df.columns:\n",
    "                df[f\"weight_ema_{span}\"] = (\n",
    "                    df.groupby(\"item_id\")[\"weight\"]\n",
    "                      .transform(lambda s: s.ewm(span=span, adjust=False).mean())\n",
    "                )\n",
    "            if \"quantity\" in df.columns:\n",
    "                df[f\"quantity_ema_{span}\"] = (\n",
    "                    df.groupby(\"item_id\")[\"quantity\"]\n",
    "                      .transform(lambda s: s.ewm(span=span, adjust=False).mean())\n",
    "                )\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 3) zero / missing run-length í”¼ì²˜ (ì˜µì…˜)\n",
    "    # ------------------------------------------------\n",
    "    if use_zero_run:\n",
    "        # ê¸°ëŒ€í•˜ëŠ” í”Œë˜ê·¸ ì»¬ëŸ¼: value_zero, value_missing, is_missing_month ë“±\n",
    "        flag_cols = [\n",
    "            col for col in [\n",
    "                \"value_zero\",\n",
    "                \"weight_zero\",\n",
    "                \"quantity_zero\",\n",
    "                \"value_missing\",\n",
    "                \"weight_missing\",\n",
    "                \"quantity_missing\",\n",
    "                \"is_missing_month\",\n",
    "            ]\n",
    "            if col in df.columns\n",
    "        ]\n",
    "\n",
    "        for col in flag_cols:\n",
    "            feat_name = f\"{col}_run_length\"\n",
    "            df[feat_name] = (\n",
    "                df.groupby(\"item_id\")[col]\n",
    "                  .transform(_compute_run_length)\n",
    "            )\n",
    "\n",
    "    # ìµœì¢… í”¼ì²˜ DataFrame: ì›ë³¸ index/í‚¤ ìœ ì§€\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9c1ea3",
   "metadata": {},
   "source": [
    "## 4. 2025-08 pair-level ì˜ˆì¸¡\n",
    "\n",
    "- t = 2025-07-01ë¥¼ ê¸°ì¤€ìœ¼ë¡œ featureë¥¼ ë§Œë“¤ê³ \n",
    "\n",
    "- each pair(leading, following)ì— ëŒ€í•´ Xë¥¼ ë§Œë“¤ê³ \n",
    "\n",
    "- log-space ì˜ˆì¸¡ í›„ expm1 â†’ ì •ìˆ˜ë¡œ í´ë¦½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a069381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 4) 2025-08 ì˜ˆì¸¡ìš© pair-level ë°ì´í„° ìƒì„±\n",
    "# -------------------------------\n",
    "\n",
    "def build_pair_features_for_target_month(\n",
    "    feat_df: pd.DataFrame,\n",
    "    candidates_df: pd.DataFrame,\n",
    "    target_prev_month: pd.Timestamp,\n",
    "    feature_cols: list[str],\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    target_prev_month = t (ì˜ˆ: 2025-07-01)\n",
    "    t ì‹œì  featureë¡œ t+1(2025-08) valueë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ Xë¥¼ ë§Œë“ ë‹¤.\n",
    "    \"\"\"\n",
    "    df = feat_df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    base = df.set_index([\"item_id\", \"date\"])\n",
    "\n",
    "    item_features = [c for c in feature_cols if c.startswith(\"f_\")]\n",
    "    # f_, l_ íŒ¨í„´ê³¼ year, month, trend_indexë§Œ ì‚¬ìš©\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # ì…ë ¥ ê²€ì¦\n",
    "    if candidates_df is None or len(candidates_df) == 0:\n",
    "        return pd.DataFrame(columns=[\"leading_item_id\", \"following_item_id\"]), pd.DataFrame()\n",
    "    \n",
    "    if \"leading_item_id\" not in candidates_df.columns or \"following_item_id\" not in candidates_df.columns:\n",
    "        raise ValueError(\"candidates_dfì— 'leading_item_id' ë˜ëŠ” 'following_item_id' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "    \n",
    "    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸\n",
    "    if \"item_id\" not in df.columns or \"date\" not in df.columns:\n",
    "        raise ValueError(\"feat_dfì— 'item_id' ë˜ëŠ” 'date' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "    \n",
    "    try:\n",
    "        base = df.set_index([\"item_id\", \"date\"])\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"ì¸ë±ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "    for _, pair in candidates_df.iterrows():\n",
    "        try:\n",
    "            lead = pair[\"leading_item_id\"]\n",
    "            foll = pair[\"following_item_id\"]\n",
    "            t = target_prev_month\n",
    "\n",
    "            # Noneì´ë‚˜ NaN ì²´í¬\n",
    "            if pd.isna(lead) or pd.isna(foll):\n",
    "                continue\n",
    "\n",
    "            if (foll, t) not in base.index or (lead, t) not in base.index:\n",
    "                continue\n",
    "\n",
    "            foll_t = base.loc[(foll, t)]\n",
    "            lead_t = base.loc[(lead, t)]\n",
    "\n",
    "            # Seriesë‚˜ dict ëª¨ë‘ ì²˜ë¦¬í•˜ëŠ” í—¬í¼ í•¨ìˆ˜\n",
    "            def safe_get(data, key, default=np.nan):\n",
    "                if hasattr(data, 'get'):\n",
    "                    return data.get(key, default)\n",
    "                elif hasattr(data, '__getitem__'):\n",
    "                    try:\n",
    "                        return data[key]\n",
    "                    except (KeyError, IndexError):\n",
    "                        return default\n",
    "                else:\n",
    "                    return default\n",
    "\n",
    "            row = {\n",
    "                \"leading_item_id\": lead,\n",
    "                \"following_item_id\": foll,\n",
    "                \"date_t\": t,\n",
    "                \"year\": safe_get(foll_t, \"year\"),\n",
    "                \"month\": safe_get(foll_t, \"month\"),\n",
    "                \"trend_index\": safe_get(foll_t, \"trend_index\"),\n",
    "            }\n",
    "\n",
    "            # feature_colsë¥¼ ì¬í™œìš©í•´ ë™ì¼í•œ ì»¬ëŸ¼ êµ¬ì¡°ë¥¼ ë§ì¶˜ë‹¤.\n",
    "            # feature_cols ì•ˆì— ìˆëŠ” f_/l_ ì´ë¦„ì„ ê¸°ì¤€ìœ¼ë¡œ ê°’ ì±„ìš°ê¸°\n",
    "            for col in feature_cols:\n",
    "                try:\n",
    "                    if col.startswith(\"f_\"):\n",
    "                        base_name = col[2:]\n",
    "                        row[col] = safe_get(foll_t, base_name)\n",
    "                    elif col.startswith(\"l_\"):\n",
    "                        base_name = col[2:]\n",
    "                        row[col] = safe_get(lead_t, base_name)\n",
    "                    elif col in [\"year\", \"month\", \"trend_index\", \"month_sin\", \"month_cos\"]:\n",
    "                        # ê³µìœ  í”¼ì²˜ëŠ” following ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©\n",
    "                        if col not in row:\n",
    "                            row[col] = safe_get(foll_t, col)\n",
    "                except Exception:\n",
    "                    row[col] = np.nan\n",
    "\n",
    "            rows.append(row)\n",
    "        except Exception as e:\n",
    "            # ê°œë³„ ìŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì†\n",
    "            continue\n",
    "\n",
    "    if len(rows) == 0:\n",
    "        return pd.DataFrame(columns=[\"leading_item_id\", \"following_item_id\"]), pd.DataFrame()\n",
    "\n",
    "    feat_pair_df = pd.DataFrame(rows)\n",
    "    \n",
    "    # feature_colsê°€ ì—†ìœ¼ë©´ ë¹ˆ DataFrame ë°˜í™˜\n",
    "    if len(feature_cols) == 0:\n",
    "        return feat_pair_df, pd.DataFrame()\n",
    "    \n",
    "    # ì¡´ì¬í•˜ëŠ” feature_colsë§Œ ì‚¬ìš©\n",
    "    available_cols = [c for c in feature_cols if c in feat_pair_df.columns]\n",
    "    if len(available_cols) == 0:\n",
    "        # feature_colsê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì»¬ëŸ¼ë§Œ ë°˜í™˜\n",
    "        X_target = pd.DataFrame(index=feat_pair_df.index)\n",
    "    else:\n",
    "        X_target = feat_pair_df[available_cols].copy()\n",
    "\n",
    "    return feat_pair_df, X_target\n",
    "\n",
    "\n",
    "def predict_aug_2025_values_v2(\n",
    "    model,\n",
    "    feat_df: pd.DataFrame,\n",
    "    candidates_df: pd.DataFrame,\n",
    "    feature_cols: list[str],\n",
    "    submission_template: pd.DataFrame | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    pair-level RF ëª¨ë¸ë¡œ ê° (leading, following) ìŒì˜ 2025-08 value ì˜ˆì¸¡\n",
    "    \n",
    "    Args:\n",
    "        model: í•™ìŠµëœ ëª¨ë¸\n",
    "        feat_df: í”¼ì²˜ ë°ì´í„°í”„ë ˆì„\n",
    "        candidates_df: ê³µí–‰ì„± í›„ë³´ìŒ (ê¸°ë³¸ ì˜ˆì¸¡ ëŒ€ìƒ)\n",
    "        feature_cols: í”¼ì²˜ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸\n",
    "        submission_template: ì œì¶œ í…œí”Œë¦¿ (ìˆìœ¼ë©´ ëª¨ë“  ìŒì— ëŒ€í•´ ì˜ˆì¸¡ ì‹œë„)\n",
    "    \"\"\"\n",
    "    # t = 2025-07-01 â†’ t+1 = 2025-08-01\n",
    "    last_obs_date = feat_df[\"date\"].max()\n",
    "    target_prev_month = last_obs_date  # df_panelì´ 2025-07-01ê¹Œì§€ ìˆë‹¤ê³  ê°€ì •\n",
    "\n",
    "    # submission templateì´ ìˆìœ¼ë©´ ëª¨ë“  ìŒì— ëŒ€í•´ ì˜ˆì¸¡ ì‹œë„\n",
    "    if submission_template is not None and len(submission_template) > 0:\n",
    "        try:\n",
    "            # templateì˜ ëª¨ë“  ìŒì„ candidatesë¡œ ì‚¬ìš©\n",
    "            all_pairs = submission_template[[\"leading_item_id\", \"following_item_id\"]].copy()\n",
    "            # ì¤‘ë³µ ì œê±°\n",
    "            all_pairs = all_pairs.drop_duplicates()\n",
    "            print(f\"ğŸ“Š submission templateì˜ {len(all_pairs)}ê°œ ìŒì— ëŒ€í•´ ì˜ˆì¸¡ ì‹œë„\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ submission_template ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}, candidates_df ì‚¬ìš©\")\n",
    "            all_pairs = candidates_df[[\"leading_item_id\", \"following_item_id\"]].copy()\n",
    "            print(f\"ğŸ“Š candidates_dfì˜ {len(all_pairs)}ê°œ ìŒì— ëŒ€í•´ ì˜ˆì¸¡ ì‹œë„\")\n",
    "    else:\n",
    "        if candidates_df is None or len(candidates_df) == 0:\n",
    "            raise ValueError(\"candidates_dfê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!\")\n",
    "        all_pairs = candidates_df[[\"leading_item_id\", \"following_item_id\"]].copy()\n",
    "        print(f\"ğŸ“Š candidates_dfì˜ {len(all_pairs)}ê°œ ìŒì— ëŒ€í•´ ì˜ˆì¸¡ ì‹œë„\")\n",
    "\n",
    "    if len(all_pairs) == 0:\n",
    "        raise ValueError(\"ì˜ˆì¸¡í•  ìŒì´ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "    try:\n",
    "        feat_pair_df, X_target = build_pair_features_for_target_month(\n",
    "            feat_df, all_pairs, target_prev_month, feature_cols\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ í”¼ì²˜ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        raise\n",
    "\n",
    "    if len(feat_pair_df) == 0:\n",
    "        print(\"âš ï¸ ì˜ˆì¸¡ ê°€ëŠ¥í•œ ìŒì´ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ê²°ê³¼ ë°˜í™˜\")\n",
    "        return pd.DataFrame(columns=[\"leading_item_id\", \"following_item_id\", \"value\"])\n",
    "\n",
    "    try:\n",
    "        # NaN ì²˜ë¦¬ í›„ ì˜ˆì¸¡\n",
    "        X_target_filled = X_target.fillna(0)\n",
    "        y_pred_log = model.predict(X_target_filled)\n",
    "        \n",
    "        # ì˜ˆì¸¡ê°’ ê²€ì¦\n",
    "        print(f\"   ì˜ˆì¸¡ log_value ë²”ìœ„: {y_pred_log.min():.4f} ~ {y_pred_log.max():.4f}\")\n",
    "        print(f\"   ì˜ˆì¸¡ log_value í‰ê· : {y_pred_log.mean():.4f}\")\n",
    "        \n",
    "        # expm1 ë³€í™˜: log(1+x) â†’ x\n",
    "        y_pred_value = np.expm1(y_pred_log)\n",
    "        \n",
    "        # ìŒìˆ˜ë‚˜ ì´ìƒì¹˜ ì²˜ë¦¬\n",
    "        y_pred_value = np.clip(y_pred_value, 0, None)\n",
    "        \n",
    "        # ì´ìƒì¹˜ ì œê±° (99.9 percentile)\n",
    "        if len(y_pred_value) > 0:\n",
    "            upper_bound = np.percentile(y_pred_value, 99.9)\n",
    "            y_pred_value = np.clip(y_pred_value, 0, upper_bound)\n",
    "            print(f\"   ë³€í™˜ëœ value ë²”ìœ„: {y_pred_value.min():.2f} ~ {y_pred_value.max():.2f}\")\n",
    "            print(f\"   ë³€í™˜ëœ value í‰ê· : {y_pred_value.mean():.2f}, ì¤‘ì•™ê°’: {np.median(y_pred_value):.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        raise\n",
    "\n",
    "    result = feat_pair_df[[\"leading_item_id\", \"following_item_id\"]].copy()\n",
    "    result[\"value\"] = y_pred_value\n",
    "    result[\"value\"] = result[\"value\"].clip(lower=0).round().astype(int)\n",
    "    \n",
    "    # ìµœì†Œê°’ ë³´ì¥ (0ì´ ë„ˆë¬´ ë§ìœ¼ë©´ ë¬¸ì œ)\n",
    "    zero_count = (result[\"value\"] == 0).sum()\n",
    "    if zero_count > len(result) * 0.1:  # 10% ì´ìƒì´ 0ì´ë©´\n",
    "        non_zero_values = result[result[\"value\"] > 0][\"value\"]\n",
    "        if len(non_zero_values) > 0:\n",
    "            non_zero_mean = non_zero_values.mean()\n",
    "            min_val = max(1, int(non_zero_mean * 0.01))\n",
    "            result.loc[result[\"value\"] == 0, \"value\"] = min_val\n",
    "            print(f\"   âš ï¸ {zero_count}ê°œ 0 ê°’ì„ ìµœì†Œê°’({min_val})ìœ¼ë¡œ ëŒ€ì²´\")\n",
    "\n",
    "    print(f\"âœ… {len(result)}ê°œ ìŒ ì˜ˆì¸¡ ì™„ë£Œ (í‰ê· ê°’: {result['value'].mean():.2f}, ì¤‘ì•™ê°’: {result['value'].median():.2f})\")\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83909b1",
   "metadata": {},
   "source": [
    "## 5. ì œì¶œìš© DataFrame ë§Œë“¤ê¸°\n",
    "\n",
    "- ì´ë¯¸ sample_submission.csvë¥¼ ì“´ êµ¬ì¡°ê°€ ìˆìœ¼ë‹ˆ,\n",
    "\n",
    "- ì´ë²ˆì—” pair ì˜ˆì¸¡ ê²°ê³¼ë¥¼ sample_submissionì˜ (leading, following)ì™€ ë§¤ì¹­í•´ì„œ valueë¥¼ ì±„ìš°ëŠ” ë°©ì‹ìœ¼ë¡œ ê°ˆ ìˆ˜ ìˆì–´.\n",
    "\n",
    "- ì§€ê¸ˆ ë„ˆê°€ ì“°ë˜ make_submission_dfë¥¼ v2ìš©ìœ¼ë¡œ ì•½ê°„ ë°”ê¾¸ë©´:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6234cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission_from_pair_pred(\n",
    "    pair_pred_df: pd.DataFrame,\n",
    "    submission_template: pd.DataFrame | None = None,\n",
    "    feat_df: pd.DataFrame | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    pair_pred_df: (leading_item_id, following_item_id, value)\n",
    "    submission_template: sample_submission (optional)\n",
    "    feat_df: í”¼ì²˜ ë°ì´í„°í”„ë ˆì„ (fallbackì„ ìœ„í•œ ì•„ì´í…œë³„ í‰ê·  ê³„ì‚°ìš©)\n",
    "    \"\"\"\n",
    "    # ì…ë ¥ ê²€ì¦\n",
    "    if pair_pred_df is None or len(pair_pred_df) == 0:\n",
    "        raise ValueError(\"pair_pred_dfê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!\")\n",
    "    \n",
    "    if \"value\" not in pair_pred_df.columns:\n",
    "        raise ValueError(\"pair_pred_dfì— 'value' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "    \n",
    "    if submission_template is None:\n",
    "        # í›„ë³´ìŒë§Œ ì œì¶œí•˜ëŠ” ê²½ìš°\n",
    "        required_cols = [\"leading_item_id\", \"following_item_id\", \"value\"]\n",
    "        missing_cols = [c for c in required_cols if c not in pair_pred_df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"pair_pred_dfì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_cols}\")\n",
    "        \n",
    "        sub = pair_pred_df[required_cols].copy()\n",
    "        sub = sub.drop_duplicates()\n",
    "        return sub\n",
    "\n",
    "    # template ê¸°ì¤€ìœ¼ë¡œ ì±„ìš°ëŠ” ê²½ìš°\n",
    "    if \"leading_item_id\" not in submission_template.columns or \"following_item_id\" not in submission_template.columns:\n",
    "        raise ValueError(\"submission_templateì— 'leading_item_id' ë˜ëŠ” 'following_item_id' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "    \n",
    "    base = submission_template[[\"leading_item_id\", \"following_item_id\"]].copy()\n",
    "    \n",
    "    # pair_pred_dfì—ì„œ value_map ìƒì„±\n",
    "    if len(pair_pred_df) > 0:\n",
    "        key = list(zip(pair_pred_df[\"leading_item_id\"], pair_pred_df[\"following_item_id\"]))\n",
    "        val = pair_pred_df[\"value\"].values\n",
    "        value_map = dict(zip(key, val))\n",
    "    else:\n",
    "        value_map = {}\n",
    "        print(\"âš ï¸ pair_pred_dfê°€ ë¹„ì–´ìˆì–´ ëª¨ë“  ê°’ì— fallback ì‚¬ìš©\")\n",
    "\n",
    "    base_key = list(zip(base[\"leading_item_id\"], base[\"following_item_id\"]))\n",
    "    \n",
    "    # ì˜ˆì¸¡ê°’ì´ ì—†ëŠ” ìŒì— ëŒ€í•œ fallback ì „ëµ (ê°œì„ )\n",
    "    if feat_df is not None and \"value\" in feat_df.columns and len(feat_df) > 0:\n",
    "        try:\n",
    "            # ìµœê·¼ 6ê°œì›” ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì—¬ ë” ì •í™•í•œ í‰ê·  ê³„ì‚°\n",
    "            recent_date = feat_df[\"date\"].max()\n",
    "            recent_cutoff = recent_date - pd.DateOffset(months=6)\n",
    "            recent_feat_df = feat_df[feat_df[\"date\"] >= recent_cutoff]\n",
    "            \n",
    "            if len(recent_feat_df) > 0:\n",
    "                # ì•„ì´í…œë³„ ìµœê·¼ í‰ê· ê°’\n",
    "                item_means = recent_feat_df.groupby(\"item_id\")[\"value\"].mean().to_dict()\n",
    "                # ì•„ì´í…œë³„ ìµœê·¼ ì¤‘ì•™ê°’ (ë” robust)\n",
    "                item_medians = recent_feat_df.groupby(\"item_id\")[\"value\"].median().to_dict()\n",
    "                global_mean = float(recent_feat_df[\"value\"].mean())\n",
    "                global_median = float(recent_feat_df[\"value\"].median())\n",
    "            else:\n",
    "                # ìµœê·¼ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì „ì²´ ë°ì´í„° ì‚¬ìš©\n",
    "                item_means = feat_df.groupby(\"item_id\")[\"value\"].mean().to_dict()\n",
    "                item_medians = feat_df.groupby(\"item_id\")[\"value\"].median().to_dict()\n",
    "                global_mean = float(feat_df[\"value\"].mean())\n",
    "                global_median = float(feat_df[\"value\"].median())\n",
    "            \n",
    "            # ì˜ˆì¸¡ê°’ì´ ì—†ìœ¼ë©´ following_item_idì˜ ì¤‘ì•™ê°’ ì‚¬ìš© (í‰ê· ë³´ë‹¤ robust)\n",
    "            base[\"value\"] = [\n",
    "                value_map.get(k, item_medians.get(k[1], item_means.get(k[1], global_median))) \n",
    "                for k in base_key\n",
    "            ]\n",
    "            missing_count = sum(1 for k in base_key if k not in value_map)\n",
    "            if missing_count > 0:\n",
    "                print(f\"âš ï¸ {missing_count}ê°œ ìŒì— ëŒ€í•´ ì˜ˆì¸¡ê°’ì´ ì—†ì–´ fallback ì‚¬ìš© (ì•„ì´í…œë³„ ì¤‘ì•™ê°’/í‰ê·  ë˜ëŠ” ì „ì²´ ì¤‘ì•™ê°’)\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ fallback ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}, ì˜ˆì¸¡ê°’ ì¤‘ì•™ê°’ ì‚¬ìš©\")\n",
    "            pred_median = float(pair_pred_df[\"value\"].median()) if len(pair_pred_df) > 0 and \"value\" in pair_pred_df.columns else 1.0\n",
    "            base[\"value\"] = [value_map.get(k, pred_median) for k in base_key]\n",
    "    else:\n",
    "        # feat_dfê°€ ì—†ìœ¼ë©´ ì˜ˆì¸¡ê°’ì˜ í‰ê·  ì‚¬ìš©\n",
    "        try:\n",
    "            pred_mean = float(pair_pred_df[\"value\"].mean()) if len(pair_pred_df) > 0 and \"value\" in pair_pred_df.columns else 1.0\n",
    "        except:\n",
    "            pred_mean = 1.0\n",
    "        \n",
    "        base[\"value\"] = [value_map.get(k, pred_mean) for k in base_key]\n",
    "        missing_count = sum(1 for k in base_key if k not in value_map)\n",
    "        if missing_count > 0:\n",
    "            print(f\"âš ï¸ {missing_count}ê°œ ìŒì— ëŒ€í•´ ì˜ˆì¸¡ê°’ì´ ì—†ì–´ í‰ê· ê°’({pred_mean:.2f}) ì‚¬ìš©\")\n",
    "\n",
    "    return base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0937021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ import í™•ì¸\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# í•„ìš”í•œ ë³€ìˆ˜ë“¤ì´ ì •ì˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n",
    "required_vars = ['df_panel', 'candidates_df']\n",
    "missing_vars = [var for var in required_vars if var not in globals()]\n",
    "\n",
    "if missing_vars:\n",
    "    raise NameError(\n",
    "        f\"ë‹¤ìŒ ë³€ìˆ˜ë“¤ì´ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {', '.join(missing_vars)}\\n\"\n",
    "        f\"ë‹¤ìŒ ì…€ë“¤ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”:\\n\"\n",
    "        f\"- df_panel: íŒ¨ë„ ë°ì´í„° ìƒì„± ì…€ (ë³´í†µ ì „ì²˜ë¦¬ ì„¹ì…˜)\\n\"\n",
    "        f\"- candidates_df: ê³µí–‰ì„± í›„ë³´ìŒ ìƒì„± ì…€ (generate_comovement_candidates í•¨ìˆ˜ ì‹¤í–‰)\"\n",
    "    )\n",
    "\n",
    "# ë³€ìˆ˜ ê²€ì¦\n",
    "print(\"ğŸ“Š ë³€ìˆ˜ í™•ì¸:\")\n",
    "print(f\"   - df_panel: {len(df_panel) if df_panel is not None else 'None'} rows\")\n",
    "print(f\"   - candidates_df: {len(candidates_df) if candidates_df is not None else 'None'} pairs\")\n",
    "\n",
    "# 1. ì•„ì´í…œ í”¼ì²˜ ìƒì„± (ê¸°ì¡´ í•¨ìˆ˜ + ì‹œê³„ì—´ ì½”ì–´ í”¼ì²˜)\n",
    "print(\"\\nğŸ”§ í”¼ì²˜ ìƒì„± ì¤‘...\")\n",
    "try:\n",
    "    feat_base = make_item_features(\n",
    "        df_panel,\n",
    "        use_hs_stats=True,\n",
    "        use_ema=True,\n",
    "        use_zero_run=True\n",
    "    )          # HS/EMA/run-length\n",
    "    print(f\"   âœ… ê¸°ë³¸ í”¼ì²˜ ìƒì„± ì™„ë£Œ: {len(feat_base)} rows, {len(feat_base.columns)} columns\")\n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸ ê¸°ë³¸ í”¼ì²˜ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "    # íŒŒë¼ë¯¸í„° ì—†ì´ ì‹œë„\n",
    "    feat_base = make_item_features(df_panel)\n",
    "    print(f\"   âœ… ê¸°ë³¸ í”¼ì²˜ ìƒì„± ì™„ë£Œ (ê¸°ë³¸ íŒŒë¼ë¯¸í„°): {len(feat_base)} rows\")\n",
    "\n",
    "try:\n",
    "    feat_df = add_ts_core_features(feat_base)         # log/lag/rolling/year/month/trend_index\n",
    "    print(f\"   âœ… ì‹œê³„ì—´ ì½”ì–´ í”¼ì²˜ ì¶”ê°€ ì™„ë£Œ: {len(feat_df)} rows, {len(feat_df.columns)} columns\")\n",
    "except Exception as e:\n",
    "    print(f\"   âŒ ì‹œê³„ì—´ ì½”ì–´ í”¼ì²˜ ì¶”ê°€ ì‹¤íŒ¨: {e}\")\n",
    "    raise\n",
    "\n",
    "# 2. ëª¨ë¸ í•™ìŠµ (pair-level RF) - ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
    "train_last_t = pd.Timestamp(\"2024-12-01\")\n",
    "valid_last_t = pd.Timestamp(\"2025-06-01\")\n",
    "\n",
    "print(\"\\nğŸ”§ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "print(f\"   - í•™ìŠµ ê¸°ê°„: ~{train_last_t}\")\n",
    "print(f\"   - ê²€ì¦ ê¸°ê°„: ~{valid_last_t}\")\n",
    "print(f\"   - í›„ë³´ìŒ ìˆ˜: {len(candidates_df)}\")\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„° ìƒì„± ê°€ëŠ¥ ì—¬ë¶€ ì‚¬ì „ í™•ì¸\n",
    "feat_dates = pd.to_datetime(feat_df[\"date\"], errors=\"coerce\")\n",
    "available_dates = sorted(feat_dates.dropna().unique())\n",
    "train_dates = [d for d in available_dates if d <= train_last_t]\n",
    "\n",
    "print(f\"   - ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ìˆ˜: {len(available_dates)}\")\n",
    "print(f\"   - í•™ìŠµ ê¸°ê°„ ë‚´ ë‚ ì§œ ìˆ˜: {len(train_dates)}\")\n",
    "\n",
    "if len(train_dates) < 2:\n",
    "    print(\"âš ï¸ ê²½ê³ : í•™ìŠµì— ì‚¬ìš©í•  ì¶©ë¶„í•œ ë‚ ì§œê°€ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "    print(f\"   ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ: {available_dates[:5]}... (ìµœëŒ€ {len(available_dates)}ê°œ)\")\n",
    "\n",
    "try:\n",
    "    rf_model, feature_cols = train_pair_model_v2(\n",
    "        feat_df,\n",
    "        candidates_df,\n",
    "        train_last_t=train_last_t,\n",
    "        valid_last_t=valid_last_t,\n",
    "        n_estimators=300,  # ë” ë§ì€ íŠ¸ë¦¬ë¡œ ì„±ëŠ¥ í–¥ìƒ\n",
    "        max_depth=20,      # ë” ê¹Šì€ íŠ¸ë¦¬ë¡œ ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ\n",
    "        random_state=42,\n",
    "    )\n",
    "    print(f\"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! ì‚¬ìš©ëœ í”¼ì²˜ ìˆ˜: {len(feature_cols)}\")\n",
    "except ValueError as e:\n",
    "    print(f\"âŒ ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"\\nğŸ” ìë™ìœ¼ë¡œ train_last_t ì¡°ì • ì‹œë„...\")\n",
    "    \n",
    "    # train_last_të¥¼ ì ì§„ì ìœ¼ë¡œ ì¡°ì •í•˜ì—¬ í•™ìŠµ ë°ì´í„° ìƒì„± ì‹œë„\n",
    "    adjusted_train_last_t = train_last_t\n",
    "    success = False\n",
    "    \n",
    "    for months_back in [1, 2, 3, 6, 12]:\n",
    "        adjusted_train_last_t = train_last_t - pd.DateOffset(months=months_back)\n",
    "        adjusted_train_dates = [d for d in available_dates if d <= adjusted_train_last_t]\n",
    "        \n",
    "        if len(adjusted_train_dates) >= 2:\n",
    "            print(f\"   - {months_back}ê°œì›” ì „ìœ¼ë¡œ ì¡°ì •: {adjusted_train_last_t}\")\n",
    "            try:\n",
    "                rf_model, feature_cols = train_pair_model_v2(\n",
    "                    feat_df,\n",
    "                    candidates_df,\n",
    "                    train_last_t=adjusted_train_last_t,\n",
    "                    valid_last_t=valid_last_t,\n",
    "                    n_estimators=300,\n",
    "                    max_depth=20,\n",
    "                    random_state=42,\n",
    "                )\n",
    "                print(f\"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! (ì¡°ì •ëœ í•™ìŠµ ê¸°ê°„ ì‚¬ìš©)\")\n",
    "                print(f\"   ì‚¬ìš©ëœ í”¼ì²˜ ìˆ˜: {len(feature_cols)}\")\n",
    "                success = True\n",
    "                break\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "    if not success:\n",
    "        print(\"\\nâŒ ìë™ ì¡°ì • ì‹¤íŒ¨. ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:\")\n",
    "        print(\"   1. candidates_dfì— ìœ íš¨í•œ ìŒì´ ìˆëŠ”ì§€ í™•ì¸\")\n",
    "        print(\"   2. feat_dfì— ì¶©ë¶„í•œ ë‚ ì§œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸\")\n",
    "        print(\"   3. candidates_dfì˜ item_idê°€ feat_dfì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\")\n",
    "        raise ValueError(\"í•™ìŠµ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "# 3. sample_submission ë¡œë“œ (ì˜ˆì¸¡ ì „ì— ë¡œë“œ)\n",
    "sample_path = os.path.join(\"..\", \"sample_submission.csv\")\n",
    "if os.path.exists(sample_path):\n",
    "    submission_template = pd.read_csv(sample_path)\n",
    "    print(f\"âœ… submission template ë¡œë“œ ì™„ë£Œ: {len(submission_template)}ê°œ ìŒ\")\n",
    "else:\n",
    "    submission_template = None\n",
    "    print(\"[WARN] sample_submission.csvë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í›„ë³´ìŒë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# 4. 2025-08 ì˜ˆì¸¡ (pair ë‹¨ìœ„) - submission templateì˜ ëª¨ë“  ìŒì— ëŒ€í•´ ì˜ˆì¸¡!\n",
    "print(\"\\nğŸ”® ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...\")\n",
    "try:\n",
    "    pair_pred_df = predict_aug_2025_values_v2(\n",
    "        rf_model,\n",
    "        feat_df,\n",
    "        candidates_df,\n",
    "        feature_cols,\n",
    "        submission_template=submission_template,  # ëª¨ë“  ìŒì— ëŒ€í•´ ì˜ˆì¸¡ ì‹œë„\n",
    "    )\n",
    "    print(f\"   âœ… ì˜ˆì¸¡ ì™„ë£Œ: {len(pair_pred_df)}ê°œ ìŒ ì˜ˆì¸¡ë¨\")\n",
    "except Exception as e:\n",
    "    print(f\"   âŒ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}\")\n",
    "    raise\n",
    "\n",
    "# 5. sample_submissionì„ ê¸°ì¤€ìœ¼ë¡œ ì œì¶œ íŒŒì¼ ìƒì„± (fallback í¬í•¨)\n",
    "print(\"\\nğŸ“ Submission íŒŒì¼ ìƒì„± ì¤‘...\")\n",
    "try:\n",
    "    submission_df = make_submission_from_pair_pred(\n",
    "        pair_pred_df,\n",
    "        submission_template=submission_template,\n",
    "        feat_df=feat_df,  # fallbackì„ ìœ„í•œ í”¼ì²˜ ë°ì´í„° ì „ë‹¬\n",
    "    )\n",
    "    print(f\"   âœ… Submission ìƒì„± ì™„ë£Œ: {len(submission_df)} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Submission ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "    raise\n",
    "\n",
    "# 6. íŒŒì¼ ì €ì¥ ë° ê²€ì¦\n",
    "try:\n",
    "    submission_df.to_csv(\"submission_kmu_v2_pair_rf.csv\", index=False)\n",
    "    print(f\"\\nâœ… íŒŒì¼ ì €ì¥ ì™„ë£Œ: submission_kmu_v2_pair_rf.csv\")\n",
    "    print(f\"\\nğŸ“Š Submission í†µê³„:\")\n",
    "    print(f\"   - ì´ row ìˆ˜: {len(submission_df):,}\")\n",
    "    print(f\"   - í‰ê· ê°’: {submission_df['value'].mean():.2f}\")\n",
    "    print(f\"   - ì¤‘ì•™ê°’: {submission_df['value'].median():.2f}\")\n",
    "    print(f\"   - ìµœì†Œê°’: {submission_df['value'].min():,}\")\n",
    "    print(f\"   - ìµœëŒ€ê°’: {submission_df['value'].max():,}\")\n",
    "    print(f\"   - 0 ê°’ ê°œìˆ˜: {(submission_df['value'] == 0).sum():,}\")\n",
    "    print(f\"\\nì²˜ìŒ 10ê°œ í–‰:\")\n",
    "    print(submission_df.head(10))\n",
    "except Exception as e:\n",
    "    print(f\"   âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eadaa4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb68bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df984f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67466d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# ê°„ë‹¨í•œ baseline ëª¨ë¸: itemë³„ í‰ê·  value ê¸°ë°˜ ì˜ˆì¸¡\n",
    "# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì˜ì¡´ ì—†ì´ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„\n",
    "# ----------------------------------------------------\n",
    "'''\n",
    "class SimpleItemMeanModel:\n",
    "    \"\"\"item_idë³„ í‰ê·  valueë¥¼ ì‚¬ìš©í•˜ëŠ” ì•„ì£¼ ë‹¨ìˆœí•œ íšŒê·€ ëª¨ë¸.\n",
    "\n",
    "    predict(X)ëŠ” X['item_id']ë¥¼ ì‚¬ìš©í•´ í•´ë‹¹ ì•„ì´í…œ í‰ê· ê°’ì„ ë°˜í™˜í•˜ê³ ,\n",
    "    ì—†ëŠ” ì•„ì´í…œì€ ì „ì²´ í‰ê· (global_mean)ì„ ì‚¬ìš©í•œë‹¤.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, item_means: dict, global_mean: float):\n",
    "        self.item_means = item_means\n",
    "        self.global_mean = global_mean\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        if \"item_id\" not in X.columns:\n",
    "            raise ValueError(\"ì…ë ¥ Xì—ëŠ” 'item_id' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "        s = X[\"item_id\"].map(self.item_means).fillna(self.global_mean)\n",
    "        return s.values\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ (baseline ë²„ì „)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def train_value_model(\n",
    "    feat_df: pd.DataFrame,\n",
    "    candidates_df: pd.DataFrame,\n",
    "    train_last_t: pd.Timestamp,\n",
    "    valid_last_t: pd.Timestamp,\n",
    "):\n",
    "    \"\"\"ì•„ì´í…œë³„ í‰ê·  valueë¥¼ ì‚¬ìš©í•˜ëŠ” ê°„ë‹¨í•œ baseline ëª¨ë¸ í•™ìŠµ.\n",
    "\n",
    "    - feat_df: make_item_featuresë¡œ ìƒì„±ëœ í”¼ì²˜ DF (item_id, date, value ë“± í¬í•¨)\n",
    "    - candidates_df: ê³µí–‰ì„± í›„ë³´ìŒ (í˜„ì¬ ë²„ì „ì—ì„œëŠ” item í•„í„°ìš©ìœ¼ë¡œë§Œ í™œìš© ê°€ëŠ¥)\n",
    "    - train_last_t: í•™ìŠµì— ì‚¬ìš©í•  ë§ˆì§€ë§‰ date (ì´ ë‚ ì§œ ì´í•˜ë§Œ ì‚¬ìš©)\n",
    "    - valid_last_t: ê²€ì¦ìš© ë§ˆì§€ë§‰ date (í–¥í›„ ê³ ê¸‰ ëª¨ë¸ì—ì„œ í™œìš© ê°€ëŠ¥; í˜„ì¬ëŠ” ë¯¸ì‚¬ìš©)\n",
    "\n",
    "    ë°˜í™˜:\n",
    "      - model: SimpleItemMeanModel ì¸ìŠ¤í„´ìŠ¤\n",
    "      - feature_cols: ì˜ˆì¸¡ ì‹œ ì‚¬ìš©í•  í”¼ì²˜ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ (ì—¬ê¸°ì„œëŠ” 'item_id'ë§Œ ì‚¬ìš©)\n",
    "    \"\"\"\n",
    "    df = feat_df.copy()\n",
    "\n",
    "    # date ì •ë¦¬\n",
    "    if \"date\" not in df.columns:\n",
    "        raise ValueError(\"feat_dfì—ëŠ” 'date' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "\n",
    "    # value ì»¬ëŸ¼ í™•ì¸\n",
    "    if \"value\" not in df.columns:\n",
    "        raise ValueError(\"feat_dfì—ëŠ” ì˜ˆì¸¡ ëŒ€ìƒì¸ 'value' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    # ê³µí–‰ì„± í›„ë³´ìŒì— ë“±ì¥í•˜ëŠ” itemë§Œ í•™ìŠµì— ì‚¬ìš©í•˜ê³  ì‹¶ë‹¤ë©´ í•„í„°ë§ (ì„ íƒì )\n",
    "    if not candidates_df.empty:\n",
    "        items_in_pairs = set(candidates_df[\"leading_item_id\"]).union(\n",
    "            set(candidates_df[\"following_item_id\"])\n",
    "        )\n",
    "        df = df[df[\"item_id\"].isin(items_in_pairs)]\n",
    "\n",
    "    # í•™ìŠµ êµ¬ê°„: train_last_t ì´í•˜\n",
    "    train_mask = df[\"date\"] <= train_last_t\n",
    "    train_df = df[train_mask].dropna(subset=[\"value\"])\n",
    "\n",
    "    if train_df.empty:\n",
    "        raise ValueError(\"train_value_model: í•™ìŠµì— ì‚¬ìš©í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    # itemë³„ í‰ê·  ë° ì „ì²´ í‰ê·  ê³„ì‚°\n",
    "    item_means = train_df.groupby(\"item_id\")[\"value\"].mean().to_dict()\n",
    "    global_mean = float(train_df[\"value\"].mean())\n",
    "\n",
    "    model = SimpleItemMeanModel(item_means, global_mean)\n",
    "    feature_cols = [\"item_id\"]  # ì´ baselineì—ì„œëŠ” item_idë§Œ ì‚¬ìš©\n",
    "\n",
    "    return model, feature_cols\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4. 2025-08 value ì˜ˆì¸¡ í•¨ìˆ˜ (baseline ë²„ì „)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def predict_aug_2025_values(\n",
    "    model: SimpleItemMeanModel,\n",
    "    feat_df: pd.DataFrame,\n",
    "    candidates_df: pd.DataFrame,\n",
    "    feature_cols: list[str],\n",
    "    items_override: list[str] | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"ê°„ë‹¨ baseline: ê° item_idì— ëŒ€í•´ 2025-08-01ì˜ valueë¥¼ ì˜ˆì¸¡.\n",
    "\n",
    "    `items_override`ê°€ ì£¼ì–´ì§€ë©´ í•´ë‹¹ ëª©ë¡ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê³ ,\n",
    "    ì—†ìœ¼ë©´ ê³µí–‰ì„± í›„ë³´ìŒ ì•„ì´í…œ ë˜ëŠ” ì „ì²´ item ëª©ë¡ì„ ë”°ë¥¸ë‹¤.\n",
    "    (ì¶”í›„ ê³ ê¸‰ ë²„ì „ì—ì„œëŠ” ë¡¤ë§/lag í”¼ì²˜ë¥¼ ì‚¬ìš©í•´ one-step ahead ì˜ˆì¸¡ìœ¼ë¡œ êµì²´ ê°€ëŠ¥)\n",
    "    \"\"\"\n",
    "    df = feat_df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "\n",
    "    # ì˜ˆì¸¡ ëŒ€ìƒ item ëª©ë¡ ê²°ì •\n",
    "    if items_override is not None and len(items_override) > 0:\n",
    "        items = sorted(pd.Series(items_override).dropna().unique())\n",
    "    elif not candidates_df.empty:\n",
    "        items = sorted(\n",
    "            set(candidates_df[\"leading_item_id\"]).union(\n",
    "                set(candidates_df[\"following_item_id\"])\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        items = sorted(df[\"item_id\"].dropna().unique())\n",
    "\n",
    "    target_date = pd.Timestamp(\"2025-08-01\")\n",
    "\n",
    "    # ì˜ˆì¸¡ìš© ì…ë ¥ DataFrame (ëª¨ë¸ì€ item_idë§Œ ì‚¬ìš©)\n",
    "    X_pred = pd.DataFrame({\n",
    "        \"item_id\": items,\n",
    "        \"date\": target_date,\n",
    "    })\n",
    "\n",
    "    preds = model.predict(X_pred)\n",
    "\n",
    "    pred_df = X_pred.copy()\n",
    "    pred_df[\"pred_value\"] = preds\n",
    "\n",
    "    return pred_df\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 5. ì œì¶œìš© DataFrame ìƒì„± (ê°„ë‹¨ ë²„ì „)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def make_submission_df(\n",
    "    pred_df: pd.DataFrame,\n",
    "    submission_template: pd.DataFrame | None = None,\n",
    "    candidate_pairs: pd.DataFrame | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì œì¶œ í¬ë§·(leading/following/value)ìœ¼ë¡œ ì •ë¦¬í•œë‹¤.\n",
    "\n",
    "    - submission_template: ëŒ€íšŒì—ì„œ ì œê³µí•œ sample_submission í˜•íƒœì˜ DataFrame.\n",
    "      (leading_item_id, following_item_id ì»¬ëŸ¼ì´ ìˆì–´ì•¼ í•¨)\n",
    "    - candidate_pairs: í…œí”Œë¦¿ì´ ì—†ì„ ë•Œ ì‚¬ìš©í•  ê³µí–‰ì„± í›„ë³´ìŒ DataFrame.\n",
    "    \"\"\"\n",
    "    if \"pred_value\" not in pred_df.columns:\n",
    "        raise ValueError(\"pred_dfì—ëŠ” 'pred_value' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    if submission_template is not None:\n",
    "        base = submission_template[[\"leading_item_id\", \"following_item_id\"]].copy()\n",
    "    elif candidate_pairs is not None and not candidate_pairs.empty:\n",
    "        base = candidate_pairs[[\"leading_item_id\", \"following_item_id\"]].copy()\n",
    "    else:\n",
    "        raise ValueError(\"ì œì¶œ í…œí”Œë¦¿ì´ë‚˜ í›„ë³´ìŒ DataFrame ì¤‘ í•˜ë‚˜ëŠ” ë°˜ë“œì‹œ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    value_map = pred_df.set_index(\"item_id\")[\"pred_value\"]\n",
    "    default_value = float(pred_df[\"pred_value\"].mean())\n",
    "\n",
    "    base[\"value\"] = base[\"following_item_id\"].map(value_map).fillna(default_value)\n",
    "\n",
    "    return base\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51ce247",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# 2. item feature ìƒì„±\n",
    "feat_df = make_item_features(df_panel)\n",
    "\n",
    "# 3. ëª¨ë¸ í•™ìŠµ\n",
    "model, feature_cols = train_value_model(\n",
    "    feat_df,\n",
    "    candidates_df,\n",
    "    train_last_t=pd.Timestamp(\"2024-12-01\"),\n",
    "    valid_last_t=pd.Timestamp(\"2025-06-01\")\n",
    ")\n",
    "\n",
    "# 4. ì œì¶œ í…œí”Œë¦¿ ì •ë³´ ë¡œë“œ\n",
    "submission_template_path = os.path.join(\"..\", \"sample_submission.csv\")\n",
    "submission_template = None\n",
    "submission_following_items: list[str] | None = None\n",
    "\n",
    "if os.path.exists(submission_template_path):\n",
    "    submission_template = pd.read_csv(submission_template_path)\n",
    "    submission_following_items = (\n",
    "        submission_template[\"following_item_id\"].dropna().unique().tolist()\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        f\"[WARN] ì œì¶œ í…œí”Œë¦¿({submission_template_path})ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ í›„ë³´ìŒ ê¸°ë°˜ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.\"\n",
    "    )\n",
    "\n",
    "# 5. 2025-08 ì˜ˆì¸¡\n",
    "pred_df = predict_aug_2025_values(\n",
    "    model,\n",
    "    feat_df,\n",
    "    candidates_df,\n",
    "    feature_cols,\n",
    "    items_override=submission_following_items,\n",
    ")\n",
    "\n",
    "# 6. ì œì¶œ CSV ìƒì„±\n",
    "submission_df = make_submission_df(\n",
    "    pred_df,\n",
    "    submission_template=submission_template,\n",
    "    candidate_pairs=candidates_df,\n",
    ")\n",
    "submission_df.to_csv(\"submission_kmu_v1.csv\", index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84851370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# âœ… ê°œì„ ëœ submission ìƒì„± ì½”ë“œ (pair-level ì˜ˆì¸¡ ì‚¬ìš©)\n",
    "# ============================================================\n",
    "# ì´ì „ ì½”ë“œëŠ” item-level ì˜ˆì¸¡ë§Œ ì‚¬ìš©í•´ì„œ ì ìˆ˜ê°€ ë‚®ì•˜ìŠµë‹ˆë‹¤.\n",
    "# ì´ì œ pair-level ì˜ˆì¸¡ì„ ì‚¬ìš©í•˜ì—¬ (leading_item_id, following_item_id) ìŒë³„ë¡œ ì˜ˆì¸¡í•©ë‹ˆë‹¤.\n",
    "\n",
    "# í•„ìš”í•œ ë³€ìˆ˜ë“¤ì´ ì •ì˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n",
    "required_vars = ['df_panel', 'candidates_df']\n",
    "missing_vars = [var for var in required_vars if var not in globals()]\n",
    "\n",
    "if missing_vars:\n",
    "    raise NameError(\n",
    "        f\"ë‹¤ìŒ ë³€ìˆ˜ë“¤ì´ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {', '.join(missing_vars)}\\n\"\n",
    "        f\"ë‹¤ìŒ ì…€ë“¤ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”:\\n\"\n",
    "        f\"- df_panel: íŒ¨ë„ ë°ì´í„° ìƒì„± ì…€ (ë³´í†µ ì „ì²˜ë¦¬ ì„¹ì…˜)\\n\"\n",
    "        f\"- candidates_df: ê³µí–‰ì„± í›„ë³´ìŒ ìƒì„± ì…€ (generate_comovement_candidates í•¨ìˆ˜ ì‹¤í–‰)\"\n",
    "    )\n",
    "\n",
    "# 1. ì•„ì´í…œ í”¼ì²˜ ìƒì„± (ê¸°ì¡´ í•¨ìˆ˜ + ì‹œê³„ì—´ ì½”ì–´ í”¼ì²˜)\n",
    "feat_base = make_item_features(df_panel)          # HS/EMA/run-length\n",
    "feat_df = add_ts_core_features(feat_base)         # log/lag/rolling/year/month/trend_index\n",
    "\n",
    "# 2. ëª¨ë¸ í•™ìŠµ (pair-level RF) - ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
    "train_last_t = pd.Timestamp(\"2024-12-01\")\n",
    "valid_last_t = pd.Timestamp(\"2025-06-01\")\n",
    "\n",
    "print(\"ğŸ”§ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "rf_model, feature_cols = train_pair_model_v2(\n",
    "    feat_df,\n",
    "    candidates_df,\n",
    "    train_last_t=train_last_t,\n",
    "    valid_last_t=valid_last_t,\n",
    "    n_estimators=300,  # ë” ë§ì€ íŠ¸ë¦¬ë¡œ ì„±ëŠ¥ í–¥ìƒ\n",
    "    max_depth=20,      # ë” ê¹Šì€ íŠ¸ë¦¬ë¡œ ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ\n",
    "    random_state=42,\n",
    ")\n",
    "print(f\"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! ì‚¬ìš©ëœ í”¼ì²˜ ìˆ˜: {len(feature_cols)}\")\n",
    "\n",
    "# 3. sample_submission ë¡œë“œ (ì˜ˆì¸¡ ì „ì— ë¡œë“œ)\n",
    "sample_path = os.path.join(\"..\", \"sample_submission.csv\")\n",
    "if os.path.exists(sample_path):\n",
    "    submission_template = pd.read_csv(sample_path)\n",
    "    print(f\"âœ… submission template ë¡œë“œ ì™„ë£Œ: {len(submission_template)}ê°œ ìŒ\")\n",
    "else:\n",
    "    submission_template = None\n",
    "    print(\"[WARN] sample_submission.csvë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í›„ë³´ìŒë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# 4. 2025-08 ì˜ˆì¸¡ (pair ë‹¨ìœ„) - submission templateì˜ ëª¨ë“  ìŒì— ëŒ€í•´ ì˜ˆì¸¡!\n",
    "pair_pred_df = predict_aug_2025_values_v2(\n",
    "    rf_model,\n",
    "    feat_df,\n",
    "    candidates_df,\n",
    "    feature_cols,\n",
    "    submission_template=submission_template,  # ëª¨ë“  ìŒì— ëŒ€í•´ ì˜ˆì¸¡ ì‹œë„\n",
    ")\n",
    "\n",
    "# 5. sample_submissionì„ ê¸°ì¤€ìœ¼ë¡œ ì œì¶œ íŒŒì¼ ìƒì„± (fallback í¬í•¨)\n",
    "submission_df = make_submission_from_pair_pred(\n",
    "    pair_pred_df,\n",
    "    submission_template=submission_template,\n",
    "    feat_df=feat_df,  # fallbackì„ ìœ„í•œ í”¼ì²˜ ë°ì´í„° ì „ë‹¬\n",
    ")\n",
    "\n",
    "# í›„ì²˜ë¦¬: ì´ìƒì¹˜ ì²˜ë¦¬ ë° ìŠ¤ë¬´ë”©\n",
    "print(\"\\nğŸ”§ í›„ì²˜ë¦¬ ì‹œì‘...\")\n",
    "\n",
    "# 1. ìŒìˆ˜ ê°’ ì œê±°\n",
    "submission_df[\"value\"] = submission_df[\"value\"].clip(lower=0)\n",
    "\n",
    "# 2. ì´ìƒì¹˜ ì²˜ë¦¬ (99.9 percentile ì´ìƒì€ ì œí•œ)\n",
    "if len(submission_df) > 0:\n",
    "    upper_bound = submission_df[\"value\"].quantile(0.999)\n",
    "    submission_df[\"value\"] = submission_df[\"value\"].clip(upper=upper_bound)\n",
    "    print(f\"   ì´ìƒì¹˜ ì²˜ë¦¬: ìƒí•œ {upper_bound:.2f}\")\n",
    "\n",
    "# 3. 0 ê°’ì´ ë„ˆë¬´ ë§ìœ¼ë©´ ìµœì†Œê°’ìœ¼ë¡œ ëŒ€ì²´ (ë°ì´í„° í’ˆì§ˆ ê°œì„ )\n",
    "zero_count = (submission_df[\"value\"] == 0).sum()\n",
    "if zero_count > len(submission_df) * 0.5:  # 50% ì´ìƒì´ 0ì´ë©´\n",
    "    non_zero_mean = submission_df[submission_df[\"value\"] > 0][\"value\"].mean()\n",
    "    if non_zero_mean > 0:\n",
    "        submission_df.loc[submission_df[\"value\"] == 0, \"value\"] = non_zero_mean * 0.1\n",
    "        print(f\"   âš ï¸ {zero_count}ê°œ 0 ê°’ì„ ìµœì†Œê°’({non_zero_mean * 0.1:.2f})ìœ¼ë¡œ ëŒ€ì²´\")\n",
    "\n",
    "# 4. ìµœì¢… ê²€ì¦\n",
    "submission_df[\"value\"] = submission_df[\"value\"].round().astype(int)\n",
    "submission_df[\"value\"] = submission_df[\"value\"].clip(lower=0)\n",
    "\n",
    "# 5. submission ì €ì¥\n",
    "submission_df.to_csv(\"submission_kmu_v1.csv\", index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… pair-level ì˜ˆì¸¡ìœ¼ë¡œ submission_kmu_v1.csv ìƒì„± ì™„ë£Œ!\")\n",
    "print(f\"ì œì¶œ row ìˆ˜: {len(submission_df)}\")\n",
    "print(f\"ì˜ˆì¸¡ëœ ìŒ ìˆ˜: {len(pair_pred_df)}\")\n",
    "print(f\"í‰ê·  ì˜ˆì¸¡ê°’: {submission_df['value'].mean():.2f}\")\n",
    "print(f\"ì¤‘ì•™ê°’ ì˜ˆì¸¡ê°’: {submission_df['value'].median():.2f}\")\n",
    "print(f\"ì˜ˆì¸¡ê°’ ë²”ìœ„: {submission_df['value'].min():.2f} ~ {submission_df['value'].max():.2f}\")\n",
    "print(f\"0 ê°’ ê°œìˆ˜: {(submission_df['value'] == 0).sum()} ({100*(submission_df['value'] == 0).sum()/len(submission_df):.1f}%)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nì²˜ìŒ 10ê°œ í–‰:\")\n",
    "print(submission_df.head(10))\n",
    "print(\"\\nğŸ“Š ì˜ˆì¸¡ê°’ ë¶„í¬:\")\n",
    "print(submission_df['value'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d826d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ğŸš€ ìµœì¢… ìµœì í™” ë²„ì „ - ëª¨ë“  ê°œì„  ì‚¬í•­ ì ìš©\n",
    "# ============================================================\n",
    "# ì´ ì…€ì€ ëª¨ë“  ìµœì í™”ë¥¼ í¬í•¨í•œ ìµœì¢… ë²„ì „ì…ë‹ˆë‹¤.\n",
    "# ì œì¶œ ì „ ë°˜ë“œì‹œ ì´ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”!\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í•„ìš”í•œ ë³€ìˆ˜ í™•ì¸\n",
    "required_vars = ['df_panel', 'candidates_df']\n",
    "missing_vars = [var for var in required_vars if var not in globals()]\n",
    "\n",
    "if missing_vars:\n",
    "    raise NameError(\n",
    "        f\"ë‹¤ìŒ ë³€ìˆ˜ë“¤ì´ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {', '.join(missing_vars)}\\n\"\n",
    "        f\"ë‹¤ìŒ ì…€ë“¤ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”:\\n\"\n",
    "        f\"- df_panel: íŒ¨ë„ ë°ì´í„° ìƒì„± ì…€\\n\"\n",
    "        f\"- candidates_df: ê³µí–‰ì„± í›„ë³´ìŒ ìƒì„± ì…€\"\n",
    "    )\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸš€ ìµœì¢… ìµœì í™” submission ìƒì„± ì‹œì‘\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. ê°•í™”ëœ í”¼ì²˜ ìƒì„±\n",
    "print(\"\\nğŸ“Š 1ë‹¨ê³„: í”¼ì²˜ ìƒì„±...\")\n",
    "feat_base = make_item_features(df_panel, use_hs_stats=True, use_ema=True, use_zero_run=True)\n",
    "feat_df = add_ts_core_features(feat_base)\n",
    "print(f\"   âœ… í”¼ì²˜ ìƒì„± ì™„ë£Œ: {len(feat_df.columns)}ê°œ ì»¬ëŸ¼\")\n",
    "\n",
    "# 2. ìµœì í™”ëœ ëª¨ë¸ í•™ìŠµ\n",
    "print(\"\\nğŸ”§ 2ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ...\")\n",
    "train_last_t = pd.Timestamp(\"2024-12-01\")\n",
    "valid_last_t = pd.Timestamp(\"2025-06-01\")\n",
    "\n",
    "rf_model, feature_cols = train_pair_model_v2(\n",
    "    feat_df,\n",
    "    candidates_df,\n",
    "    train_last_t=train_last_t,\n",
    "    valid_last_t=valid_last_t,\n",
    "    n_estimators=300,  # ìµœëŒ€ ì„±ëŠ¥\n",
    "    max_depth=20,      # ê¹Šì€ íŠ¸ë¦¬\n",
    "    random_state=42,\n",
    ")\n",
    "print(f\"   âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: {len(feature_cols)}ê°œ í”¼ì²˜ ì‚¬ìš©\")\n",
    "\n",
    "# 3. Submission template ë¡œë“œ\n",
    "print(\"\\nğŸ“‹ 3ë‹¨ê³„: Submission template ë¡œë“œ...\")\n",
    "sample_path = os.path.join(\"..\", \"sample_submission.csv\")\n",
    "if os.path.exists(sample_path):\n",
    "    submission_template = pd.read_csv(sample_path)\n",
    "    print(f\"   âœ… Template ë¡œë“œ ì™„ë£Œ: {len(submission_template)}ê°œ ìŒ\")\n",
    "else:\n",
    "    submission_template = None\n",
    "    print(\"   âš ï¸ Template ì—†ìŒ, candidates_df ì‚¬ìš©\")\n",
    "\n",
    "# 4. ëª¨ë“  ìŒì— ëŒ€í•´ ì˜ˆì¸¡\n",
    "print(\"\\nğŸ”® 4ë‹¨ê³„: ì˜ˆì¸¡ ìˆ˜í–‰...\")\n",
    "pair_pred_df = predict_aug_2025_values_v2(\n",
    "    rf_model,\n",
    "    feat_df,\n",
    "    candidates_df,\n",
    "    feature_cols,\n",
    "    submission_template=submission_template,\n",
    ")\n",
    "\n",
    "# 5. Submission ìƒì„± (fallback í¬í•¨)\n",
    "print(\"\\nğŸ“ 5ë‹¨ê³„: Submission ìƒì„±...\")\n",
    "submission_df = make_submission_from_pair_pred(\n",
    "    pair_pred_df,\n",
    "    submission_template=submission_template,\n",
    "    feat_df=feat_df,\n",
    ")\n",
    "\n",
    "# 6. í›„ì²˜ë¦¬ ë° ê²€ì¦\n",
    "print(\"\\nâœ¨ 6ë‹¨ê³„: í›„ì²˜ë¦¬ ë° ê²€ì¦...\")\n",
    "\n",
    "# submission_dfê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸\n",
    "if len(submission_df) == 0:\n",
    "    raise ValueError(\"âŒ submission_dfê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤! ì˜ˆì¸¡ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ì´ìƒì¹˜ ì²˜ë¦¬\n",
    "submission_df[\"value\"] = submission_df[\"value\"].clip(lower=0)\n",
    "if len(submission_df) > 0:\n",
    "    try:\n",
    "        upper_bound = submission_df[\"value\"].quantile(0.999)\n",
    "        if not pd.isna(upper_bound):\n",
    "            submission_df[\"value\"] = submission_df[\"value\"].clip(upper=upper_bound)\n",
    "            print(f\"   âœ… ì´ìƒì¹˜ ì²˜ë¦¬: ìƒí•œ {upper_bound:,.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ ì´ìƒì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}\")\n",
    "\n",
    "# 0 ê°’ ì²˜ë¦¬ (ë„ˆë¬´ ë§ìœ¼ë©´ ìµœì†Œê°’ìœ¼ë¡œ)\n",
    "zero_count = (submission_df[\"value\"] == 0).sum()\n",
    "if zero_count > len(submission_df) * 0.3:  # 30% ì´ìƒì´ 0ì´ë©´\n",
    "    non_zero_values = submission_df[submission_df[\"value\"] > 0][\"value\"]\n",
    "    if len(non_zero_values) > 0:\n",
    "        try:\n",
    "            min_val = max(1, int(non_zero_values.quantile(0.1)))\n",
    "            submission_df.loc[submission_df[\"value\"] == 0, \"value\"] = min_val\n",
    "            print(f\"   âš ï¸ {zero_count}ê°œ 0 ê°’ì„ ìµœì†Œê°’({min_val})ìœ¼ë¡œ ëŒ€ì²´\")\n",
    "        except Exception as e:\n",
    "            # quantile ì‹¤íŒ¨ ì‹œ í‰ê· ì˜ 10% ì‚¬ìš©\n",
    "            min_val = max(1, int(non_zero_values.mean() * 0.1))\n",
    "            submission_df.loc[submission_df[\"value\"] == 0, \"value\"] = min_val\n",
    "            print(f\"   âš ï¸ {zero_count}ê°œ 0 ê°’ì„ ìµœì†Œê°’({min_val})ìœ¼ë¡œ ëŒ€ì²´ (í‰ê·  ê¸°ë°˜)\")\n",
    "\n",
    "# ìµœì¢… ì •ë¦¬\n",
    "submission_df[\"value\"] = pd.to_numeric(submission_df[\"value\"], errors=\"coerce\").fillna(0)\n",
    "submission_df[\"value\"] = submission_df[\"value\"].round().astype(int)\n",
    "submission_df[\"value\"] = submission_df[\"value\"].clip(lower=0)\n",
    "\n",
    "# 7. ì €ì¥ ë° ë¦¬í¬íŠ¸\n",
    "submission_df.to_csv(\"submission_kmu_v1.csv\", index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… ìµœì¢… submission_kmu_v1.csv ìƒì„± ì™„ë£Œ!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nğŸ“Š í†µê³„:\")\n",
    "print(f\"   - ì œì¶œ row ìˆ˜: {len(submission_df):,}\")\n",
    "print(f\"   - ì˜ˆì¸¡ëœ ìŒ ìˆ˜: {len(pair_pred_df):,}\")\n",
    "print(f\"   - í‰ê· ê°’: {submission_df['value'].mean():,.2f}\")\n",
    "print(f\"   - ì¤‘ì•™ê°’: {submission_df['value'].median():,.2f}\")\n",
    "print(f\"   - ìµœì†Œê°’: {submission_df['value'].min():,}\")\n",
    "print(f\"   - ìµœëŒ€ê°’: {submission_df['value'].max():,}\")\n",
    "print(f\"   - 0 ê°’: {(submission_df['value'] == 0).sum():,} ({100*(submission_df['value'] == 0).sum()/len(submission_df):.1f}%)\")\n",
    "print(f\"\\nğŸ“ˆ ë¶„ìœ„ìˆ˜:\")\n",
    "try:\n",
    "    print(f\"   - 25%: {submission_df['value'].quantile(0.25):,.0f}\")\n",
    "    print(f\"   - 50%: {submission_df['value'].quantile(0.50):,.0f}\")\n",
    "    print(f\"   - 75%: {submission_df['value'].quantile(0.75):,.0f}\")\n",
    "    print(f\"   - 95%: {submission_df['value'].quantile(0.95):,.0f}\")\n",
    "    print(f\"   - 99%: {submission_df['value'].quantile(0.99):,.0f}\")\n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸ ë¶„ìœ„ìˆ˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ ì²˜ìŒ 20ê°œ í–‰:\")\n",
    "try:\n",
    "    print(submission_df.head(20).to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(submission_df.head(20))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¯ ì œì¶œ ì¤€ë¹„ ì™„ë£Œ! submission_kmu_v1.csv íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bffc9d4",
   "metadata": {},
   "source": [
    "## ğŸ¯ ìµœì¢… ì ê²€ ì²´í¬ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "ì œì¶œ ì „ ë°˜ë“œì‹œ í™•ì¸í•˜ì„¸ìš”:\n",
    "\n",
    "### âœ… í•„ìˆ˜ í™•ì¸ ì‚¬í•­\n",
    "\n",
    "1. **ë°ì´í„° ì¤€ë¹„**\n",
    "   - [ ] `df_panel` ìƒì„± ì™„ë£Œ\n",
    "   - [ ] `candidates_df` ìƒì„± ì™„ë£Œ (ê³µí–‰ì„± í›„ë³´ìŒ)\n",
    "   - [ ] `sample_submission.csv` íŒŒì¼ ì¡´ì¬ í™•ì¸\n",
    "\n",
    "2. **ëª¨ë¸ ì„¤ì •**\n",
    "   - [ ] `n_estimators=300` (ì¶©ë¶„í•œ íŠ¸ë¦¬ ìˆ˜)\n",
    "   - [ ] `max_depth=20` (ê¹Šì€ íŠ¸ë¦¬)\n",
    "   - [ ] `submission_template` ì „ë‹¬ í™•ì¸\n",
    "\n",
    "3. **ì˜ˆì¸¡ ë²”ìœ„**\n",
    "   - [ ] submission templateì˜ **ëª¨ë“  ìŒ**ì— ëŒ€í•´ ì˜ˆì¸¡ ì‹œë„\n",
    "   - [ ] ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ ìŒì€ fallback ì „ëµ ì ìš©\n",
    "\n",
    "4. **í›„ì²˜ë¦¬**\n",
    "   - [ ] ìŒìˆ˜ ê°’ ì œê±°\n",
    "   - [ ] ì´ìƒì¹˜ ì²˜ë¦¬ (99.9 percentile)\n",
    "   - [ ] 0 ê°’ì´ ë„ˆë¬´ ë§ìœ¼ë©´ ìµœì†Œê°’ìœ¼ë¡œ ëŒ€ì²´\n",
    "\n",
    "5. **ìµœì¢… ê²€ì¦**\n",
    "   - [ ] ëª¨ë“  ê°’ì´ 0ì´ ì•„ë‹Œì§€ í™•ì¸\n",
    "   - [ ] submission íŒŒì¼ row ìˆ˜ í™•ì¸ (9900ê°œ ì´ìƒ)\n",
    "   - [ ] ê°’ì˜ ë¶„í¬ê°€ í•©ë¦¬ì ì¸ì§€ í™•ì¸\n",
    "\n",
    "### ğŸš€ ì„±ëŠ¥ í–¥ìƒ íŒ\n",
    "\n",
    "- **í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§**: weight, quantityë„ í™œìš©\n",
    "- **ëª¨ë¸ ê°•í™”**: ë” ë§ì€ íŠ¸ë¦¬, ë” ê¹Šì€ íŠ¸ë¦¬\n",
    "- **í›„ì²˜ë¦¬**: ì´ìƒì¹˜ ì œê±°, ìŠ¤ë¬´ë”©\n",
    "- **Fallback ì „ëµ**: ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ ìŒì— ëŒ€í•œ ìŠ¤ë§ˆíŠ¸í•œ ëŒ€ì²´ê°’\n",
    "\n",
    "### âš ï¸ ì£¼ì˜ì‚¬í•­\n",
    "\n",
    "- submission íŒŒì¼ì´ ëª¨ë‘ 0ì´ë©´ ì•ˆ ë©ë‹ˆë‹¤!\n",
    "- ì˜ˆì¸¡ê°’ì´ ì—†ì„ ë•ŒëŠ” ì•„ì´í…œë³„ í‰ê·  ì‚¬ìš©\n",
    "- ì´ìƒì¹˜ê°€ ë„ˆë¬´ í¬ë©´ ì œí•œ í•„ìš”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaa6542",
   "metadata": {},
   "source": [
    "## ğŸš€ ê³µí–‰ì„± ì ìˆ˜ ê°œì„  ë²„ì „ (v2)\n",
    "\n",
    "ê¸°ì¡´ ë²„ì „ì˜ ê°œì„  ì‚¬í•­:\n",
    "1. **ë‹¤ì–‘í•œ lag ê¸¸ì´ ê³ ë ¤** (1, 2, 3ê°œì›” lead)\n",
    "2. **Spearman ìƒê´€ê³„ìˆ˜ ì¶”ê°€** (ë¹„ì„ í˜• ê´€ê³„ íƒì§€)\n",
    "3. **Cross-correlation ìµœëŒ€ê°’ ì‚¬ìš©** (ìµœì  lag ìë™ íƒì§€)\n",
    "4. **ì„ê³„ê°’ ì¡°ì •** (ë” ë§ì€ í›„ë³´ìŒ ë°œê²¬)\n",
    "5. **ìƒí˜¸ì •ë³´ëŸ‰ ê¸°ë°˜ ì ìˆ˜** (ì„ í˜•/ë¹„ì„ í˜• ëª¨ë‘ í¬ì°©)\n",
    "6. **ì •ê·œí™”ëœ ìƒê´€ê³„ìˆ˜** (ì‹œê³„ì—´ ê¸¸ì´ ë³´ì •)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c883c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ğŸš€ ê°œì„ ëœ ê³µí–‰ì„± í›„ë³´ìŒ ìƒì„± í•¨ìˆ˜ (v2)\n",
    "# ============================================================\n",
    "# ì£¼ìš” ê°œì„ : ë‹¤ì–‘í•œ lag, Spearman ìƒê´€, cross-correlation ìµœëŒ€ê°’\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from typing import Optional\n",
    "\n",
    "def compute_correlations_v2(resid_mat: pd.DataFrame, max_lag: int = 3):\n",
    "    \"\"\"\n",
    "    ê°œì„ ëœ ìƒê´€ê³„ìˆ˜ ê³„ì‚°:\n",
    "    - ë‹¤ì–‘í•œ lag ê¸¸ì´ (1, 2, 3ê°œì›”)\n",
    "    - Pearson + Spearman ìƒê´€ê³„ìˆ˜\n",
    "    - Cross-correlation ìµœëŒ€ê°’\n",
    "    \"\"\"\n",
    "    if resid_mat.empty or len(resid_mat.columns) == 0:\n",
    "        raise ValueError(\"resid_matê°€ ë¹„ì–´ìˆê±°ë‚˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    if len(resid_mat) < max_lag + 1:\n",
    "        max_lag = max(1, len(resid_mat) - 1)\n",
    "        print(f\"âš ï¸ ë°ì´í„°ê°€ ì§§ì•„ max_lagë¥¼ {max_lag}ë¡œ ì¡°ì •í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    resid_centered = resid_mat - resid_mat.mean(axis=0)\n",
    "    items = resid_centered.columns\n",
    "    \n",
    "    if len(items) == 0:\n",
    "        raise ValueError(\"ìœ íš¨í•œ ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ë™ì‹œ ìƒê´€ (Pearson + Spearman)\n",
    "    corr_same_pearson = resid_centered.corr()\n",
    "    corr_same_spearman = resid_centered.corr(method='spearman')\n",
    "    \n",
    "    # Lead ìƒê´€ì„ ì—¬ëŸ¬ lagì— ëŒ€í•´ ê³„ì‚°\n",
    "    corr_lead_dict = {}  # {lag: DataFrame}\n",
    "    \n",
    "    for lag in range(1, max_lag + 1):\n",
    "        corr_lead = pd.DataFrame(index=items, columns=items, dtype=float)\n",
    "        \n",
    "        resid_t = resid_centered.iloc[:-lag, :] if lag < len(resid_centered) else resid_centered.iloc[:0, :]\n",
    "        resid_tlag = resid_centered.iloc[lag:, :] if lag < len(resid_centered) else resid_centered.iloc[:0, :]\n",
    "        \n",
    "        for i in items:\n",
    "            x = resid_t[i].values if i in resid_t.columns else np.array([])\n",
    "            mask_x = ~np.isnan(x)\n",
    "            \n",
    "            for j in items:\n",
    "                if i == j:\n",
    "                    corr_lead.loc[i, j] = np.nan\n",
    "                    continue\n",
    "                \n",
    "                y = resid_tlag[j].values if j in resid_tlag.columns else np.array([])\n",
    "                mask_y = ~np.isnan(y)\n",
    "                \n",
    "                mask = mask_x & mask_y\n",
    "                if mask.sum() < 5:\n",
    "                    corr_lead.loc[i, j] = np.nan\n",
    "                    continue\n",
    "                \n",
    "                # Pearson ìƒê´€\n",
    "                try:\n",
    "                    corr_pearson = np.corrcoef(x[mask], y[mask])[0, 1]\n",
    "                    if np.isnan(corr_pearson):\n",
    "                        corr_lead.loc[i, j] = np.nan\n",
    "                        continue\n",
    "                except:\n",
    "                    corr_lead.loc[i, j] = np.nan\n",
    "                    continue\n",
    "                \n",
    "                # Spearman ìƒê´€\n",
    "                try:\n",
    "                    corr_spearman, _ = spearmanr(x[mask], y[mask])\n",
    "                    if np.isnan(corr_spearman):\n",
    "                        corr_spearman = corr_pearson\n",
    "                except:\n",
    "                    corr_spearman = corr_pearson\n",
    "                \n",
    "                # ë‘ ìƒê´€ê³„ìˆ˜ì˜ í‰ê·  (ë” robust)\n",
    "                corr_lead.loc[i, j] = (corr_pearson + corr_spearman) / 2\n",
    "        \n",
    "        corr_lead_dict[lag] = corr_lead\n",
    "    \n",
    "    return corr_same_pearson, corr_same_spearman, corr_lead_dict\n",
    "\n",
    "\n",
    "def select_candidate_pairs_v2(\n",
    "    corr_same_pearson: pd.DataFrame,\n",
    "    corr_same_spearman: pd.DataFrame,\n",
    "    corr_lead_dict: dict,\n",
    "    th_same: float = 0.3,  # ë‚®ì¶¤ (ë” ë§ì€ í›„ë³´)\n",
    "    th_lead: float = 0.4,  # ë‚®ì¶¤\n",
    "    min_diff: float = 0.03,  # ë‚®ì¶¤\n",
    "    use_best_lag: bool = True,  # ìµœì  lag ìë™ ì„ íƒ\n",
    "    top_k: Optional[int] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ê°œì„ ëœ í›„ë³´ìŒ ì„ ì •:\n",
    "    - Pearsonê³¼ Spearman ìƒê´€ê³„ìˆ˜ ëª¨ë‘ ê³ ë ¤\n",
    "    - ì—¬ëŸ¬ lag ì¤‘ ìµœì  lag ì„ íƒ\n",
    "    - ë” ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ë” ë§ì€ í›„ë³´ ë°œê²¬\n",
    "    \"\"\"\n",
    "    items = corr_same_pearson.index\n",
    "    pairs = []\n",
    "    \n",
    "    # ë™ì‹œ ìƒê´€ì€ Pearsonê³¼ Spearmanì˜ í‰ê·  ì‚¬ìš©\n",
    "    corr_same = (corr_same_pearson + corr_same_spearman) / 2\n",
    "    \n",
    "    for i in items:\n",
    "        for j in items:\n",
    "            if i == j:\n",
    "                continue\n",
    "            \n",
    "            same = corr_same.loc[i, j]\n",
    "            if pd.isna(same) or same < th_same:\n",
    "                continue\n",
    "            \n",
    "            # ì—¬ëŸ¬ lag ì¤‘ ìµœì  lag ì°¾ê¸°\n",
    "            best_lag = 1\n",
    "            best_lead_ij = np.nan\n",
    "            best_lead_ji = np.nan\n",
    "            \n",
    "            if use_best_lag:\n",
    "                # ê° lagì— ëŒ€í•´ ìƒê´€ê³„ìˆ˜ í™•ì¸í•˜ê³  ìµœëŒ€ê°’ ì„ íƒ\n",
    "                for lag in sorted(corr_lead_dict.keys()):\n",
    "                    lead_ij_lag = corr_lead_dict[lag].loc[i, j]\n",
    "                    lead_ji_lag = corr_lead_dict[lag].loc[j, i]\n",
    "                    \n",
    "                    if pd.notna(lead_ij_lag) and (pd.isna(best_lead_ij) or lead_ij_lag > best_lead_ij):\n",
    "                        best_lead_ij = lead_ij_lag\n",
    "                        best_lag = lag\n",
    "                    \n",
    "                    if pd.notna(lead_ji_lag) and (pd.isna(best_lead_ji) or lead_ji_lag > best_lead_ji):\n",
    "                        best_lead_ji = lead_ji_lag\n",
    "                        if pd.isna(best_lead_ij) or lead_ji_lag > best_lead_ij:\n",
    "                            best_lag = lag\n",
    "            else:\n",
    "                # lag=1ë§Œ ì‚¬ìš©\n",
    "                best_lead_ij = corr_lead_dict[1].loc[i, j] if 1 in corr_lead_dict else np.nan\n",
    "                best_lead_ji = corr_lead_dict[1].loc[j, i] if 1 in corr_lead_dict else np.nan\n",
    "            \n",
    "            # iâ†’j ë°©í–¥\n",
    "            if pd.notna(best_lead_ij):\n",
    "                if pd.isna(best_lead_ji) or (best_lead_ij - best_lead_ji) >= min_diff:\n",
    "                    if best_lead_ij >= th_lead:\n",
    "                        pairs.append({\n",
    "                            \"leading_item_id\": i,\n",
    "                            \"following_item_id\": j,\n",
    "                            \"corr_same\": same,\n",
    "                            \"corr_lead\": best_lead_ij,\n",
    "                            \"lag\": best_lag,\n",
    "                            \"direction\": \"i_leads_j\"\n",
    "                        })\n",
    "            \n",
    "            # jâ†’i ë°©í–¥\n",
    "            if pd.notna(best_lead_ji):\n",
    "                if pd.isna(best_lead_ij) or (best_lead_ji - best_lead_ij) >= min_diff:\n",
    "                    if best_lead_ji >= th_lead:\n",
    "                        pairs.append({\n",
    "                            \"leading_item_id\": j,\n",
    "                            \"following_item_id\": i,\n",
    "                            \"corr_same\": same,\n",
    "                            \"corr_lead\": best_lead_ji,\n",
    "                            \"lag\": best_lag,\n",
    "                            \"direction\": \"j_leads_i\"\n",
    "                        })\n",
    "    \n",
    "    candidates_df = pd.DataFrame(pairs)\n",
    "    \n",
    "    if candidates_df.empty:\n",
    "        return candidates_df\n",
    "    \n",
    "    # ì¤‘ë³µ ì œê±° ë° ì •ë ¬\n",
    "    candidates_df = candidates_df.sort_values(\n",
    "        [\"corr_lead\", \"corr_same\"], ascending=False\n",
    "    ).drop_duplicates(\n",
    "        subset=[\"leading_item_id\", \"following_item_id\"]\n",
    "    )\n",
    "    \n",
    "    if top_k is not None:\n",
    "        candidates_df = candidates_df.head(top_k)\n",
    "    \n",
    "    return candidates_df\n",
    "\n",
    "\n",
    "def generate_comovement_candidates_v2(\n",
    "    df_panel: pd.DataFrame,\n",
    "    min_nonzero_months: int = 6,  # ë‚®ì¶¤ (ë” ë§ì€ ì•„ì´í…œ í¬í•¨)\n",
    "    min_std: float = 0.15,  # ë‚®ì¶¤\n",
    "    th_same: float = 0.3,  # ë‚®ì¶¤\n",
    "    th_lead: float = 0.4,  # ë‚®ì¶¤\n",
    "    min_diff: float = 0.03,  # ë‚®ì¶¤\n",
    "    max_lag: int = 3,  # ì—¬ëŸ¬ lag ê³ ë ¤\n",
    "    use_best_lag: bool = True,\n",
    "    top_k: Optional[int] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ê°œì„ ëœ ê³µí–‰ì„± í›„ë³´ìŒ ìƒì„± íŒŒì´í”„ë¼ì¸ (v2)\n",
    "    ê¸°ì¡´ í•¨ìˆ˜ë“¤ì„ ì§ì ‘ í˜¸ì¶œ (ë…¸íŠ¸ë¶ì— ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆìŒ)\n",
    "    \"\"\"\n",
    "    # ì…ë ¥ ê²€ì¦\n",
    "    if df_panel is None or df_panel.empty:\n",
    "        raise ValueError(\"df_panelì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    required_cols = [\"date\", \"item_id\", \"value\"]\n",
    "    missing_cols = [c for c in required_cols if c not in df_panel.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"df_panelì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_cols}\")\n",
    "    \n",
    "    # 0) value matrix (ê¸°ì¡´ í•¨ìˆ˜ ì‚¬ìš©)\n",
    "    df = df_panel.copy()\n",
    "    df = df.sort_values([\"date\", \"item_id\"])\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
    "    df[\"value\"] = df[\"value\"].clip(lower=0)\n",
    "    df[\"log_value\"] = np.log1p(df[\"value\"].astype(float))\n",
    "    df.loc[~np.isfinite(df[\"log_value\"]), \"log_value\"] = np.nan\n",
    "    \n",
    "    # pivot ì „ì— ë°ì´í„° í™•ì¸\n",
    "    if df[\"item_id\"].nunique() == 0 or df[\"date\"].nunique() == 0:\n",
    "        raise ValueError(\"ìœ íš¨í•œ item_idë‚˜ dateê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    try:\n",
    "        value_mat = df.pivot(index=\"date\", columns=\"item_id\", values=\"log_value\").sort_index()\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"pivot ì‹¤íŒ¨: {e}. ì¤‘ë³µëœ (date, item_id) ì¡°í•©ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    \n",
    "    if value_mat.empty:\n",
    "        raise ValueError(\"value_matê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # 1) item filtering (ë” ê´€ëŒ€í•œ ê¸°ì¤€)\n",
    "    value_nonzero_cnt = (value_mat > 0).sum(axis=0)\n",
    "    value_std = value_mat.std(axis=0, skipna=True)\n",
    "    valid_items = value_mat.columns[\n",
    "        (value_nonzero_cnt >= min_nonzero_months) & (value_std >= min_std)\n",
    "    ]\n",
    "    value_mat_f = value_mat[valid_items]\n",
    "    \n",
    "    # 2) residuals (ê¸°ì¡´ í•¨ìˆ˜ ì‚¬ìš©)\n",
    "    market_index = value_mat_f.mean(axis=1, skipna=True)\n",
    "    common_index = value_mat_f.index.intersection(market_index.index)\n",
    "    X = market_index.loc[common_index].values\n",
    "    Y = value_mat_f.loc[common_index]\n",
    "    resid_mat = pd.DataFrame(index=common_index, columns=Y.columns, dtype=float)\n",
    "    for col in Y.columns:\n",
    "        y = Y[col].values\n",
    "        mask = ~np.isnan(y) & ~np.isnan(X)\n",
    "        if mask.sum() < 5:\n",
    "            resid_mat[col] = np.nan\n",
    "            continue\n",
    "        X_m = X[mask]\n",
    "        y_m = y[mask]\n",
    "        A = np.vstack([np.ones_like(X_m), X_m]).T\n",
    "        coef, _, _, _ = np.linalg.lstsq(A, y_m, rcond=None)\n",
    "        a, b = coef\n",
    "        y_hat = a + b * X\n",
    "        resid = y - y_hat\n",
    "        resid_mat[col] = resid\n",
    "    \n",
    "    # 3) ê°œì„ ëœ ìƒê´€ê³„ìˆ˜ ê³„ì‚°\n",
    "    corr_same_pearson, corr_same_spearman, corr_lead_dict = compute_correlations_v2(\n",
    "        resid_mat, max_lag=max_lag\n",
    "    )\n",
    "    \n",
    "    # 4) ê°œì„ ëœ í›„ë³´ìŒ ì„ ì •\n",
    "    candidates_df = select_candidate_pairs_v2(\n",
    "        corr_same_pearson, corr_same_spearman, corr_lead_dict,\n",
    "        th_same=th_same,\n",
    "        th_lead=th_lead,\n",
    "        min_diff=min_diff,\n",
    "        use_best_lag=use_best_lag,\n",
    "        top_k=top_k\n",
    "    )\n",
    "    \n",
    "    return candidates_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d40ae2",
   "metadata": {},
   "source": [
    "### ê°œì„ ëœ ê³µí–‰ì„± í›„ë³´ìŒ ìƒì„± ì‹¤í–‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0c2d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°œì„ ëœ ê³µí–‰ì„± í›„ë³´ìŒ ìƒì„± (ë” ë§ì€ í›„ë³´ ë°œê²¬)\n",
    "print(\"ğŸš€ ê°œì„ ëœ ê³µí–‰ì„± í›„ë³´ìŒ ìƒì„± ì¤‘...\")\n",
    "print(\"   - ë” ë‚®ì€ ì„ê³„ê°’ ì‚¬ìš©\")\n",
    "print(\"   - ì—¬ëŸ¬ lag ê¸¸ì´ ê³ ë ¤ (1, 2, 3ê°œì›”)\")\n",
    "print(\"   - Pearson + Spearman ìƒê´€ê³„ìˆ˜ ì¡°í•©\")\n",
    "\n",
    "# df_panel ë³€ìˆ˜ í™•ì¸\n",
    "if 'df_panel' not in globals() or df_panel is None:\n",
    "    raise ValueError(\"âŒ df_panel ë³€ìˆ˜ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € íŒ¨ë„ ë°ì´í„°ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "try:\n",
    "    candidates_df_v2 = generate_comovement_candidates_v2(\n",
    "        df_panel,\n",
    "        min_nonzero_months=6,  # ê¸°ì¡´ 8 â†’ 6 (ë” ë§ì€ ì•„ì´í…œ)\n",
    "        min_std=0.15,  # ê¸°ì¡´ 0.2 â†’ 0.15\n",
    "        th_same=0.3,  # ê¸°ì¡´ 0.4 â†’ 0.3\n",
    "        th_lead=0.4,  # ê¸°ì¡´ 0.5 â†’ 0.4\n",
    "        min_diff=0.03,  # ê¸°ì¡´ 0.05 â†’ 0.03\n",
    "        max_lag=3,  # 1, 2, 3ê°œì›” ëª¨ë‘ ê³ ë ¤\n",
    "        use_best_lag=True,  # ìµœì  lag ìë™ ì„ íƒ\n",
    "        top_k=1000  # ìƒìœ„ 1000ìŒ\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ… í›„ë³´ìŒ ìƒì„± ì™„ë£Œ!\")\n",
    "    print(f\"   - ê¸°ì¡´ ë²„ì „: {len(candidates_df) if 'candidates_df' in globals() else 0}ê°œ ìŒ\")\n",
    "    print(f\"   - ê°œì„  ë²„ì „: {len(candidates_df_v2)}ê°œ ìŒ\")\n",
    "    \n",
    "    if len(candidates_df_v2) > 0:\n",
    "        print(f\"\\nğŸ“Š ê°œì„  ë²„ì „ ìƒìœ„ 20ê°œ ìŒ:\")\n",
    "        print(candidates_df_v2.head(20))\n",
    "        \n",
    "        # candidates_dfë¥¼ v2ë¡œ ì—…ë°ì´íŠ¸ (ì´í›„ ì½”ë“œì—ì„œ ì‚¬ìš©)\n",
    "        candidates_df = candidates_df_v2.copy()\n",
    "        print(f\"\\nâœ… candidates_dfë¥¼ v2ë¡œ ì—…ë°ì´íŠ¸ ì™„ë£Œ!\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ í›„ë³´ìŒì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„ê³„ê°’ì„ ë” ë‚®ì¶°ë³´ì„¸ìš”.\")\n",
    "        # ê¸°ì¡´ candidates_df ìœ ì§€\n",
    "        if 'candidates_df' not in globals():\n",
    "            candidates_df = pd.DataFrame(columns=['leading_item_id', 'following_item_id', 'corr_same', 'corr_lead'])\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    print(f\"   ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}\")\n",
    "    import traceback\n",
    "    print(f\"\\nìƒì„¸ ì˜¤ë¥˜:\")\n",
    "    traceback.print_exc()\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38327800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
