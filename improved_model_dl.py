"""
ë”¥ëŸ¬ë‹ ëª¨ë¸ ì¶”ê°€ ë²„ì „
- LSTM ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸
- 1D CNN ê¸°ë°˜ ëª¨ë¸
- ê¸°ì¡´ ì•™ìƒë¸”ê³¼ ë”¥ëŸ¬ë‹ ëª¨ë¸ ì•™ìƒë¸”
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import TimeSeriesSplit
import warnings
warnings.filterwarnings('ignore')

# ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì„ íƒì )
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
    DL_AVAILABLE = True
    print("TensorFlow ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    try:
        import torch
        import torch.nn as nn
        DL_AVAILABLE = False  # PyTorchëŠ” ë³„ë„ êµ¬í˜„ í•„ìš”
        print("TensorFlow ì—†ìŒ, PyTorchë§Œ ì‚¬ìš© ê°€ëŠ¥ (êµ¬í˜„ í•„ìš”)")
    except ImportError:
        DL_AVAILABLE = False
        print("ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - pip install tensorflow í•„ìš”")


# ============================================================================
# ê¸°ì¡´ í•¨ìˆ˜ë“¤ (improved_model_v2.pyì—ì„œ ê°€ì ¸ì˜´)
# ============================================================================

def load_and_preprocess():
    """ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì „ì²˜ë¦¬"""
    print("ë°ì´í„° ë¡œë“œ ì¤‘...")
    df = pd.read_csv('train.csv')
    
    num_int_cols = ["year", "month", "hs4"]
    for c in num_int_cols:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")
    
    num_float_cols = ["value", "weight", "quantity"]
    for c in num_float_cols:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")
    
    if "date" not in df.columns:
        df["date"] = pd.to_datetime(
            df["year"].astype(int).astype(str) + "-" +
            df["month"].astype(int).astype(str) + "-01"
        )
    else:
        df["date"] = pd.to_datetime(df["date"])
    
    df["has_tx"] = 1
    
    agg_dict = {
        "value": lambda x: x.sum(min_count=1),
        "weight": lambda x: x.sum(min_count=1),
        "quantity": lambda x: x.sum(min_count=1),
        "has_tx": "sum",
    }
    
    group_cols = ["item_id", "hs4", "date"]
    df_month = df.groupby(group_cols, as_index=False).agg(agg_dict)
    df_month["has_tx"] = (df_month["has_tx"] > 0).astype(int)
    
    for col in ["value", "weight", "quantity"]:
        df_month[f"{col}_missing"] = df_month[col].isna().astype(int)
        df_month[f"{col}_zero"] = ((df_month[col] == 0) & df_month[col].notna()).astype(int)
    
    min_month = df_month["date"].min()
    max_month = df_month["date"].max()
    full_months = pd.date_range(min_month, max_month, freq="MS")
    all_items = df_month["item_id"].unique()
    
    panel = pd.MultiIndex.from_product(
        [all_items, full_months],
        names=["item_id", "date"]
    ).to_frame(index=False)
    
    df_panel = (
        panel.merge(df_month, on=["item_id", "date"], how="left")
        .sort_values(["item_id", "date"])
        .reset_index(drop=True)
    )
    
    df_panel["has_tx"] = df_panel["has_tx"].fillna(0).astype(int)
    
    no_tx_mask = df_panel["has_tx"] == 0
    for col in ["value", "weight", "quantity"]:
        df_panel.loc[no_tx_mask, col] = 0
    
    for col in [
        "value_zero", "weight_zero", "quantity_zero",
        "value_missing", "weight_missing", "quantity_missing",
    ]:
        df_panel[col] = df_panel[col].fillna(0).astype(int)
    
    df_panel["year"] = df_panel["date"].dt.year
    df_panel["month"] = df_panel["date"].dt.month
    
    print(f"ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df_panel)} rows, {df_panel['item_id'].nunique()} items")
    return df_panel


def find_comovement_pairs_v2(ts_matrix, max_lag=12, min_corr=0.25, min_common=6):
    """ê³µí–‰ì„± ìŒ íƒìƒ‰"""
    items = ts_matrix.columns.tolist()
    n_items = len(items)
    results = []
    
    def _zscore(col: pd.Series) -> pd.Series:
        std = col.std()
        if std is None or np.isnan(std) or std == 0:
            return col - col.mean()
        return (col - col.mean()) / std
    
    ts_std = ts_matrix.apply(_zscore, axis=0)
    
    print(f"ê³µí–‰ì„± ìŒ íƒìƒ‰ ì¤‘... (ì´ {n_items * (n_items - 1) // 2}ê°œ ìŒ)")
    
    for i in range(n_items):
        if (i + 1) % 20 == 0:
            print(f"ì§„í–‰ë¥ : {i+1}/{n_items}")
        
        for j in range(i + 1, n_items):
            a_name = items[i]
            b_name = items[j]
            
            a = ts_std[a_name]
            b = ts_std[b_name]
            
            ab = pd.concat([a, b], axis=1).dropna()
            if len(ab) < min_common:
                continue
            
            a_clean = ab.iloc[:, 0]
            b_clean = ab.iloc[:, 1]
            
            best_corr = 0.0
            best_lag = 0
            
            for lag in range(-max_lag, max_lag + 1):
                if lag == 0:
                    corr_p = a_clean.corr(b_clean)
                    corr_s = a_clean.corr(b_clean, method='spearman')
                    corr = max(abs(corr_p) if not np.isnan(corr_p) else 0,
                              abs(corr_s) if not np.isnan(corr_s) else 0)
                elif lag > 0:
                    if len(a_clean) > lag:
                        corr_p = a_clean.iloc[:-lag].corr(b_clean.iloc[lag:])
                        corr_s = a_clean.iloc[:-lag].corr(b_clean.iloc[lag:], method='spearman')
                        corr = max(abs(corr_p) if not np.isnan(corr_p) else 0,
                                  abs(corr_s) if not np.isnan(corr_s) else 0)
                    else:
                        continue
                else:
                    k = -lag
                    if len(a_clean) > k:
                        corr_p = a_clean.iloc[k:].corr(b_clean.iloc[:-k])
                        corr_s = a_clean.iloc[k:].corr(b_clean.iloc[:-k], method='spearman')
                        corr = max(abs(corr_p) if not np.isnan(corr_p) else 0,
                                  abs(corr_s) if not np.isnan(corr_s) else 0)
                    else:
                        continue
                
                if corr is None or np.isnan(corr):
                    continue
                
                if abs(corr) > abs(best_corr):
                    best_corr = corr
                    best_lag = lag
            
            if abs(best_corr) >= min_corr and best_lag != 0:
                if best_lag > 0:
                    leading = a_name
                    following = b_name
                    lead_lag = best_lag
                else:
                    leading = b_name
                    following = a_name
                    lead_lag = -best_lag
                
                results.append({
                    "leading_item_id": leading,
                    "following_item_id": following,
                    "lead_lag_months": lead_lag,
                    "corr": best_corr,
                    "corr_abs": abs(best_corr),
                })
    
    if not results:
        return pd.DataFrame()
    
    result_df = pd.DataFrame(results)
    result_df = result_df.sort_values(["corr_abs", "lead_lag_months"], ascending=[False, True]).reset_index(drop=True)
    return result_df


def create_advanced_features(df_panel, max_lag=12):
    """Feature Engineering"""
    df = df_panel.copy()
    
    for item_id in df["item_id"].unique():
        item_mask = df["item_id"] == item_id
        item_data = df.loc[item_mask].sort_values("date").reset_index(drop=True)
        
        for k in range(1, max_lag + 1):
            df.loc[item_mask, f"value_lag_{k}"] = item_data["value"].shift(k).values
        
        item_data_sorted = df.loc[item_mask].sort_values("date")
        df.loc[item_mask, "value_ma3"] = item_data_sorted["value"].rolling(3, min_periods=1).mean().values
        df.loc[item_mask, "value_ma6"] = item_data_sorted["value"].rolling(6, min_periods=1).mean().values
        df.loc[item_mask, "value_ma12"] = item_data_sorted["value"].rolling(12, min_periods=1).mean().values
        df.loc[item_mask, "value_std3"] = item_data_sorted["value"].rolling(3, min_periods=1).std().fillna(0).values
        df.loc[item_mask, "value_pct_change"] = item_data_sorted["value"].pct_change().fillna(0).values
        df.loc[item_mask, "month_sin"] = np.sin(2 * np.pi * item_data_sorted["month"] / 12).values
        df.loc[item_mask, "month_cos"] = np.cos(2 * np.pi * item_data_sorted["month"] / 12).values
    
    return df


# ============================================================================
# ë”¥ëŸ¬ë‹ ëª¨ë¸ ì •ì˜
# ============================================================================

def build_lstm_model(input_shape, sequence_length=12):
    """LSTM ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸"""
    if not DL_AVAILABLE:
        return None
    
    model = keras.Sequential([
        layers.Input(shape=input_shape),
        layers.LSTM(64, return_sequences=True, dropout=0.2),
        layers.LSTM(32, return_sequences=False, dropout=0.2),
        layers.Dense(16, activation='relu'),
        layers.Dropout(0.2),
        layers.Dense(1)
    ])
    
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='mse',
        metrics=['mae']
    )
    
    return model


def build_cnn_lstm_model(input_shape, sequence_length=12):
    """CNN + LSTM í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸"""
    if not DL_AVAILABLE:
        return None
    
    model = keras.Sequential([
        layers.Input(shape=input_shape),
        layers.Conv1D(filters=64, kernel_size=3, activation='relu'),
        layers.Conv1D(filters=64, kernel_size=3, activation='relu'),
        layers.MaxPooling1D(pool_size=2),
        layers.LSTM(50, return_sequences=False, dropout=0.2),
        layers.Dense(16, activation='relu'),
        layers.Dropout(0.2),
        layers.Dense(1)
    ])
    
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='mse',
        metrics=['mae']
    )
    
    return model


def build_attention_lstm_model(input_shape, sequence_length=12):
    """Attention ë©”ì»¤ë‹ˆì¦˜ í¬í•¨ LSTM ëª¨ë¸"""
    if not DL_AVAILABLE:
        return None
    
    inputs = layers.Input(shape=input_shape)
    
    # LSTM layers
    lstm1 = layers.LSTM(64, return_sequences=True, dropout=0.2)(inputs)
    lstm2 = layers.LSTM(32, return_sequences=True, dropout=0.2)(lstm1)
    
    # Attention
    attention = layers.MultiHeadAttention(num_heads=4, key_dim=8)(lstm2, lstm2)
    attention = layers.GlobalAveragePooling1D()(attention)
    
    # Dense layers
    dense1 = layers.Dense(16, activation='relu')(attention)
    dropout = layers.Dropout(0.2)(dense1)
    outputs = layers.Dense(1)(dropout)
    
    model = keras.Model(inputs=inputs, outputs=outputs)
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='mse',
        metrics=['mae']
    )
    
    return model


# ============================================================================
# ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
# ============================================================================

def create_sequences_for_dl(df_panel, comovement_candidates, sequence_length=12, max_lag=12):
    """ë”¥ëŸ¬ë‹ìš© ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±"""
    print("ë”¥ëŸ¬ë‹ìš© ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± ì¤‘...")
    
    df_panel = create_advanced_features(df_panel, max_lag)
    
    item_groups = {
        item: g.sort_values("date").reset_index(drop=True)
        for item, g in df_panel.groupby("item_id")
    }
    
    X_sequences = []
    y_values = []
    metadata = []
    
    for row in comovement_candidates.itertuples(index=False):
        leading = row.leading_item_id
        following = row.following_item_id
        
        if leading not in item_groups or following not in item_groups:
            continue
        
        lead_df = item_groups[leading].copy()
        foll_df = item_groups[following].copy()
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        for i in range(sequence_length, len(foll_df) - 1):
            # Following ì‹œí€€ìŠ¤
            foll_seq = foll_df.iloc[i-sequence_length:i][["value", "value_ma3", "value_ma6", 
                                                          "value_std3", "value_pct_change",
                                                          "month_sin", "month_cos"]].values
            
            # Leading ì‹œí€€ìŠ¤ (lag ê³ ë ¤)
            lead_start = max(0, i - sequence_length - row.lead_lag_months)
            lead_end = max(0, i - row.lead_lag_months)
            if lead_end > lead_start:
                lead_seq = lead_df.iloc[lead_start:lead_end][["value", "value_ma3", "value_ma6",
                                                              "value_std3", "value_pct_change"]].values
                # íŒ¨ë”©
                if len(lead_seq) < sequence_length:
                    padding = np.zeros((sequence_length - len(lead_seq), lead_seq.shape[1]))
                    lead_seq = np.vstack([padding, lead_seq])
            else:
                lead_seq = np.zeros((sequence_length, 5))
            
            # ê²°í•©
            combined_seq = np.concatenate([foll_seq, lead_seq], axis=1)
            
            X_sequences.append(combined_seq)
            y_values.append(foll_df.iloc[i+1]["value"])
            metadata.append({
                "leading": leading,
                "following": following,
                "date_idx": i
            })
    
    if len(X_sequences) == 0:
        return None, None, None
    
    X_sequences = np.array(X_sequences)
    y_values = np.array(y_values)
    
    print(f"ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(X_sequences)} samples, shape: {X_sequences.shape}")
    return X_sequences, y_values, metadata


# ============================================================================
# ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ
# ============================================================================

def train_dl_models(X_seq, y, sequence_length=12):
    """ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ"""
    if not DL_AVAILABLE:
        print("ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - ìŠ¤í‚µ")
        return None
    
    if X_seq is None or len(X_seq) == 0:
        print("ì‹œí€€ìŠ¤ ë°ì´í„° ì—†ìŒ - ìŠ¤í‚µ")
        return None
    
    print("ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì¤‘...")
    
    # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler()
    
    # ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ 2Dë¡œ ë³€í™˜í•˜ì—¬ ìŠ¤ì¼€ì¼ë§
    n_samples, seq_len, n_features = X_seq.shape
    X_flat = X_seq.reshape(-1, n_features)
    X_scaled_flat = scaler_X.fit_transform(X_flat)
    X_scaled = X_scaled_flat.reshape(n_samples, seq_len, n_features)
    
    y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()
    
    # Train/Validation split
    split_idx = int(len(X_scaled) * 0.8)
    X_train, X_val = X_scaled[:split_idx], X_scaled[split_idx:]
    y_train, y_val = y_scaled[:split_idx], y_scaled[split_idx:]
    
    input_shape = (sequence_length, X_scaled.shape[2])
    models = {}
    
    # LSTM ëª¨ë¸
    print("  LSTM ëª¨ë¸ í•™ìŠµ...")
    lstm_model = build_lstm_model(input_shape, sequence_length)
    if lstm_model:
        history = lstm_model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=50,
            batch_size=32,
            verbose=0,
            callbacks=[
                keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),
                keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5)
            ]
        )
        models["lstm"] = (lstm_model, scaler_y)
        print(f"    LSTM Val Loss: {min(history.history['val_loss']):.4f}")
    
    # CNN-LSTM ëª¨ë¸
    print("  CNN-LSTM ëª¨ë¸ í•™ìŠµ...")
    cnn_lstm_model = build_cnn_lstm_model(input_shape, sequence_length)
    if cnn_lstm_model:
        history = cnn_lstm_model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=50,
            batch_size=32,
            verbose=0,
            callbacks=[
                keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),
                keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5)
            ]
        )
        models["cnn_lstm"] = (cnn_lstm_model, scaler_y)
        print(f"    CNN-LSTM Val Loss: {min(history.history['val_loss']):.4f}")
    
    return models, scaler_X


# ============================================================================
# ê¸°ì¡´ ì•™ìƒë¸” ëª¨ë¸ (ê°„ë‹¨ ë²„ì „)
# ============================================================================

def train_traditional_models(train_df, feature_cols):
    """ê¸°ì¡´ ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ"""
    print("ê¸°ì¡´ ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ ì¤‘...")
    
    X = train_df[feature_cols].values
    y = train_df["f_target_value_next"].values
    
    models = {
        "rf": RandomForestRegressor(n_estimators=500, max_depth=20, random_state=42, n_jobs=-1),
        "gbm": GradientBoostingRegressor(n_estimators=500, learning_rate=0.03, max_depth=6, random_state=42),
        "et": ExtraTreesRegressor(n_estimators=500, max_depth=20, random_state=42, n_jobs=-1)
    }
    
    trained = {}
    for name, model in models.items():
        model.fit(X, y)
        trained[name] = model
        print(f"  {name} ì™„ë£Œ")
    
    return trained


# ============================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("ë”¥ëŸ¬ë‹ ëª¨ë¸ í¬í•¨ ê³µí–‰ì„± ìŒ ì˜ˆì¸¡ ëª¨ë¸")
    print("=" * 60)
    
    if not DL_AVAILABLE:
        print("\nâš ï¸ TensorFlowê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´: pip install tensorflow")
        print("ê¸°ì¡´ ì•™ìƒë¸” ëª¨ë¸ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n")
    
    # 1. ë°ì´í„° ë¡œë“œ
    df_panel = load_and_preprocess()
    
    # 2. ì‹œê³„ì—´ í–‰ë ¬ ìƒì„±
    print("\nì‹œê³„ì—´ í–‰ë ¬ ìƒì„± ì¤‘...")
    all_months = pd.date_range(df_panel["date"].min(), df_panel["date"].max(), freq="MS")
    
    ts_matrix = (
        df_panel.set_index("date")
        .groupby("item_id")["value"]
        .apply(lambda s: s.reindex(all_months).fillna(0))
        .unstack(0)
    )
    
    ts_matrix_log = np.log1p(ts_matrix)
    print(f"ì‹œê³„ì—´ í–‰ë ¬ shape: {ts_matrix_log.shape}")
    
    # 3. ê³µí–‰ì„± ìŒ íƒìƒ‰
    print("\nê³µí–‰ì„± ìŒ íƒìƒ‰ ì¤‘...")
    comovement_candidates = find_comovement_pairs_v2(
        ts_matrix=ts_matrix_log,
        max_lag=12,
        min_corr=0.25,
        min_common=6
    )
    
    print(f"ë°œê²¬ëœ ê³µí–‰ì„± ìŒ: {len(comovement_candidates)}ê°œ")
    
    # 4. ë”¥ëŸ¬ë‹ìš© ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
    if DL_AVAILABLE:
        print("\n" + "=" * 60)
        X_seq, y_seq, metadata = create_sequences_for_dl(df_panel, comovement_candidates, sequence_length=12)
        
        if X_seq is not None:
            dl_models, scaler_X = train_dl_models(X_seq, y_seq, sequence_length=12)
        else:
            dl_models = None
            scaler_X = None
    else:
        dl_models = None
        scaler_X = None
    
    # 5. ê¸°ì¡´ ì•™ìƒë¸” ëª¨ë¸ (ê°„ë‹¨ ë²„ì „ - ì‹¤ì œë¡œëŠ” improved_model_v2.pyì˜ í•¨ìˆ˜ ì‚¬ìš©)
    print("\n" + "=" * 60)
    print("ê¸°ì¡´ ì•™ìƒë¸” ëª¨ë¸ì€ improved_model_v2.pyë¥¼ ì°¸ê³ í•˜ì„¸ìš”.")
    print("ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ ë°ëª¨ìš©ì…ë‹ˆë‹¤.")
    
    print("\n" + "=" * 60)
    print("ì™„ë£Œ!")
    print("=" * 60)
    print("\nğŸ’¡ ë”¥ëŸ¬ë‹ ëª¨ë¸ ì‚¬ìš© ì‹œ ì˜ˆìƒ ê°œì„  íš¨ê³¼:")
    print("  - ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµ: +5-10%")
    print("  - ì¥ê±°ë¦¬ ì˜ì¡´ì„± í¬ì°©: +3-7%")
    print("  - ë³µì¡í•œ ë¹„ì„ í˜• ê´€ê³„: +2-5%")
    print("  - ì´ ì˜ˆìƒ ê°œì„ : +10-22% (ë°ì´í„° í¬ê¸°ì— ë”°ë¼ ë‹¤ë¦„)")
    print("\nâš ï¸ ì£¼ì˜ì‚¬í•­:")
    print("  - ì‘ì€ ë°ì´í„°ì…‹ì—ì„œëŠ” ì˜¤ë²„í”¼íŒ… ìœ„í—˜")
    print("  - í•™ìŠµ ì‹œê°„ ì¦ê°€")
    print("  - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í•„ìš”")


if __name__ == "__main__":
    main()


